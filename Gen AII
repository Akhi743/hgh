import fitz  # PyMuPDF
import os
import csv
import json
import uuid
from pathlib import Path
from typing import List, Optional, Union, Dict, Any
from google.genai import types
import google.genai as genai
from langchain.prompts import PromptTemplate
from langchain.output_parsers import PydanticOutputParser, OutputFixingParser
from langchain_core.exceptions import OutputParserException
from pydantic import BaseModel, Field
import pandas as pd

def pdf_to_images(pdf_path, output_dir=None, dpi=150, image_format='png'):
    """Convert each page of a PDF to individual images."""
    pdf_path = Path(pdf_path)
    
    if not pdf_path.exists():
        raise FileNotFoundError(f"PDF file not found: {pdf_path}")
    
    if output_dir is None:
        output_dir = pdf_path.parent
    else:
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
    
    doc = fitz.open(pdf_path)
    image_paths = []
    
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        mat = fitz.Matrix(dpi/72, dpi/72)
        pix = page.get_pixmap(matrix=mat)
        
        output_filename = f"{pdf_path.stem}_page_{page_num + 1:03d}.{image_format}"
        output_path = output_dir / output_filename
        
        pix.save(str(output_path))
        image_paths.append(str(output_path))
        
        print(f"Saved page {page_num + 1} to {output_path}")
        if page_num == 50:  # Limit for large PDFs
            print("Limiting to first 50 pages for large PDFs")
            break
    
    doc.close()
    return image_paths

class DocumentMetadata(BaseModel):
    """Document-level metadata extraction."""
    effective_date: Optional[str] = Field(default=None, description="Effective date found in document")
    hospital_names: Optional[List[str]] = Field(default=None, description="List of hospital names from main title")
    lob_type: Optional[str] = Field(default=None, description="Line of Business type")

class ServiceSection(BaseModel):
    """Individual Place of Service section with its table data."""
    place_of_service: str = Field(description="INPATIENT, OUTPATIENT, INPATIENT CARVE OUT, OUTPATIENT CARVE OUT")
    service_rows: List[Dict[str, str]] = Field(description="List of service rows with service, billing_codes, rates exactly as they appear in table")

class PageExtraction(BaseModel):
    """Complete page extraction including metadata and all sections."""
    has_new_document_section: bool = Field(description="Whether page starts new hospital/LOB section")
    metadata: Optional[DocumentMetadata] = Field(default=None, description="Document metadata if new section")
    service_sections: List[ServiceSection] = Field(description="All Place of Service sections found on this page")
    continuation_codes: Optional[str] = Field(default=None, description="Billing codes that continue from previous page")

class DocumentContext:
    """Tracks document-level context across pages."""
    def __init__(self):
        self.current_effective_date = None
        self.current_hospital_names = []
        self.current_lob = None
        self.last_incomplete_row = None  # Track row that might continue to next page
        
    def update_context(self, metadata: DocumentMetadata):
        """Update context with new metadata."""
        if metadata:
            if metadata.effective_date:
                self.current_effective_date = metadata.effective_date
            if metadata.hospital_names:
                self.current_hospital_names = metadata.hospital_names
            if metadata.lob_type:
                self.current_lob = metadata.lob_type

class HospitalRateExtractor:
    """Main extractor class with simple continuation handling."""
    
    def __init__(self, client, output_dir: str = None):
        self.client = client
        self.output_dir = Path(output_dir) if output_dir else Path.cwd()
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.context = DocumentContext()
        
        # Standard CSV columns
        self.csv_columns = [
            'Hospital_Names', 'LOB', 'Place_of_Service', 'Service', 
            'Billing_Codes', 'Rates', 'Effective_Date'
        ]
        
        # Initialize main CSV file
        self.csv_path = self.output_dir / "extracted_hospital_rates.csv"
        with open(self.csv_path, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(self.csv_columns)
    
    def create_extraction_prompt(self) -> tuple:
        """Create prompt for extracting data with continuation awareness."""
        base_parser = PydanticOutputParser(pydantic_object=PageExtraction)
        
        class GeminiLLM:
            def __init__(self, client):
                self.client = client
                
            def __call__(self, prompt: str) -> str:
                try:
                    response = self.client.models.generate_content(
                        model='gemini-2.0-flash',
                        contents=[prompt]
                    )
                    return response.text
                except Exception as e:
                    return f"Error: {e}"
        
        llm_wrapper = GeminiLLM(self.client)
        parser = OutputFixingParser.from_llm(parser=base_parser, llm=llm_wrapper)
        
        template = """
You are an expert at extracting ALL structured data from hospital rate schedules.

STEP 1 - DETECT NEW DOCUMENT SECTION:
Look for NEW MAIN DOCUMENT HEADER with hospital names like:
- "Sistersville General Hospital"
- "Marietta Memorial and Selby General Hospital"
NEVER extract random text like "MHS REFERENCE LAB" - only real hospital names from main headers.

STEP 2 - EXTRACT METADATA (only if NEW MAIN HEADER detected):
- Hospital Names: Only from main document title
- Effective Date: Date ranges in header
- LOB Type: Business line from header

STEP 3 - CHECK FOR CONTINUATION CODES:
Look at the TOP of the page for billing codes that continue from previous page.
These appear as codes without a service name, like:
"A9536, A9537, A9538, A9539, A9540, A9541, A9542, A9546, A9547, A9548, A9550, A9551..."

If you see such codes, extract them as continuation_codes.

STEP 4 - EXTRACT ALL COMPLETE TABLE ROWS:
For each Place of Service section, extract ALL complete rows:
- service: Exact text from Service column
- billing_codes: Complete text from Billing Codes column
- rates: Complete text from Rates column

CRITICAL RULES:
- Extract EVERY row you can see
- Copy text exactly as it appears
- If billing codes continue to next page, that's fine - just extract what's on this page
- Don't miss any complete rows

{format_instructions}

Extract all data from this page:
"""
        
        prompt = PromptTemplate(
            template=template,
            input_variables=[],
            partial_variables={"format_instructions": parser.get_format_instructions()}
        )
        
        return prompt, parser
    
    def process_page(self, image_path: str, page_num: int) -> Dict[str, Any]:
        """Process a single page with simple continuation handling."""
        print(f"Processing page {page_num}: {Path(image_path).name}")
        
        try:
            with open(image_path, 'rb') as f:
                image_bytes = f.read()
            
            mime_type = 'image/png' if image_path.lower().endswith('.png') else 'image/jpeg'
            
            prompt, parser = self.create_extraction_prompt()
            prompt_text = prompt.format()
            
            response = self.client.models.generate_content(
                model='gemini-2.0-flash',
                contents=[
                    types.Part.from_bytes(data=image_bytes, mime_type=mime_type),
                    prompt_text
                ]
            )
            
            parsed_data = parser.parse(response.text.strip())
            
            # Handle continuation codes
            if parsed_data.continuation_codes and self.context.last_incomplete_row:
                print(f"  Found continuation codes: {parsed_data.continuation_codes[:100]}...")
                # Merge with previous incomplete row
                self.context.last_incomplete_row['billing_codes'] += " " + parsed_data.continuation_codes
                # Save the completed row
                self.save_single_row(self.context.last_incomplete_row)
                self.context.last_incomplete_row = None
                print(f"  Merged continuation with previous row")
            
            # Update context if new document section
            if parsed_data.has_new_document_section and parsed_data.metadata:
                # Simple hospital name validation
                if parsed_data.metadata.hospital_names:
                    valid_hospitals = []
                    for hospital in parsed_data.metadata.hospital_names:
                        hospital_lower = hospital.lower()
                        if not any(invalid in hospital_lower for invalid in ['reference lab', 'lab', 'mhs']):
                            if any(valid in hospital_lower for valid in ['hospital', 'medical center', 'general']):
                                valid_hospitals.append(hospital)
                    
                    if valid_hospitals:
                        parsed_data.metadata.hospital_names = valid_hospitals
                        self.context.update_context(parsed_data.metadata)
                        print(f"New section - Hospitals: {self.context.current_hospital_names}")
            
            # Process all service sections
            total_rows_added = 0
            sections_processed = []
            
            for section in parsed_data.service_sections:
                for row in section.service_rows:
                    # Check if this row might continue to next page (very long billing codes)
                    billing_codes = row.get('billing_codes', '')
                    if len(billing_codes) > 300 and billing_codes.rstrip().endswith(','):
                        print(f"  Row might continue to next page: {row.get('service', 'Unknown')}")
                        self.context.last_incomplete_row = {
                            'service': row.get('service', ''),
                            'billing_codes': billing_codes,
                            'rates': row.get('rates', ''),
                            'place_of_service': section.place_of_service
                        }
                    else:
                        # Complete row - save it
                        self.save_single_row({
                            'service': row.get('service', ''),
                            'billing_codes': billing_codes,
                            'rates': row.get('rates', ''),
                            'place_of_service': section.place_of_service
                        })
                        total_rows_added += 1
                
                sections_processed.append({
                    'place_of_service': section.place_of_service,
                    'rows_added': len(section.service_rows)
                })
                print(f"  - {section.place_of_service}: {len(section.service_rows)} rows")
            
            return {
                'page_num': page_num,
                'has_new_section': parsed_data.has_new_document_section,
                'sections_found': [s['place_of_service'] for s in sections_processed],
                'total_rows_added': total_rows_added,
                'current_context': {
                    'hospitals': self.context.current_hospital_names,
                    'lob': self.context.current_lob,
                    'effective_date': self.context.current_effective_date
                }
            }
            
        except Exception as e:
            print(f"Error processing page {page_num}: {e}")
            return {
                'page_num': page_num,
                'error': str(e),
                'total_rows_added': 0
            }
    
    def save_single_row(self, row_data: Dict[str, str]) -> None:
        """Save a single row to CSV."""
        csv_row = [
            '; '.join(self.context.current_hospital_names) if self.context.current_hospital_names else 'N/A',
            self.context.current_lob or 'N/A',
            row_data.get('place_of_service', 'N/A'),
            row_data.get('service', 'N/A'),
            row_data.get('billing_codes', 'N/A'),
            row_data.get('rates', 'N/A'),
            self.context.current_effective_date or 'N/A'
        ]
        
        with open(self.csv_path, 'a', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(csv_row)
    
    def extract_from_pdf(self, pdf_path: str, dpi: int = 300) -> Dict[str, Any]:
        """Extract all data from PDF."""
        print(f"Starting extraction for: {pdf_path}")
        
        image_paths = pdf_to_images(pdf_path, self.output_dir, dpi, 'png')
        
        results = {
            'pdf_path': pdf_path,
            'total_pages': len(image_paths),
            'pages_processed': [],
            'total_rows_extracted': 0,
            'csv_file': str(self.csv_path)
        }
        
        for page_num, image_path in enumerate(image_paths, 1):
            page_result = self.process_page(image_path, page_num)
            results['pages_processed'].append(page_result)
            results['total_rows_extracted'] += page_result.get('total_rows_added', 0)
        
        print(f"\n=== EXTRACTION COMPLETE ===")
        print(f"Total pages processed: {results['total_pages']}")
        print(f"Total rows extracted: {results['total_rows_extracted']}")
        print(f"Output CSV: {results['csv_file']}")
        
        return results

# Usage
if __name__ == "__main__":
    pdf_file = "path/to/your/hospital_rates.pdf"
    
    from google import genai
    client = genai.Client(vertexai=True, project="your-project", location="us-central1")
    
    extractor = HospitalRateExtractor(client, output_dir="extracted_data")
    results = extractor.extract_from_pdf(pdf_file)
    
    print("Extraction completed successfully!")
