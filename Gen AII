import fitz  # PyMuPDF
import os
import csv
import json
import uuid
import logging
from pathlib import Path
from typing import List, Optional, Union, Dict, Any
from google.genai import types
import google.genai as genai
from langchain.prompts import PromptTemplate
from langchain.output_parsers import PydanticOutputParser, OutputFixingParser
from pydantic import BaseModel, Field
import pandas as pd

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('hospital_extraction.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def pdf_to_images(pdf_path, output_dir=None, dpi=150, image_format='png'):
    """Convert each page of a PDF to individual images."""
    pdf_path = Path(pdf_path)
    
    if not pdf_path.exists():
        raise FileNotFoundError(f"PDF file not found: {pdf_path}")
    
    if output_dir is None:
        output_dir = pdf_path.parent
    else:
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
    
    doc = fitz.open(pdf_path)
    image_paths = []
    
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        mat = fitz.Matrix(dpi/72, dpi/72)
        pix = page.get_pixmap(matrix=mat)
        
        output_filename = f"{pdf_path.stem}_page_{page_num + 1:03d}.{image_format}"
        output_path = output_dir / output_filename
        
        pix.save(str(output_path))
        image_paths.append(str(output_path))
        
        print(f"Saved page {page_num + 1} to {output_path}")
        if page_num == 20:
            print("Limiting to first 20 pages for large PDFs")
            break
    
    doc.close()
    return image_paths

class PageExtraction(BaseModel):
    """Model for extracting all data from a page."""
    has_table: bool = Field(description="Whether this page contains table data")
    
    # Document context
    effective_date: Optional[str] = Field(default=None, description="Effective date from top corner")
    hospital_names: Optional[str] = Field(default=None, description="Hospital names as single string")
    lob_type: Optional[str] = Field(default=None, description="Line of Business")
    place_of_service: Optional[str] = Field(default=None, description="INPATIENT, OUTPATIENT, INPATIENT CARVE OUT, OUTPATIENT CARVE OUT")
    
    # Table data - extract as-is
    table_rows: Optional[List[Dict[str, str]]] = Field(default=None, description="Raw table data with service, billing_codes, rates")
    
    # Page classification
    is_new_section: bool = Field(default=False, description="Whether this starts a new hospital/LOB section")
    is_continuation: bool = Field(default=False, description="Whether this continues previous table")

def validate_extraction(extraction: PageExtraction, page_num: int) -> List[str]:
    """Validate extracted data and return list of issues - focused on zero data loss."""
    issues = []
    
    # For new sections, ensure we have the key identifiers
    if extraction.is_new_section:
        if not extraction.hospital_names:
            issues.append(f"Page {page_num}: New section missing hospital names")
        if not extraction.lob_type:
            issues.append(f"Page {page_num}: New section missing LOB type")
    
    # Check for potential data loss scenarios
    if extraction.has_table and not extraction.table_rows:
        issues.append(f"Page {page_num}: CRITICAL - has_table=True but no table_rows extracted (potential data loss)")
    
    if extraction.table_rows:
        for i, row in enumerate(extraction.table_rows):
            # Only flag completely empty rows as potential issues
            if not any(row.values()):
                issues.append(f"Page {page_num}, Row {i+1}: Completely empty row detected")
            
            # Check for critical missing data that indicates extraction problems
            if not row.get('service') and not row.get('billing_codes') and not row.get('rates'):
                issues.append(f"Page {page_num}, Row {i+1}: CRITICAL - No data extracted for any field (potential data loss)")
    
    # Warn if we have continuation but no data (might indicate missed table)
    if extraction.is_continuation and not extraction.table_rows:
        issues.append(f"Page {page_num}: WARNING - Marked as continuation but no table data found")
    
    return issues

class DocumentState:
    """Track current document context."""
    
    def __init__(self):
        self.current_effective_date = None
        self.current_hospitals = None
        self.current_lob = None
        self.current_place_of_service = None
        self.current_csv_file = None
        self.current_csv_writer = None
        self.current_csv_path = None

class StateManager:
    """Manage document state and CSV output."""
    
    def __init__(self, output_dir: Path):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.state = DocumentState()
        self.csv_files_created = []
        
        # Simple columns - extract as-is
        self.columns = [
            'Hospital_Names', 'LOB', 'Place_of_Service', 'Service', 
            'Billing_Codes', 'Rates', 'Effective_Date'
        ]
    
    def needs_new_csv(self, hospitals: str, lob: str) -> bool:
        """Check if we need to start a new CSV file."""
        return (hospitals != self.state.current_hospitals or 
                lob != self.state.current_lob)
    
    def start_new_csv(self, hospitals: str, lob: str, effective_date: str):
        """Start new CSV file for new hospital/LOB combination."""
        logger.info(f"Starting new CSV for: {hospitals} - {lob}")
        
        # Close current CSV
        self.close_current_csv()
        
        # Update state
        self.state.current_hospitals = hospitals
        self.state.current_lob = lob
        self.state.current_effective_date = effective_date
        
        # Create CSV filename
        safe_hospitals = hospitals.replace(" ", "_").replace(",", "").replace("/", "_")[:50]
        safe_lob = lob.replace(" ", "_")
        csv_filename = f"{safe_hospitals}_{safe_lob}.csv"
        
        try:
            self.state.current_csv_path = self.output_dir / csv_filename
            self.state.current_csv_file = open(self.state.current_csv_path, 'w', newline='', encoding='utf-8')
            self.state.current_csv_writer = csv.writer(self.state.current_csv_file)
            self.state.current_csv_writer.writerow(self.columns)
            
            self.csv_files_created.append(str(self.state.current_csv_path))
            logger.info(f"Created CSV file: {csv_filename}")
            
        except Exception as e:
            logger.error(f"Failed to create CSV file {csv_filename}: {e}")
            raise
    
    def update_place_of_service(self, place_of_service: str):
        """Update current place of service."""
        self.state.current_place_of_service = place_of_service
        logger.info(f"Updated place of service: {place_of_service}")
    
    def add_table_rows(self, table_rows: List[Dict[str, str]]):
        """Add table rows to current CSV."""
        if not self.state.current_csv_writer:
            logger.warning("No active CSV file - cannot add data")
            return
        
        try:
            rows_added = 0
            for row in table_rows:
                csv_row = [
                    self.state.current_hospitals or "N/A",
                    self.state.current_lob or "N/A", 
                    self.state.current_place_of_service or "N/A",
                    row.get('service', 'N/A'),
                    row.get('billing_codes', 'N/A'),
                    row.get('rates', 'N/A'),
                    self.state.current_effective_date or "N/A"
                ]
                self.state.current_csv_writer.writerow(csv_row)
                rows_added += 1
            
            if self.state.current_csv_file:
                self.state.current_csv_file.flush()
            
            logger.info(f"Added {rows_added} rows to CSV")
            
        except Exception as e:
            logger.error(f"Failed to add table rows: {e}")
            raise
    
    def close_current_csv(self):
        """Close current CSV file."""
        if self.state.current_csv_file:
            try:
                self.state.current_csv_file.close()
                self.state.current_csv_file = None
                self.state.current_csv_writer = None
                logger.info(f"Closed CSV: {self.state.current_csv_path}")
            except Exception as e:
                logger.error(f"Error closing CSV file: {e}")

class SimpleExtractor:
    """Simplified extractor focused on data extraction."""
    
    def __init__(self, client, output_dir: str = None):
        self.client = client
        self.output_dir = Path(output_dir) if output_dir else Path.cwd()
        self.state_manager = StateManager(self.output_dir)
    
    def create_extraction_prompt(self) -> tuple:
        """Create prompt for data extraction."""
        base_parser = PydanticOutputParser(pydantic_object=PageExtraction)
        
        class GeminiLLM:
            def __init__(self, client):
                self.client = client
            
            def __call__(self, prompt: str) -> str:
                try:
                    response = self.client.models.generate_content(
                        model='gemini-2.0-flash',
                        contents=[prompt]
                    )
                    return response.text
                except Exception as e:
                    return f"Error: {e}"
        
        llm_wrapper = GeminiLLM(self.client)
        parser = OutputFixingParser.from_llm(parser=base_parser, llm=llm_wrapper)
        
        template = """
You are a hospital rate schedule data extraction specialist. Follow a strict TOP-TO-BOTTOM approach and extract EVERYTHING visible on this page. There must be ZERO data loss.

BASED ON SAMPLE DOCUMENTS ANALYSIS:
- Hospital names can be complex: "Mercy Springfield and Urbana Hospital Qualified Health Plan rates"
- LOB may be embedded in titles or not clearly labeled
- Sections include "INPATIENT RATES:", "INPATIENT CARVE OUT RATES:", "OUTPATIENT RATES:", "OUTPATIENT CARVE OUT RATES:"
- Tables have complex merged cells and variable structures
- Rate information can be in various formats (Case Rate, Per Diem, percentages)

TOP-TO-BOTTOM EXTRACTION PROCESS:

1. TOP RIGHT CORNER:
   - IGNORE first line (system name like "Mercy Health", "Memorial Health System")
   - EXTRACT second line (effective date like "Effective Date: 09/01/2024")

2. MAIN TITLE AREA (scan carefully):
   - Extract hospital facility names (hospitals NEVER end with "plan", "rates", "schedule")
   - Examples: "Mercy Springfield and Urbana Hospital", "Sistersville General Hospital"
   - LOB comes AFTER hospital names: "Qualified Health Plan", "Commercial", "Medicare" 
   - Common sense: If it says "Hospital XYZ Plan", then "Hospital" = facility, "XYZ Plan" = LOB

3. ALL SECTION HEADERS (scan entire page for ANY of these):
   - "INPATIENT RATES" or "INPATIENT RATES:"
   - "OUTPATIENT RATES" or "OUTPATIENT RATES:"
   - "INPATIENT CARVE OUT RATES" or "INPATIENT CARVE OUT RATES:"
   - "OUTPATIENT CARVE OUT RATES" or "OUTPATIENT CARVE OUT RATES:"
   - ANY other rate section headers
   - Extract the CURRENT section context for this page

4. ALL TABLES (extract from EVERY table structure found):
   - Handle merged cells by extracting the most complete information
   - Extract from tables with ANY column structure
   - Include tables with: Service/Billing Codes/Rates, Categories/Groupers/Rates, DRG/Revenue Codes/Rates
   - Do NOT skip rows because of formatting differences

FOR EACH TABLE ROW EXTRACT:
- service: Complete service description (DRG, Ambulatory Surgery, specific procedures, etc.)
- billing_codes: ALL codes exactly as shown (DRG Codes: 880-887, Revenue Codes: 190-192, etc.)
- rates: Complete rate information ($1,800.00 Per Diem, $9,600.00 Base Rate, 33% of Billed Charges, etc.)

HANDLE COMPLEX STRUCTURES:
- For merged cells: Extract the full merged content
- For nested information: Include all details
- For rate conditions: Include full text like "Applied based upon Medicare Weights"
- For percentage rates: Include complete text like "33% of Billed Charges"

PAGE CLASSIFICATION:
- is_new_section: True if NEW hospital name OR NEW LOB appears
- is_continuation: True if continuing tables from previous page
- has_table: True if ANY structured data table exists

CRITICAL SUCCESS FACTORS:
1. Extract EVERY visible table row regardless of format
2. Handle different table structures (3-column, 4-column, merged cells)
3. Capture ALL rate sections including carve out rates
4. Include complete rate descriptions with conditions
5. Don't lose data due to formatting variations

{format_instructions}

Perform complete extraction ensuring ZERO data loss from this specific hospital rate schedule format:
"""
        
        prompt = PromptTemplate(
            template=template,
            input_variables=[],
            partial_variables={"format_instructions": parser.get_format_instructions()}
        )
        
        return prompt, parser
    
    def process_page(self, image_path: str, page_num: int) -> Dict[str, Any]:
        """Process single page with error recovery."""
        logger.info(f"Processing page {page_num}: {Path(image_path).name}")
        
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                with open(image_path, 'rb') as f:
                    image_bytes = f.read()
                
                mime_type = 'image/png' if image_path.lower().endswith('.png') else 'image/jpeg'
                
                # Extract data from page
                prompt, parser = self.create_extraction_prompt()
                prompt_text = prompt.format()
                
                response = self.client.models.generate_content(
                    model='gemini-2.0-flash',
                    contents=[
                        types.Part.from_bytes(data=image_bytes, mime_type=mime_type),
                        prompt_text
                    ]
                )
                
                extraction = parser.parse(response.text.strip())
                
                # Validate extraction
                validation_issues = validate_extraction(extraction, page_num)
                if validation_issues:
                    logger.warning(f"Validation issues on page {page_num}: {validation_issues}")
                
                # Process extraction
                result = self.handle_extraction(extraction, page_num)
                logger.info(f"Successfully processed page {page_num}")
                return result
                
            except Exception as e:
                retry_count += 1
                logger.error(f"Attempt {retry_count} failed for page {page_num}: {e}")
                
                if retry_count >= max_retries:
                    logger.error(f"Failed to process page {page_num} after {max_retries} attempts")
                    return {
                        'page_num': page_num,
                        'success': False,
                        'error': str(e),
                        'retry_attempts': retry_count
                    }
                else:
                    logger.info(f"Retrying page {page_num} (attempt {retry_count + 1})")
                    continue
    
    def handle_extraction(self, extraction: PageExtraction, page_num: int) -> Dict[str, Any]:
        """Handle extracted data with validation and completeness tracking."""
        
        try:
            # Log what was found on this page
            logger.info(f"Page {page_num} extraction summary:")
            logger.info(f"  - Has table data: {extraction.has_table}")
            logger.info(f"  - Is new section: {extraction.is_new_section}")
            logger.info(f"  - Is continuation: {extraction.is_continuation}")
            logger.info(f"  - Hospital names: {extraction.hospital_names}")
            logger.info(f"  - LOB: {extraction.lob_type}")
            logger.info(f"  - Place of service: {extraction.place_of_service}")
            logger.info(f"  - Table rows found: {len(extraction.table_rows) if extraction.table_rows else 0}")
            
            # Check if we need new CSV file
            if extraction.is_new_section and extraction.hospital_names and extraction.lob_type:
                logger.info(f"Page {page_num}: Starting new section - {extraction.hospital_names}, {extraction.lob_type}")
                self.state_manager.start_new_csv(
                    extraction.hospital_names,
                    extraction.lob_type, 
                    extraction.effective_date
                )
            
            # Update place of service if provided
            if extraction.place_of_service:
                self.state_manager.update_place_of_service(extraction.place_of_service)
            
            # Add table data if present - EXTRACT ALL ROWS
            rows_added = 0
            if extraction.has_table and extraction.table_rows:
                logger.info(f"Page {page_num}: Extracting ALL {len(extraction.table_rows)} table rows")
                self.state_manager.add_table_rows(extraction.table_rows)
                rows_added = len(extraction.table_rows)
                
                # Log details of what was extracted
                for i, row in enumerate(extraction.table_rows):
                    logger.debug(f"  Row {i+1}: Service='{row.get('service', '')}', Codes='{row.get('billing_codes', '')}', Rates='{row.get('rates', '')}'")
            
            # Warn if page has table indicator but no data extracted
            if extraction.has_table and not extraction.table_rows:
                logger.warning(f"Page {page_num}: POTENTIAL DATA LOSS - has_table=True but no rows extracted")
            
            return {
                'page_num': page_num,
                'success': True,
                'has_table': extraction.has_table,
                'is_new_section': extraction.is_new_section,
                'hospital_names': extraction.hospital_names,
                'lob': extraction.lob_type,
                'place_of_service': extraction.place_of_service,
                'rows_added': rows_added,
                'extraction_complete': True
            }
            
        except Exception as e:
            logger.error(f"Failed to handle extraction for page {page_num}: {e}")
            return {
                'page_num': page_num,
                'success': False,
                'error': f"Handling failed: {e}",
                'extraction_complete': False
            }
    
    def extract_from_pdf(self, pdf_path: str, dpi: int = 150) -> Dict[str, Any]:
        """Extract from entire PDF with comprehensive logging."""
        logger.info(f"Starting extraction for PDF: {pdf_path}")
        
        try:
            # Convert PDF to images
            image_paths = pdf_to_images(pdf_path, self.output_dir, dpi)
            logger.info(f"Converted PDF to {len(image_paths)} images")
            
            # Process each page
            results = {
                'pdf_path': pdf_path,
                'total_pages': len(image_paths),
                'processed_pages': [],
                'failed_pages': [],
                'total_rows_extracted': 0
            }
            
            for page_num, image_path in enumerate(image_paths, 1):
                page_result = self.process_page(image_path, page_num)
                results['processed_pages'].append(page_result)
                
                if page_result.get('success', False):
                    results['total_rows_extracted'] += page_result.get('rows_added', 0)
                else:
                    results['failed_pages'].append(page_num)
                    logger.error(f"Page {page_num} failed: {page_result.get('error', 'Unknown error')}")
            
            # Close final CSV
            self.state_manager.close_current_csv()
            
            # Add summary
            results['csv_files_created'] = self.state_manager.csv_files_created
            results['total_csv_files'] = len(self.state_manager.csv_files_created)
            results['success_rate'] = (len(image_paths) - len(results['failed_pages'])) / len(image_paths) * 100
            
            # Log final summary
            logger.info(f"Extraction completed for {pdf_path}")
            logger.info(f"Total pages: {results['total_pages']}")
            logger.info(f"Failed pages: {len(results['failed_pages'])}")
            logger.info(f"Success rate: {results['success_rate']:.1f}%")
            logger.info(f"Total rows extracted: {results['total_rows_extracted']}")
            logger.info(f"CSV files created: {results['total_csv_files']}")
            
            if results['failed_pages']:
                logger.warning(f"Failed pages: {results['failed_pages']}")
            
            return results
            
        except Exception as e:
            logger.error(f"Critical error during PDF extraction: {e}")
            return {
                'pdf_path': pdf_path,
                'success': False,
                'error': str(e)
            }

# Usage
if __name__ == "__main__":
    pdf_file = r"path/to/your/hospital_rates.pdf"
    
    from google import genai
    client = genai.Client(vertexai=True, project="your-project", location="us-central1")
    
    extractor = SimpleExtractor(client, output_dir="extracted_rates")
    results = extractor.extract_from_pdf(pdf_file, dpi=300)
