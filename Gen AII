import fitz  # PyMuPDF
import os
import csv
import json
import uuid
from pathlib import Path
from typing import List, Optional, Union, Dict, Any
from google.genai import types
import google.genai as genai
from langchain.prompts import PromptTemplate
from langchain.output_parsers import PydanticOutputParser, OutputFixingParser
from langchain_core.exceptions import OutputParserException
from pydantic import BaseModel, Field
import pandas as pd

def pdf_to_images(pdf_path, output_dir=None, dpi=150, image_format='png'):
    """Convert each page of a PDF to individual images."""
    pdf_path = Path(pdf_path)
    
    if not pdf_path.exists():
        raise FileNotFoundError(f"PDF file not found: {pdf_path}")
    
    if output_dir is None:
        output_dir = pdf_path.parent
    else:
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
    
    doc = fitz.open(pdf_path)
    image_paths = []
    
    for page_num in range(len(doc)):
        page = doc.load_page(page_num)
        mat = fitz.Matrix(dpi/72, dpi/72)
        pix = page.get_pixmap(matrix=mat)
        
        output_filename = f"{pdf_path.stem}_page_{page_num + 1:03d}.{image_format}"
        output_path = output_dir / output_filename
        
        pix.save(str(output_path))
        image_paths.append(str(output_path))
        
        print(f"Saved page {page_num + 1} to {output_path}")
        if page_num == 50:  # Limit for large PDFs
            print("Limiting to first 50 pages for large PDFs")
            break
    
    doc.close()
    return image_paths

class DocumentMetadata(BaseModel):
    """Document-level metadata extraction."""
    effective_date: Optional[str] = Field(default=None, description="Effective date found in document")
    hospital_names: Optional[List[str]] = Field(default=None, description="List of hospital names from main title")
    lob_type: Optional[str] = Field(default=None, description="Line of Business type")

class ServiceSection(BaseModel):
    """Individual Place of Service section with its table data."""
    place_of_service: str = Field(description="INPATIENT, OUTPATIENT, INPATIENT CARVE OUT, OUTPATIENT CARVE OUT")
    service_rows: List[Dict[str, str]] = Field(description="List of service rows with service, billing_codes, rates exactly as they appear in table")

class ContinuationData(BaseModel):
    """Data for handling table continuations across pages."""
    is_continuation_page: bool = Field(description="Whether this page continues tables from previous page")
    continued_sections: List[Dict[str, str]] = Field(description="Data that continues from previous page with section_name, continued_content")
    new_rows: List[Dict[str, str]] = Field(description="New complete rows that start on this page")

class PageExtraction(BaseModel):
    """Complete page extraction including metadata and all sections."""
    has_new_document_section: bool = Field(description="Whether page starts new hospital/LOB section")
    metadata: Optional[DocumentMetadata] = Field(default=None, description="Document metadata if new section")
    service_sections: List[ServiceSection] = Field(description="All Place of Service sections found on this page")
    continuation_data: Optional[ContinuationData] = Field(default=None, description="Continuation handling for multi-page tables")
    is_continuation: bool = Field(default=False, description="Whether this continues tables from previous page")

class DocumentContext:
    """Tracks document-level context across pages."""
    def __init__(self):
        self.current_effective_date = None
        self.current_hospital_names = []
        self.current_lob = None
        self.pending_continuations = {}  # Track incomplete rows from previous page
        
    def update_context(self, metadata: DocumentMetadata):
        """Update context with new metadata."""
        if metadata:
            if metadata.effective_date:
                self.current_effective_date = metadata.effective_date
            if metadata.hospital_names:
                self.current_hospital_names = metadata.hospital_names
            if metadata.lob_type:
                self.current_lob = metadata.lob_type
    
    def add_pending_continuation(self, section_name: str, partial_row: Dict[str, str]):
        """Add a partial row that continues to next page."""
        self.pending_continuations[section_name] = partial_row
    
    def get_pending_continuation(self, section_name: str) -> Optional[Dict[str, str]]:
        """Get and remove pending continuation for a section."""
        return self.pending_continuations.pop(section_name, None)

class HospitalRateExtractor:
    """Main extractor class with improved multi-section detection."""
    
    def __init__(self, client, output_dir: str = None):
        self.client = client
        self.output_dir = Path(output_dir) if output_dir else Path.cwd()
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.context = DocumentContext()
        
        # Standard CSV columns - simplified to match table structure
        self.csv_columns = [
            'Hospital_Names', 'LOB', 'Place_of_Service', 'Service', 
            'Billing_Codes', 'Rates', 'Effective_Date'
        ]
        
        # Initialize main CSV file
        self.csv_path = self.output_dir / "extracted_hospital_rates.csv"
        with open(self.csv_path, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(self.csv_columns)
    
    def create_multi_section_prompt(self) -> tuple:
        """Create prompt for extracting ALL sections from a page."""
        base_parser = PydanticOutputParser(pydantic_object=PageExtraction)
        
        class GeminiLLM:
            def __init__(self, client):
                self.client = client
                
            def __call__(self, prompt: str) -> str:
                try:
                    response = self.client.models.generate_content(
                        model='gemini-2.0-flash',
                        contents=[prompt]
                    )
                    return response.text
                except Exception as e:
                    return f"Error: {e}"
        
        llm_wrapper = GeminiLLM(self.client)
        parser = OutputFixingParser.from_llm(parser=base_parser, llm=llm_wrapper)
        
        template = """
You are an expert at extracting ALL structured data from hospital rate schedule documents.

CRITICAL: Handle multi-page table continuations correctly!

STEP 1 - DETECT PAGE TYPE:
Determine if this is:
A) NEW document section with main header (e.g., "Sistersville General Hospital")
B) CONTINUATION page where table data continues from previous page
C) MIXED page with both continuation and new sections

STEP 2 - DETECT CONTINUATION PATTERNS:
Look for these continuation indicators:
- Billing codes that start mid-list (e.g., starting with "A9536, A9537..." instead of "HCPC Codes: A4641...")
- Table rows without service names (empty left column, only billing codes)
- Partial table structures that seem to continue from above

STEP 3 - EXTRACT METADATA (only if NEW MAIN HEADER detected):
- Hospital Names: ONLY from main document title (e.g., "Sistersville General Hospital") 
- NEVER extract random text like "MHS REFERENCE LAB"
- Effective Date: Date ranges in header (e.g., "6/1/2025 through 5/31/2026")
- LOB Type: Business line from header (COMMERCIAL, MEDICARE, etc.)

STEP 4 - HANDLE CONTINUATION DATA:
If you see continuation patterns:
1. Identify which service the continuation belongs to
2. Extract the continued billing codes or other data
3. Mark it as continuation data with the service name

EXAMPLE CONTINUATION:
If you see codes "A9536, A9537, A9538, A9539, A9540..." at the top of a page without a service name, this is likely continuation of "Imaging Enhancing Substance" from the previous page.

STEP 5 - EXTRACT COMPLETE NEW ROWS:
For each complete new table row (that has service name + billing codes + rates):
- service: Exact text from Service column
- billing_codes: Complete text from Billing Codes column
- rates: Complete text from Rates column

STEP 6 - STRUCTURE THE OUTPUT:
Return:
- continuation_data: Any billing codes or data that continues from previous page
- service_sections: Complete new rows that start and end on this page

CRITICAL RULES:
1. If you see billing codes without a service name, it's likely a continuation
2. Don't lose any billing codes - capture both continuation codes AND new complete rows
3. Identify which service the continuation codes belong to (usually the last service from previous page)

EXAMPLE - Page with continuation:
Continuation codes: "A9536, A9537, A9538...Q9983" (for Imaging Enhancing Substance)
New complete row: "Physical, Occupational and Speech Therapy | CPT4 Codes: 07911, 92507-92508... | 85.1% of Billed Charges"

{format_instructions}

Extract all data including continuations from this page:
"""
        
        prompt = PromptTemplate(
            template=template,
            input_variables=[],
            partial_variables={"format_instructions": parser.get_format_instructions()}
        )
        
        return prompt, parser
    
    def process_page(self, image_path: str, page_num: int) -> Dict[str, Any]:
        """Process a single page extracting ALL sections."""
        print(f"Processing page {page_num}: {Path(image_path).name}")
        
        try:
            with open(image_path, 'rb') as f:
                image_bytes = f.read()
            
            mime_type = 'image/png' if image_path.lower().endswith('.png') else 'image/jpeg'
            
            # Extract ALL sections using AI with error checking
            prompt, parser = self.create_multi_section_prompt()
            prompt_text = prompt.format()
            
            response = self.client.models.generate_content(
                model='gemini-2.0-flash',
                contents=[
                    types.Part.from_bytes(data=image_bytes, mime_type=mime_type),
                    prompt_text
                ]
            )
            
            print(f"Raw AI response for page {page_num}:")
            print(response.text[:500] + "..." if len(response.text) > 500 else response.text)
            
            parsed_data = parser.parse(response.text.strip())
            
            # Update context ONLY if new document section with valid hospital names
            if parsed_data.has_new_document_section and parsed_data.metadata:
                # Validate hospital names - reject system names and random text
                if parsed_data.metadata.hospital_names:
                    valid_hospitals = []
                    for hospital in parsed_data.metadata.hospital_names:
                        # Filter out system names and random text
                        hospital_lower = hospital.lower()
                        if not any(invalid_word in hospital_lower for invalid_word in 
                                 ['health system', 'health', 'memorial health system', 'mercy health', 
                                  'mhs reference lab', 'reference lab', 'lab']):
                            # Also check it contains "hospital" or recognizable hospital terms
                            if any(valid_word in hospital_lower for valid_word in 
                                 ['hospital', 'medical center', 'general', 'memorial', 'regional']):
                                valid_hospitals.append(hospital)
                    
                    if valid_hospitals:
                        # Only update if we have valid hospital names
                        old_metadata = parsed_data.metadata
                        old_metadata.hospital_names = valid_hospitals
                        self.context.update_context(old_metadata)
                        print(f"New section detected - Hospitals: {self.context.current_hospital_names}, LOB: {self.context.current_lob}")
                    else:
                        print(f"Ignoring invalid hospital extraction '{parsed_data.metadata.hospital_names}', keeping current context: {self.context.current_hospital_names}")
                else:
                    # Update other metadata but keep existing hospital names
                    if parsed_data.metadata.lob_type:
                        self.context.current_lob = parsed_data.metadata.lob_type
                    if parsed_data.metadata.effective_date:
                        self.context.current_effective_date = parsed_data.metadata.effective_date
            
            # Process continuation data first
            if parsed_data.continuation_data and parsed_data.continuation_data.continued_sections:
                for continuation in parsed_data.continuation_data.continued_sections:
                    section_name = continuation.get('section_name', 'UNKNOWN')
                    continued_content = continuation.get('continued_content', '')
                    
                    print(f"  Found continuation for '{section_name}': {continued_content[:100]}...")
                    
                    # Try to merge with pending continuation from previous page
                    pending = self.context.get_pending_continuation(section_name)
                    if pending:
                        # Merge the billing codes
                        existing_codes = pending.get('billing_codes', '')
                        merged_codes = f"{existing_codes} {continued_content}".strip()
                        
                        # Create complete row with merged data
                        complete_row = {
                            'service': pending.get('service', ''),
                            'billing_codes': merged_codes,
                            'rates': pending.get('rates', '')
                        }
                        
                        # Save the complete row
                        self.save_single_row(complete_row, section_name)
                        total_rows_added += 1
                        print(f"  Merged continuation: {complete_row['service']} with {len(merged_codes)} chars of billing codes")
            
            # Process ALL service sections with detailed logging
            if not parsed_data.service_sections:
                print(f"  Found {len(parsed_data.service_sections)} service sections")
            
            for i, section in enumerate(parsed_data.service_sections):
                print(f"  Processing section {i+1}: {section.place_of_service}")
                
                if not section.service_rows:
                    print(f"    Section has 0 rows")
                else:
                    print(f"    Found {len(section.service_rows)} rows")
                
                # Check for incomplete rows that might continue to next page
                for j, row in enumerate(section.service_rows):
                    service_text = row.get('service', 'MISSING')
                    billing_text = row.get('billing_codes', 'MISSING')
                    rates_text = row.get('rates', 'MISSING')
                    
                    # Check if this row seems incomplete (very long billing codes that might continue)
                    if len(billing_text) > 200 and billing_text.endswith(','):
                        print(f"    Row {j+1} might continue to next page (ends with comma)")
                        # Store as pending continuation
                        self.context.add_pending_continuation(section.place_of_service, row)
                    
                    print(f"    Row {j+1}:")
                    print(f"      Service: '{service_text}'")
                    print(f"      Billing: '{billing_text[:100]}{'...' if len(billing_text) > 100 else ''}'")
                    print(f"      Rates: '{rates_text}'")
                
                rows_added = self.save_section_data(section)
                total_rows_added += rows_added
                sections_processed.append({
                    'place_of_service': section.place_of_service,
                    'rows_added': rows_added
                })
                print(f"  - {section.place_of_service}: {rows_added} rows added to CSV")
            
            return {
                'page_num': page_num,
                'has_new_section': parsed_data.has_new_document_section,
                'sections_found': [s['place_of_service'] for s in sections_processed],
                'total_rows_added': total_rows_added,
                'sections_detail': sections_processed,
                'current_context': {
                    'hospitals': self.context.current_hospital_names,
                    'lob': self.context.current_lob,
                    'effective_date': self.context.current_effective_date
                }
            }
            
        except Exception as e:
            print(f"Error processing page {page_num}: {e}")
            return {
                'page_num': page_num,
                'error': str(e),
                'total_rows_added': 0,
                'sections_found': []
            }
    
    def save_single_row(self, row_data: Dict[str, str], place_of_service: str) -> None:
        """Save a single completed row to CSV."""
        csv_row = [
            '; '.join(self.context.current_hospital_names) if self.context.current_hospital_names else 'N/A',
            self.context.current_lob or 'N/A',
            place_of_service,
            row_data.get('service', 'N/A'),
            row_data.get('billing_codes', 'N/A'),
            row_data.get('rates', 'N/A'),
            self.context.current_effective_date or 'N/A'
        ]
        
        # Append to main CSV
        with open(self.csv_path, 'a', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow(csv_row)
    
    def save_section_data(self, section: ServiceSection) -> int:
        """Save extracted section data to CSV exactly as extracted."""
        if not section.service_rows:
            return 0
            
        rows_to_write = []
        
        for row_data in section.service_rows:
            csv_row = [
                '; '.join(self.context.current_hospital_names) if self.context.current_hospital_names else 'N/A',
                self.context.current_lob or 'N/A',
                section.place_of_service,
                row_data.get('service', 'N/A'),
                row_data.get('billing_codes', 'N/A'),
                row_data.get('rates', 'N/A'),
                self.context.current_effective_date or 'N/A'
            ]
            rows_to_write.append(csv_row)
        
        # Append to main CSV
        with open(self.csv_path, 'a', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerows(rows_to_write)
        
        return len(rows_to_write)
    
    def extract_from_pdf(self, pdf_path: str, dpi: int = 300) -> Dict[str, Any]:
        """Extract all data from PDF with multi-section awareness."""
        print(f"Starting extraction for: {pdf_path}")
        
        # Convert PDF to images
        image_paths = pdf_to_images(pdf_path, self.output_dir, dpi, 'png')
        
        # Process each page
        results = {
            'pdf_path': pdf_path,
            'total_pages': len(image_paths),
            'pages_processed': [],
            'total_rows_extracted': 0,
            'sections_summary': {},
            'csv_file': str(self.csv_path)
        }
        
        for page_num, image_path in enumerate(image_paths, 1):
            page_result = self.process_page(image_path, page_num)
            results['pages_processed'].append(page_result)
            results['total_rows_extracted'] += page_result.get('total_rows_added', 0)
            
            # Track sections found
            for section in page_result.get('sections_found', []):
                if section not in results['sections_summary']:
                    results['sections_summary'][section] = 0
                results['sections_summary'][section] += 1
        
        # Summary
        print(f"\n=== EXTRACTION COMPLETE ===")
        print(f"Total pages processed: {results['total_pages']}")
        print(f"Total rows extracted: {results['total_rows_extracted']}")
        print(f"Sections found: {results['sections_summary']}")
        print(f"Output CSV: {results['csv_file']}")
        
        return results

# Usage
if __name__ == "__main__":
    pdf_file = "path/to/your/hospital_rates.pdf"
    
    # Initialize Google AI client
    from google import genai
    client = genai.Client(vertexai=True, project="your-project", location="us-central1")
    
    # Create extractor and process PDF
    extractor = HospitalRateExtractor(client, output_dir="extracted_data")
    results = extractor.extract_from_pdf(pdf_file)
    
    print("Extraction completed successfully!")
