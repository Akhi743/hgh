import fitz  # PyMuPDF
import os
import csv
import json
import logging
from pathlib import Path
from typing import List, Optional, Dict, Any

# Pydantic is used for data modeling and validation
from pydantic import BaseModel, Field, ValidationError

# Google GenAI for interacting with the Gemini API
import google.generativeai as genai
from google.genai import types

# Setup logging to file and console
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('contract_extraction.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ==========================================
# 1. DATA MODELS (Pydantic)
# Defines the expected structure of the extracted data.
# ==========================================

class ContractRow(BaseModel):
    """Represents a single row of data from a contract table."""
    hospital_name: str = Field(description="The specific name of the hospital or facility in a table row.")
    charge_master_limit: Optional[str] = Field(None, description="The contractual charge master limit for this facility (e.g., '4.5%', '2.75% (CPI)').")
    charge_master_increase: Optional[str] = Field(None, description="The charge master increase percentage for this facility (e.g., '7.0%').")
    effective_date: Optional[str] = Field(None, description="The effective date for this specific facility if provided in the row.")

class ContractPageExtraction(BaseModel):
    """
    Defines the structure for all data to be extracted from a single page.
    This includes document-level fields and a list of table rows.
    """
    # Document-level fields that might be found on any page
    notification_date: Optional[str] = Field(None, description="The primary date of the letter or notification (e.g., 'December 17, 2024').")
    contract_duration: Optional[str] = Field(None, description="The term or duration of the agreement (e.g., '12-month period').")
    
    # Table rows found on the page
    contract_rows: List[ContractRow] = Field(default_factory=list, description="A list of all contract table rows found on the page.")

# ==========================================
# 2. HIERARCHICAL STATE MANAGER
# Manages and applies context (state) across multiple pages of a single document.
# ==========================================

class ContractStateManager:
    """
    Manages the state of document-level fields during extraction.
    This allows information found on one page (e.g., a notification date)
    to be applied to data extracted from subsequent pages.
    """
    
    def __init__(self):
        self.columns = [
            "Hospital Name", "Charge Master Limit", "Charge Master Increase",
            "Effective Date", "Contract Duration", "Notification Date",
            "Document Source"
        ]
        self._csv_file = None
        self._csv_writer = None
        self.reset_document_state()
    
    def reset_document_state(self):
        """Resets the stored state, typically called for each new PDF."""
        # Internal state uses None for "not found"
        self.notification_date: Optional[str] = None
        self.contract_duration: Optional[str] = None
        logger.info("Document state has been reset.")
    
    def start_csv(self, output_path: Path):
        """Creates and opens a new CSV file for writing results."""
        output_path.parent.mkdir(parents=True, exist_ok=True)
        self._csv_file = open(output_path, 'w', newline='', encoding='utf-8')
        self._csv_writer = csv.writer(self._csv_file)
        self._csv_writer.writerow(self.columns)
        logger.info(f"CSV file started at: {output_path}")
    
    def update_from_page(self, extraction: ContractPageExtraction):
        """
        Updates the document-level state from a page's extraction result.
        It only stores the *first* instance of a document-level field found.
        """
        if extraction.notification_date and self.notification_date is None:
            self.notification_date = extraction.notification_date
            logger.info(f"Stored document-level notification date: '{self.notification_date}'")
            
        if extraction.contract_duration and self.contract_duration is None:
            self.contract_duration = extraction.contract_duration
            logger.info(f"Stored document-level contract duration: '{self.contract_duration}'")

    def write_rows(self, extraction: ContractPageExtraction, document_name: str, page_num: int) -> int:
        """
        Writes extracted rows to the CSV file, enriching them with the stored document-level state.
        """
        if not self._csv_writer:
            return 0
        
        rows_written = 0
        for row in extraction.contract_rows:
            # Use stored document-level values, defaulting to "N/A" for the CSV
            csv_row = [
                row.hospital_name,
                row.charge_master_limit or "N/A",
                row.charge_master_increase or "N/A",
                row.effective_date or "N/A",
                self.contract_duration or "N/A",
                self.notification_date or "N/A",
                document_name
            ]
            self._csv_writer.writerow(csv_row)
            rows_written += 1
        
        if self._csv_file:
            self._csv_file.flush() # Ensure data is written to disk immediately
            
        if rows_written > 0:
            logger.info(f"Wrote {rows_written} row(s) to CSV from page {page_num}.")
        return rows_written
    
    def get_current_state(self) -> Dict[str, Any]:
        """Returns the current stored document-level state for debugging."""
        return {
            'notification_date': self.notification_date,
            'contract_duration': self.contract_duration
        }
        
    def close_csv(self):
        """Closes the CSV file if it is open."""
        if self._csv_file:
            self._csv_file.close()
            self._csv_file = None
            self._csv_writer = None
            logger.info("CSV file has been closed.")

# ==========================================
# 3. PDF PROCESSING UTILITY
# Converts PDF files into images for visual analysis by the AI model.
# ==========================================

def pdf_to_images(pdf_path: Path, output_dir: Path, dpi: int = 200) -> List[Path]:
    """
    Converts each page of a PDF into a high-resolution PNG image.
    
    Args:
        pdf_path: Path to the source PDF file.
        output_dir: Directory to save the output images.
        dpi: Dots per inch for image resolution.
        
    Returns:
        A list of paths to the created image files.
    """
    if not pdf_path.exists():
        logger.error(f"PDF file not found: {pdf_path}")
        raise FileNotFoundError(f"PDF file not found: {pdf_path}")
    
    output_dir.mkdir(parents=True, exist_ok=True)
    image_paths = []
    
    try:
        doc = fitz.open(pdf_path)
        logger.info(f"Converting '{pdf_path.name}' ({len(doc)} pages) to images at {dpi} DPI...")
        
        for i, page in enumerate(doc):
            mat = fitz.Matrix(dpi / 72, dpi / 72)
            pix = page.get_pixmap(matrix=mat)
            
            output_path = output_dir / f"{pdf_path.stem}_page_{i + 1:03d}.png"
            pix.save(output_path)
            image_paths.append(output_path)
            
        doc.close()
        logger.info(f"Successfully converted {len(image_paths)} pages.")
        
    except Exception as e:
        logger.error(f"Failed during PDF to image conversion for '{pdf_path.name}': {e}")
        raise
        
    return image_paths

# ==========================================
# 4. CONTRACT EXTRACTOR CLASS
# The main class that orchestrates the entire extraction process.
# ==========================================

class HealthcareContractExtractor:
    """
    Extracts structured data from healthcare contract PDFs by processing
    each page and using a state manager to maintain document-level context.
    """
    
    def __init__(self, client, output_dir: str = "contract_extraction"):
        self.client = client
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.state_manager = ContractStateManager()
        self.extraction_prompt = self._build_extraction_prompt()

    def _build_extraction_prompt(self) -> str:
        """Builds a robust, structured prompt for the AI model."""
        json_schema = json.dumps(ContractPageExtraction.model_json_schema(), indent=2)

        return f"""
You are an expert data extraction AI specializing in healthcare contracts.
Your task is to analyze the provided image of a document page and extract information precisely according to the rules and JSON schema below.

### EXTRACTION RULES

1.  **Analyze the Entire Page:** Review the whole page to find all requested fields.
2.  **Follow the Schema:** Your output MUST be a valid JSON object that conforms to the provided schema.
3.  **Exact Data:** Extract text exactly as it appears in the document. Do not correct or infer values.
4.  **Row Integrity:** For tables, ensure all data in a single `ContractRow` object comes from the *same* physical row in the table. Do not mix data between rows.
5.  **Null for Missing Fields:** If any field is not found on the page, its value must be `null` in the JSON output. Do not omit the key.
6.  **Extract All Rows:** Create a `ContractRow` object for EVERY row found in any relevant table. If no tables are present, the `contract_rows` array should be empty.

### FIELD DEFINITIONS

* `notification_date`: The main date of the letter (e.g., "December 17, 2024"). Extract only if found on this page.
* `contract_duration`: The length of the agreement (e.g., "12-month period"). Extract only if found on this page.
* `contract_rows`: An array of objects, where each object represents one row from a table of facilities and their rates.
    * `hospital_name`: The name of the facility from the row.
    * `charge_master_limit`: The limit percentage from that specific row.
    * `charge_master_increase`: The increase percentage from that specific row.
    * `effective_date`: The effective date, if present in that specific row.

### JSON OUTPUT SCHEMA
{json_schema}
"""

    def process_page(self, image_path: Path, page_num: int, document_name: str) -> Optional[ContractPageExtraction]:
        """Processes a single page image to extract data."""
        logger.info(f"--- Processing Page {page_num} of {document_name} ---")
        logger.info(f"Current State: {self.state_manager.get_current_state()}")
        response_text = ""
        
        try:
            with open(image_path, 'rb') as f:
                image_bytes = f.read()
            
            mime_type = 'image/png' if image_path.suffix.lower() == '.png' else 'image/jpeg'
            
            logger.info("Sending extraction request to Gemini...")
            
            # Use genai.generate_content directly for Vertex AI
            response = self.client.generate_content(
                model='gemini-1.5-flash',
                contents=[
                    self.extraction_prompt,
                    types.Part.from_bytes(data=image_bytes, mime_type=mime_type)
                ]
            )
            
            response_text = response.text.strip()
            logger.info(f"Received raw JSON response from model: {response_text[:500]}...")
            
            extraction = ContractPageExtraction.model_validate_json(response_text)
            logger.info(f"Successfully parsed and validated extracted data for page {page_num}.")
            
            self.state_manager.update_from_page(extraction)
            self.state_manager.write_rows(extraction, document_name, page_num)
            
            return extraction
            
        except (json.JSONDecodeError, ValidationError) as e:
            logger.error(f"CRITICAL: Failed to parse or validate AI response on page {page_num}. Error: {e}")
            logger.error(f"Problematic response text: {response_text}")
        except Exception as e:
            logger.error(f"CRITICAL: An unexpected error occurred on page {page_num}: {e}")
            import traceback
            logger.error(f"Full traceback: {traceback.format_exc()}")
            
        return None

    def extract_from_pdf(self, pdf_path: str, dpi: int = 200) -> Dict[str, Any]:
        """Orchestrates the full extraction process for a single PDF."""
        pdf_path = Path(pdf_path)
        logger.info(f"======== STARTING EXTRACTION FOR: {pdf_path.name} ========")
        
        total_rows = 0
        successful_pages = 0
        
        try:
            self.state_manager.reset_document_state()
            image_output_dir = self.output_dir / pdf_path.stem / "images"
            image_paths = pdf_to_images(pdf_path, image_output_dir, dpi)
            total_pages = len(image_paths)
            
            csv_path = self.output_dir / pdf_path.stem / f"{pdf_path.stem}_extracted_data.csv"
            self.state_manager.start_csv(csv_path)

            for i, image_path in enumerate(image_paths, 1):
                extraction = self.process_page(image_path, i, pdf_path.name)
                if extraction:
                    successful_pages += 1
                    total_rows += len(extraction.contract_rows)
            
            results = {
                'pdf_name': pdf_path.name,
                'total_pages': total_pages,
                'successful_pages': successful_pages,
                'total_rows_extracted': total_rows,
                'csv_path': str(csv_path),
                'final_state': self.state_manager.get_current_state(),
                'success_rate': f"{successful_pages}/{total_pages}"
            }
            logger.info(f"======== EXTRACTION COMPLETE FOR: {pdf_path.name} ========")
            logger.info(f"Results: {results}")
            return results
            
        except Exception as e:
            logger.critical(f"Process failed for PDF '{pdf_path.name}'. Error: {e}")
            raise
        finally:
            self.state_manager.close_csv()


# ==========================================
# CLIENT SETUP
# ==========================================

def create_client():
    """Create Google Cloud client with Vertex AI"""
    try:
        # Configure genai for Vertex AI
        genai.configure(
            project="anbc-hcb-dev",  # Replace with your project ID
            location="us-central1"   # Replace with your location
        )
        
        # For Vertex AI, we don't need to create a separate client
        # genai functions work directly after configure
        logger.info("Configured Vertex AI successfully")
        return genai  # Return the genai module itself
    except Exception as e:
        logger.error(f"Failed to configure Vertex AI: {e}")
        raise

# ==========================================
# MAIN FUNCTION
# ==========================================

def extract_healthcare_contract(pdf_path: str, output_dir: str = "hierarchical_contract_extraction") -> Dict:
    """Extract healthcare contract data using hierarchical state management"""
    
    # Create client and extractor
    client = create_client()
    extractor = HealthcareContractExtractor(client=client, output_dir=output_dir)
    
    # Extract using hierarchical approach
    results = extractor.extract_from_pdf(pdf_path)
    
    return results

# ==========================================
# USAGE
# ==========================================

if __name__ == "__main__":
    # Your PDF file path
    pdf_file = "Aultman Hospital.pdf"  # Replace with your actual PDF path
    
    try:
        # Extract contract data using hierarchical approach
        results = extract_healthcare_contract(pdf_file)
        
        # Print results
        print("\n" + "="*60)
        print("HIERARCHICAL CONTRACT EXTRACTION RESULTS")
        print("="*60)
        print(f"PDF: {results['pdf_name']}")
        print(f"Pages processed: {results['success_rate']}")
        print(f"Total rows: {results['total_rows_extracted']}")
        print(f"Final state: {results['final_state']}")
        print(f"CSV output: {results['csv_path']}")
        print("="*60)
        print("Check contract_extraction.log for detailed logs")
        
    except Exception as e:
        logger.critical(f"Extraction failed: {e}")
        print(f"Error: {e}")
