if __name__ == "__main__":
    pdf_file_path = "Aultman Hospital.pdf"  # Replace with your PDF path
    
    try:
        # Run the healthcare contract extraction
        results = run_healthcare_document_extraction(pdf_file_path)
        
        print("\n" + "="*70)
        print("HEALTHCARE CONTRACT EXTRACTION RESULTS")
        print("="*70)
        print(f"Document Type: {results.get('document_type', 'N/A')}")
        print(f"PDF: {results.get('pdf_name', 'N/A')}")
        print(f"Total Pages in PDF: {results.get('total_pages_in_pdf', 'N/A')}")
        print(f"Pages Processed: {results.get('pages_processed', 'N/A')}")
        print(f"Total Rows Extracted: {results.get('total_rows', 'N/A')}")
        print(f"CSV Output: {results.get('csv_path', 'N/A')}")
        print("="*70)
        print("\nKey Features:")
        print("1.import fitz
import csv
import json
import logging
from pathlib import Path
from typing import List, Optional, Dict, Any, Type
from pydantic import BaseModel, Field, create_model, ValidationError
from google.genai import types
import google.genai as genai

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('document_extraction.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ==========================================
# CONFIGURATION SYSTEM
# ==========================================

class ExtractionField(BaseModel):
    """Configuration for a single extraction field."""
    name: str
    description: str
    field_type: str = "Optional[str]"
    examples: List[str] = Field(default_factory=list)

class DocumentTypeConfig(BaseModel):
    """Configuration for a specific document type, making the extractor generic."""
    document_type: str
    description: str
    document_fields: List[ExtractionField] = Field(default_factory=list)
    row_fields: List[ExtractionField] = Field(default_factory=list)
    detection_prompt: str
    extraction_instructions: str
    csv_columns: List[str]

HEALTHCARE_CONTRACT_CONFIG = DocumentTypeConfig(
    document_type="HealthcareContract",
    description="Aetna healthcare contract for charge master increases.",
    document_fields=[
        # Extract notification date from email body text, not letter header
        ExtractionField(
            name="notification_received_date", 
            description="The date mentioned in the sentence 'We have received your email dated [DATE]' in the letter body - NOT the letter header date.", 
            examples=["December 6, 2024", "11/13/2024"]
        ),
        ExtractionField(name="contract_duration", description="The contract term or period.", examples=["12-month period"]),
        # Moved effective_date to document-level since it applies to all facilities in the letter
        ExtractionField(name="effective_date", description="The effective date for all charge master increases mentioned in this letter.", examples=["January 1, 2025", "1/1/2025"])
    ],
    row_fields=[
        ExtractionField(name="hospital_name", description="The name of the facility from a table row."),
        # Capture complete limit information including both Commercial and Medicare rates
        ExtractionField(
            name="charge_master_limit", 
            description="The complete contractual limit information including both Commercial and Medicare rates if present. Format as 'Commercial X%, Medicare Y%' or single percentage if only one rate.",
            examples=["Commercial 4%, Medicare 3%", "CP 4.5%/ME 3.0%", "4%"]
        ),
        ExtractionField(name="charge_master_increase", description="The increase percentage from the same row.")
    ],
    detection_prompt="""Analyze the image. Is this a formal letter from Aetna about a 'charge master increase' where they state 'we have received' a notification? Answer only YES or NO.""",
    extraction_instructions="""Extract ALL relevant data from this Aetna charge master increase notification letter. 

CRITICAL INSTRUCTIONS:
1. For 'notification_received_date': Look for the phrase 'We have received your email dated [DATE]' in the letter body paragraphs - DO NOT use the letter header date.
2. For 'charge_master_limit': Capture the COMPLETE limit information including both Commercial and Medicare rates when present. Examples:
   - If you see "Commercial 4% and Medicare 3%" → extract as "Commercial 4%, Medicare 3%"
   - If you see "CP 4.5%/ME 3.0%" → extract as "CP 4.5%/ME 3.0%"
   - If only one rate is shown → extract that single rate
3. For 'effective_date': Extract the effective date that applies to all facilities in this letter - usually mentioned as "effective [DATE]" or "to become effective [DATE]"
4. Create one row for each facility mentioned, ensuring no fields are left empty when information is available.""",
    csv_columns=[
        "hospital_name", "charge_master_limit", "charge_master_increase", # Row Fields
        "notification_received_date", "contract_duration", "effective_date", # Document Fields
        "document_source", "page_number" # Metadata
    ]
)

# ==========================================
# DYNAMIC COMPONENT FACTORY
# ==========================================

def create_dynamic_models(config: DocumentTypeConfig) -> (Type[BaseModel], Type[BaseModel]):
    """Dynamically creates Pydantic models for row and document data based on the config."""
    
    # Create the RowModel
    row_model_fields = {
        field.name: (Optional[str], Field(None, description=field.description))
        for field in config.row_fields
    }
    RowModel = create_model(f"{config.document_type}Row", **row_model_fields)

    # Create the DocumentExtractionModel
    doc_model_fields = {
        field.name: (Optional[str], Field(None, description=field.description))
        for field in config.document_fields
    }
    doc_model_fields['rows'] = (List[RowModel], Field(default_factory=list, description="All extracted table rows."))
    DocumentModel = create_model(f"{config.document_type}Extraction", **doc_model_fields)
    
    logger.info(f"Dynamically created Pydantic models for document type: {config.document_type}")
    return RowModel, DocumentModel

# ==========================================
# DYNAMIC STATE MANAGER
# ==========================================

class StateManager:
    """State manager that treats multi-page letters as single documents with two-pass processing."""
    def __init__(self, config: DocumentTypeConfig):
        self.config = config
        self.columns = config.csv_columns
        self._csv_file = None
        self._csv_writer = None
        self.reset_document_state()

    def reset_document_state(self):
        """Resets state for a new document and initializes temporary storage."""
        self.doc_state = {field.name: None for field in self.config.document_fields}
        self.temp_rows = []  # Store all rows temporarily before writing
        logger.info(f"State manager reset for new document. Tracking fields: {list(self.doc_state.keys())}")

    def start_csv(self, output_path: Path):
        output_path.parent.mkdir(parents=True, exist_ok=True)
        self._csv_file = open(output_path, 'w', newline='', encoding='utf-8')
        self._csv_writer = csv.writer(self._csv_file)
        self._csv_writer.writerow(self.columns)

    def collect_page_data(self, extraction_data: BaseModel, document_name: str, page_num: int):
        """First pass: Collect all data from pages without writing to CSV yet."""
        # Update document-level fields (any page can contribute)
        for field_name in self.doc_state.keys():
            extracted_value = getattr(extraction_data, field_name, None)
            if extracted_value and self.doc_state[field_name] is None:
                self.doc_state[field_name] = extracted_value
                logger.info(f"Collected document-level field '{field_name}': '{extracted_value}' from page {page_num}")

        # Collect all rows from this page
        for row_data in extraction_data.rows:
            row_dict = {}
            # Add row-level data
            for field in self.config.row_fields:
                row_dict[field.name] = getattr(row_data, field.name, "N/A")
            # Add metadata
            row_dict["document_source"] = document_name
            row_dict["page_number"] = page_num
            
            self.temp_rows.append(row_dict)
            
        logger.info(f"Collected {len(extraction_data.rows)} rows from page {page_num}. Total rows so far: {len(self.temp_rows)}")

    def write_all_collected_data(self) -> int:
        """Second pass: Write all collected rows with complete document-level information."""
        total_rows_written = 0
        
        logger.info(f"Writing all collected data with complete document-level fields:")
        for field_name, value in self.doc_state.items():
            logger.info(f"  {field_name}: {value}")
        
        # Write all rows with complete document-level information
        for row_dict in self.temp_rows:
            csv_row_data = {}
            
            # Add row-level data
            for field in self.config.row_fields:
                csv_row_data[field.name] = row_dict.get(field.name, "N/A")
            
            # Add complete document-level data
            for field in self.config.document_fields:
                csv_row_data[field.name] = self.doc_state.get(field.name, "N/A")
            
            # Add metadata
            csv_row_data["document_source"] = row_dict.get("document_source", "N/A")
            csv_row_data["page_number"] = row_dict.get("page_number", "N/A")
            
            # Write in the order specified by config
            self._csv_writer.writerow([csv_row_data.get(col, "N/A") for col in self.columns])
            total_rows_written += 1
        
        if self._csv_file:
            self._csv_file.flush()
            
        logger.info(f"Successfully wrote {total_rows_written} rows with complete document-level information")
        return total_rows_written

    def close_csv(self):
        if self._csv_file:
            self._csv_file.close()

# Enhanced Document Extractor for Healthcare Contracts
class HealthcareDocumentExtractor:
    def __init__(self, client, config: DocumentTypeConfig, output_dir: str = "document_extraction"):
        self.client = client
        self.config = config
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Create components from config
        self.RowModel, self.DocumentModel = create_dynamic_models(config)
        self.state_manager = StateManager(config)
        self.extraction_prompt = self._build_extraction_prompt()

    def _build_extraction_prompt(self) -> str:
        """Builds extraction prompt optimized for continuation pages."""
        doc_fields_str = "\n".join(
            f"- **{f.name}**: {f.description} (e.g., '{', '.join(f.examples)}')" for f in self.config.document_fields
        )
        row_fields_str = "\n".join(
            f"- **{f.name}**: {f.description} (e.g., '{', '.join(f.examples)}')" for f in self.config.row_fields
        )
        json_schema = json.dumps(self.DocumentModel.model_json_schema(), indent=2)
        
        return f"""
You are an expert data extraction AI. {self.config.extraction_instructions}

### EXTRACTION SCHEMA
Follow this JSON schema precisely. Use `null` for any missing values.

#### Document-Level Fields (extract if present, use null if not found on this page):
{doc_fields_str}

#### Row-Level Fields (extract ALL facility rows from this page):
{row_fields_str}

### IMPORTANT FOR CONTINUATION PAGES:
- If this is a continuation page (page 2, 3, etc. of a letter), focus on extracting facility/hospital rows
- Document-level fields like notification_received_date may not be on continuation pages - that's OK, use null
- Extract ALL table rows/facilities visible on this page, even if document-level fields are missing
- Don't leave rows array empty if there are facilities/hospitals listed on the page

### JSON OUTPUT SCHEMA:
{json_schema}
"""

    def _find_target_pages_with_ai(self, doc: fitz.Document, image_dir: Path, dpi: int) -> Dict[int, List[int]]:
        """Uses AI to classify pages and group them into separate letters."""
        logger.info("Starting AI-powered page classification with letter boundary detection...")
        all_healthcare_pages = []
        letter_groups = {}  # letter_id -> list of page indices
        current_letter_id = 1
        created_images = []
        
        i = 0
        while i < len(doc):
            page_num = i + 1
            try:
                page = doc.load_page(i)
                mat = fitz.Matrix(dpi / 72, dpi / 72)
                pix = page.get_pixmap(matrix=mat)
                
                pdf_name = Path(doc.name).stem if doc.name else "document"
                image_path = image_dir / f"{pdf_name}_page_{page_num:03d}.png"
                pix.save(str(image_path))
                created_images.append(image_path)
                
                logger.info(f"Created image: {image_path}")

                with open(image_path, 'rb') as f:
                    image_bytes = f.read()
                
                response = self.client.models.generate_content(
                    model='gemini-1.5-flash',
                    contents=[
                        types.Part.from_bytes(data=image_bytes, mime_type='image/png'), 
                        self.config.detection_prompt
                    ]
                )
                answer = response.text.strip().upper()
                
                if 'YES' in answer:
                    logger.info(f"AI classified page {page_num} as '{self.config.document_type}'.")
                    all_healthcare_pages.append(i)
                    
                    # Check if this is the start of a NEW letter
                    page_text = page.get_text().lower()
                    is_new_letter_start = self._is_letter_start(page_text, page_num)
                    
                    if is_new_letter_start and current_letter_id in letter_groups:
                        # Start a new letter group
                        current_letter_id += 1
                        logger.info(f"Detected NEW letter starting at page {page_num}. Letter ID: {current_letter_id}")
                    
                    # Add page to current letter group
                    if current_letter_id not in letter_groups:
                        letter_groups[current_letter_id] = []
                    letter_groups[current_letter_id].append(i)
                    
                    # Check for letter ending and continuation logic
                    has_letter_ending = any(ending in page_text for ending in ["sincerely", "enclosure"])
                    
                    if not has_letter_ending and (i + 1) < len(doc):
                        # Check if next page is continuation of SAME letter or NEW letter
                        next_page = doc.load_page(i + 1)
                        next_mat = fitz.Matrix(dpi / 72, dpi / 72)
                        next_pix = next_page.get_pixmap(matrix=next_mat)
                        next_image_path = image_dir / f"{pdf_name}_page_{i + 2:03d}.png"
                        next_pix.save(str(next_image_path))
                        
                        with open(next_image_path, 'rb') as f:
                            next_image_bytes = f.read()
                        
                        next_response = self.client.models.generate_content(
                            model='gemini-1.5-flash',
                            contents=[
                                types.Part.from_bytes(data=next_image_bytes, mime_type='image/png'), 
                                self.config.detection_prompt
                            ]
                        )
                        next_answer = next_response.text.strip().upper()
                        
                        if 'YES' in next_answer:
                            next_page_text = next_page.get_text().lower()
                            is_next_new_letter = self._is_letter_start(next_page_text, i + 2)
                            
                            if not is_next_new_letter:
                                logger.info(f"Page {i + 2} is continuation of Letter {current_letter_id}")
                                letter_groups[current_letter_id].append(i + 1)
                                i += 1
                            else:
                                logger.info(f"Page {i + 2} starts a new letter")
                        else:
                            logger.info(f"Page {i + 2} is not a healthcare contract. Letter {current_letter_id} ends.")
                    else:
                        if has_letter_ending:
                            logger.info(f"Letter {current_letter_id} ending detected on page {page_num}")
                else:
                    logger.info(f"AI classified page {page_num} as NOT '{self.config.document_type}'.")
                    
            except Exception as e:
                logger.error(f"Could not classify page {page_num}: {e}")
            i += 1
            
        logger.info(f"Created {len(created_images)} image files")
        logger.info(f"Detected {len(letter_groups)} separate letters:")
        for letter_id, pages in letter_groups.items():
            page_numbers = [p + 1 for p in pages]
            logger.info(f"  Letter {letter_id}: Pages {page_numbers}")
        
        return letter_groups

    def _is_letter_start(self, page_text: str, page_num: int) -> bool:
        """Detects if a page is the start of a new letter based on content patterns."""
        # Indicators of letter start
        letter_start_indicators = [
            "dear mr.", "dear ms.", "dear dr.",
            "we have received your letter",
            "we have received your email", 
            "notification received",
            "this letter serves as notice"
        ]
        
        # Check for date patterns at the top (new letters typically start with dates)
        has_date_pattern = any(pattern in page_text for pattern in [
            "december", "january", "february", "march", "april", "may",
            "june", "july", "august", "september", "october", "november"
        ])
        
        # Check for formal letter greeting patterns
        has_greeting = any(indicator in page_text for indicator in letter_start_indicators)
        
        # First page is always a potential letter start
        is_first_healthcare_page = page_num == 1
        
        # Combination logic: likely letter start if has greeting + date, or is first page with greeting
        is_letter_start = (has_greeting and has_date_pattern) or (is_first_healthcare_page and has_greeting)
        
        if is_letter_start:
            logger.info(f"Page {page_num} identified as letter start: greeting={has_greeting}, date={has_date_pattern}")
        
        return is_letter_start

    def process_page(self, image_path: Path, page_num: int, document_name: str):
        """Processes a single page for data extraction and stores data temporarily."""
        try:
            if not image_path.exists():
                logger.error(f"Image file not found: {image_path}")
                return None
                
            with open(image_path, 'rb') as f:
                image_bytes = f.read()
            
            logger.info(f"Processing page {page_num} for data extraction...")
            
            response = self.client.models.generate_content(
                model='gemini-1.5-flash',
                contents=[
                    types.Part.from_bytes(data=image_bytes, mime_type='image/png'), 
                    self.extraction_prompt
                ]
            )
            response_text = response.text.strip().replace("```json", "").replace("```", "")
            
            logger.info(f"Raw AI response for page {page_num} (first 300 chars): {response_text[:300]}...")
            
            extraction = self.DocumentModel.model_validate_json(response_text)
            
            # Log extraction results for debugging
            logger.info(f"Extracted from page {page_num}:")
            logger.info(f"  notification_received_date: {getattr(extraction, 'notification_received_date', 'None')}")
            logger.info(f"  contract_duration: {getattr(extraction, 'contract_duration', 'None')}")
            logger.info(f"  Number of rows: {len(extraction.rows)}")
            for i, row in enumerate(extraction.rows):
                logger.info(f"  Row {i+1} - hospital_name: {getattr(row, 'hospital_name', 'None')}")
            
            # Collect data instead of immediately writing
            self.state_manager.collect_page_data(extraction, document_name, page_num)
            logger.info(f"Data from page {page_num} collected for later processing")
            
            return extraction
        except (ValidationError, json.JSONDecodeError) as e:
            logger.error(f"Pydantic/JSON validation failed on page {page_num}: {e}")
            logger.error(f"Raw response was: {response_text}")
        except Exception as e:
            logger.error(f"Extraction failed on page {page_num}: {e}")
        return None

    def extract_from_pdf(self, pdf_path: str, dpi: int = 200) -> Dict:
        """Main orchestration method handling multiple letters within a single PDF."""
        pdf_path = Path(pdf_path)
        doc = None
        
        try:
            logger.info(f"Starting multi-letter extraction for: {pdf_path.name}")
            doc = fitz.open(str(pdf_path))
            
            image_dir = self.output_dir / self.config.document_type / pdf_path.stem
            image_dir.mkdir(parents=True, exist_ok=True)
            
            # Get letter groups instead of single page list  
            letter_groups = self._find_target_pages_with_ai(doc, image_dir, dpi)
            
            if not letter_groups:
                logger.warning(f"No '{self.config.document_type}' letters found in '{pdf_path.name}'.")
                return {'total_pages_in_pdf': len(doc), 'letters_processed': 0, 'total_rows': 0}

            # Process each letter independently
            all_results = []
            total_rows_all_letters = 0
            
            for letter_id, page_indices in letter_groups.items():
                logger.info(f"Processing Letter {letter_id} with {len(page_indices)} pages...")
                
                # Reset state for each letter
                self.state_manager.reset_document_state()
                
                # Create separate CSV for each letter
                csv_path = image_dir / f"{pdf_path.stem}_letter_{letter_id}_data.csv"
                self.state_manager.start_csv(csv_path)
                
                # PASS 1: Collect all data from pages of this specific letter
                logger.info(f"PASS 1 - Letter {letter_id}: Collecting data from {len(page_indices)} pages...")
                successful_extractions = 0
                for page_index in page_indices:
                    pdf_name = pdf_path.stem
                    image_path = image_dir / f"{pdf_name}_page_{page_index + 1:03d}.png"
                    
                    logger.info(f"Letter {letter_id}: Collecting data from page {page_index + 1}")
                    extraction = self.process_page(image_path, page_index + 1, f"{pdf_path.name}_Letter_{letter_id}")
                    if extraction:
                        successful_extractions += 1
                
                # PASS 2: Write all collected data for this letter with complete document-level information
                logger.info(f"PASS 2 - Letter {letter_id}: Writing collected data to CSV...")
                letter_rows = self.state_manager.write_all_collected_data()
                total_rows_all_letters += letter_rows
                
                self.state_manager.close_csv()
                
                letter_result = {
                    'letter_id': letter_id,
                    'pages': [p + 1 for p in page_indices],
                    'pages_processed': len(page_indices),
                    'successful_extractions': successful_extractions,
                    'total_rows': letter_rows,
                    'csv_path': str(csv_path)
                }
                all_results.append(letter_result)
                
                logger.info(f"Letter {letter_id} completed: {letter_rows} rows from {successful_extractions} pages")
            
            # Summary result
            result = {
                'pdf_name': pdf_path.name,
                'document_type': self.config.document_type,
                'total_pages_in_pdf': len(doc),
                'letters_found': len(letter_groups),
                'letters_processed': len(all_results),
                'total_rows_all_letters': total_rows_all_letters,
                'letter_details': all_results
            }
            
            logger.info(f"Multi-letter extraction completed for {pdf_path.name}")
            logger.info(f"Final result: {len(letter_groups)} letters, {total_rows_all_letters} total rows")
            return result
            
        except Exception as e:
            logger.error(f"Critical error during PDF extraction: {e}")
            return {'error': str(e), 'pdf_name': pdf_path.name}
        finally:
            if doc:
                try:
                    doc.close()
                    logger.info(f"Document {pdf_path.name} closed successfully")
                except Exception as e:
                    logger.error(f"Error closing document: {e}")

# ==========================================
# CLIENT SETUP & MAIN FUNCTION
# ==========================================

def create_client():
    return genai.Client(vertexai=True, project="anbc-hcb-dev", location="us-central1")

def run_healthcare_document_extraction(pdf_path: str, output_dir: str = "healthcare_extraction_output") -> Dict:
    """Run healthcare contract extraction with optimized page classification."""
    client = create_client()
    extractor = HealthcareDocumentExtractor(client=client, config=HEALTHCARE_CONTRACT_CONFIG, output_dir=output_dir)
    return extractor.extract_from_pdf(pdf_path)
