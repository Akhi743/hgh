if __name__ == "__main__":
    pdf_file_path = "Aultman Hospital.pdf"  # Replace with your PDF path
    
    try:
        # Run the healthcare contract extraction
        results = run_healthcare_document_extraction(pdf_file_path)
        
        print("\n" + "="*70)
        print("HEALTHCARE CONTRACT EXTRACTION RESULTS")
        print("="*70)
        print(f"Document Type: {results.get('document_type', 'N/A')}")
        print(f"PDF: {results.get('pdf_name', 'N/A')}")
        print(f"Total Pages in PDF: {results.get('total_pages_in_pdf', 'N/A')}")
        print(f"Pages Processed: {results.get('pages_processed', 'N/A')}")
        print(f"Total Rows Extracted: {results.get('total_rows', 'N/A')}")
        print(f"CSV Output: {results.get('csv_path', 'N/A')}")
        print("="*70)
        print("\nKey Features:")
        print("1.import fitz
import csv
import json
import logging
from pathlib import Path
from typing import List, Optional, Dict, Any, Type
from pydantic import BaseModel, Field, create_model, ValidationError
from google.genai import types
import google.genai as genai

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('document_extraction.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ==========================================
# CONFIGURATION SYSTEM
# ==========================================

class ExtractionField(BaseModel):
    """Configuration for a single extraction field."""
    name: str
    description: str
    field_type: str = "Optional[str]"
    examples: List[str] = Field(default_factory=list)

class DocumentTypeConfig(BaseModel):
    """Configuration for a specific document type, making the extractor generic."""
    document_type: str
    description: str
    document_fields: List[ExtractionField] = Field(default_factory=list)
    row_fields: List[ExtractionField] = Field(default_factory=list)
    detection_prompt: str
    extraction_instructions: str
    csv_columns: List[str]

HEALTHCARE_CONTRACT_CONFIG = DocumentTypeConfig(
    document_type="HealthcareContract",
    description="Aetna healthcare contract for charge master increases.",
    document_fields=[
        # Extract notification date from email body text, not letter header
        ExtractionField(
            name="notification_received_date", 
            description="The date mentioned in the sentence 'We have received your email dated [DATE]' in the letter body - NOT the letter header date.", 
            examples=["December 6, 2024", "11/13/2024"]
        ),
        ExtractionField(name="contract_duration", description="The contract term or period.", examples=["12-month period"]),
        # Moved effective_date to document-level since it applies to all facilities in the letter
        ExtractionField(name="effective_date", description="The effective date for all charge master increases mentioned in this letter.", examples=["January 1, 2025", "1/1/2025"])
    ],
    row_fields=[
        ExtractionField(name="hospital_name", description="The name of the facility from a table row."),
        # Capture complete limit information including both Commercial and Medicare rates
        ExtractionField(
            name="charge_master_limit", 
            description="The complete contractual limit information including both Commercial and Medicare rates if present. Format as 'Commercial X%, Medicare Y%' or single percentage if only one rate.",
            examples=["Commercial 4%, Medicare 3%", "CP 4.5%/ME 3.0%", "4%"]
        ),
        ExtractionField(name="charge_master_increase", description="The increase percentage from the same row.")
    ],
    detection_prompt="""Analyze the image. Is this a formal letter from Aetna about a 'charge master increase' where they state 'we have received' a notification? Answer only YES or NO.""",
    extraction_instructions="""Extract ALL relevant data from this Aetna charge master increase notification letter. 

CRITICAL INSTRUCTIONS:
1. For 'notification_received_date': Look for the phrase 'We have received your email dated [DATE]' in the letter body paragraphs - DO NOT use the letter header date.
2. For 'charge_master_limit': Capture the COMPLETE limit information including both Commercial and Medicare rates when present. Examples:
   - If you see "Commercial 4% and Medicare 3%" → extract as "Commercial 4%, Medicare 3%"
   - If you see "CP 4.5%/ME 3.0%" → extract as "CP 4.5%/ME 3.0%"
   - If only one rate is shown → extract that single rate
3. For 'effective_date': Extract the effective date that applies to all facilities in this letter - usually mentioned as "effective [DATE]" or "to become effective [DATE]"
4. Create one row for each facility mentioned, ensuring no fields are left empty when information is available.""",
    csv_columns=[
        "hospital_name", "charge_master_limit", "charge_master_increase", # Row Fields
        "notification_received_date", "contract_duration", "effective_date", # Document Fields
        "document_source", "page_number" # Metadata
    ]
)

# ==========================================
# DYNAMIC COMPONENT FACTORY
# ==========================================

def create_dynamic_models(config: DocumentTypeConfig) -> (Type[BaseModel], Type[BaseModel]):
    """Dynamically creates Pydantic models for row and document data based on the config."""
    
    # Create the RowModel
    row_model_fields = {
        field.name: (Optional[str], Field(None, description=field.description))
        for field in config.row_fields
    }
    RowModel = create_model(f"{config.document_type}Row", **row_model_fields)

    # Create the DocumentExtractionModel
    doc_model_fields = {
        field.name: (Optional[str], Field(None, description=field.description))
        for field in config.document_fields
    }
    doc_model_fields['rows'] = (List[RowModel], Field(default_factory=list, description="All extracted table rows."))
    DocumentModel = create_model(f"{config.document_type}Extraction", **doc_model_fields)
    
    logger.info(f"Dynamically created Pydantic models for document type: {config.document_type}")
    return RowModel, DocumentModel

# ==========================================
# DYNAMIC STATE MANAGER
# ==========================================

class SequentialStateManager:
    """State manager that processes letters sequentially and writes when letter boundaries are detected."""
    def __init__(self, config: DocumentTypeConfig):
        self.config = config
        self.columns = config.csv_columns
        self.output_dir = None
        self.pdf_name = None
        
        # Current letter state
        self.current_letter_rows = []  # Accumulate rows for current letter
        self.current_letter_id = 1
        self.total_letters_processed = 0
        self.total_rows_written = 0
        
        logger.info(f"Sequential state manager initialized")

    def setup_output(self, output_dir: Path, pdf_name: str):
        """Setup output directory and PDF name for later CSV creation."""
        self.output_dir = output_dir
        self.pdf_name = pdf_name

    def add_page_data(self, extraction_data: BaseModel, document_name: str, page_num: int):
        """Add page data to current letter accumulation."""
        # Collect all rows from this page for the current letter
        for row_data in extraction_data.rows:
            row_dict = {}
            # Add row-level data
            for field in self.config.row_fields:
                row_dict[field.name] = getattr(row_data, field.name, "N/A")
            # Add metadata
            row_dict["document_source"] = document_name
            row_dict["page_number"] = page_num
            
            self.current_letter_rows.append(row_dict)
            
        logger.info(f"Added {len(extraction_data.rows)} rows from page {page_num}. Current letter total: {len(self.current_letter_rows)}")

    def finalize_current_letter(self, final_extraction_data: BaseModel) -> int:
        """Extract document-level fields from letter-ending page and write complete letter data."""
        if not self.current_letter_rows:
            logger.info(f"No rows to write for letter {self.current_letter_id}")
            return 0
            
        # Extract document-level fields from the letter-ending page
        document_fields = {}
        for field in self.config.document_fields:
            document_fields[field.name] = getattr(final_extraction_data, field.name, "N/A")
        
        logger.info(f"Finalizing Letter {self.current_letter_id} with document fields: {document_fields}")
        
        # Create CSV for this letter
        csv_path = self.output_dir / f"{self.pdf_name}_letter_{self.current_letter_id}_data.csv"
        
        rows_written = 0
        with open(csv_path, 'w', newline='', encoding='utf-8') as csv_file:
            csv_writer = csv.writer(csv_file)
            csv_writer.writerow(self.columns)
            
            # Write all accumulated rows with the document-level fields
            for row_dict in self.current_letter_rows:
                csv_row_data = {}
                
                # Add row-level data
                for field in self.config.row_fields:
                    csv_row_data[field.name] = row_dict.get(field.name, "N/A")
                
                # Add document-level data from letter-ending page
                for field in self.config.document_fields:
                    csv_row_data[field.name] = document_fields.get(field.name, "N/A")
                
                # Add metadata
                csv_row_data["document_source"] = row_dict.get("document_source", "N/A")
                csv_row_data["page_number"] = row_dict.get("page_number", "N/A")
                
                # Write in the order specified by config
                csv_writer.writerow([csv_row_data.get(col, "N/A") for col in self.columns])
                rows_written += 1
        
        logger.info(f"Letter {self.current_letter_id} completed: {rows_written} rows written to {csv_path}")
        
        # Reset for next letter
        self.current_letter_rows = []
        self.current_letter_id += 1
        self.total_letters_processed += 1
        self.total_rows_written += rows_written
        
        return rows_written

    def get_summary(self) -> Dict:
        """Get processing summary."""
        return {
            'total_letters_processed': self.total_letters_processed,
            'total_rows_written': self.total_rows_written,
            'current_letter_rows_pending': len(self.current_letter_rows)
        }
        """Second pass: Write all collected rows with complete document-level information."""
        total_rows_written = 0
        
        logger.info(f"Writing all collected data with complete document-level fields:")
        for field_name, value in self.doc_state.items():
            logger.info(f"  {field_name}: {value}")
        
        # Write all rows with complete document-level information
        for row_dict in self.temp_rows:
            csv_row_data = {}
            
            # Add row-level data
            for field in self.config.row_fields:
                csv_row_data[field.name] = row_dict.get(field.name, "N/A")
            
            # Add complete document-level data
            for field in self.config.document_fields:
                csv_row_data[field.name] = self.doc_state.get(field.name, "N/A")
            
            # Add metadata
            csv_row_data["document_source"] = row_dict.get("document_source", "N/A")
            csv_row_data["page_number"] = row_dict.get("page_number", "N/A")
            
            # Write in the order specified by config
            self._csv_writer.writerow([csv_row_data.get(col, "N/A") for col in self.columns])
            total_rows_written += 1
        
        if self._csv_file:
            self._csv_file.flush()
            
        logger.info(f"Successfully wrote {total_rows_written} rows with complete document-level information")
        return total_rows_written

    def close_csv(self):
        if self._csv_file:
            self._csv_file.close()

# Enhanced Document Extractor for Healthcare Contracts
class HealthcareDocumentExtractor:
    def __init__(self, client, config: DocumentTypeConfig, output_dir: str = "document_extraction"):
        self.client = client
        self.config = config
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Create components from config
        self.RowModel, self.DocumentModel = create_dynamic_models(config)
        self.state_manager = SequentialStateManager(config)
        self.extraction_prompt = self._build_extraction_prompt()

    def _build_extraction_prompt(self) -> str:
        """Builds extraction prompt optimized for sequential processing."""
        doc_fields_str = "\n".join(
            f"- **{f.name}**: {f.description} (e.g., '{', '.join(f.examples)}')" for f in self.config.document_fields
        )
        row_fields_str = "\n".join(
            f"- **{f.name}**: {f.description} (e.g., '{', '.join(f.examples)}')" for f in self.config.row_fields
        )
        json_schema = json.dumps(self.DocumentModel.model_json_schema(), indent=2)
        
        return f"""
You are an expert data extraction AI. {self.config.extraction_instructions}

### EXTRACTION SCHEMA
Follow this JSON schema precisely. Use `null` for any missing values.

#### Document-Level Fields (extract if present on this page):
{doc_fields_str}

#### Row-Level Fields (extract ALL facility rows from this page):
{row_fields_str}

### IMPORTANT:
- Extract ALL visible facility/hospital rows from this page
- For document-level fields, extract them if present (they may not be on every page)
- Use null for missing values

### JSON OUTPUT SCHEMA:
{json_schema}
"""

    def _is_letter_ending_page(self, page_text: str) -> bool:
        """Check if this page contains letter ending indicators."""
        page_text_lower = page_text.lower()
        return any(ending in page_text_lower for ending in ["sincerely", "enclosure"])

    def process_page(self, image_path: Path, page_num: int, document_name: str):
        """Process a single page and return extraction data."""
        try:
            if not image_path.exists():
                logger.error(f"Image file not found: {image_path}")
                return None
                
            with open(image_path, 'rb') as f:
                image_bytes = f.read()
            
            logger.info(f"Processing page {page_num} for data extraction...")
            
            response = self.client.models.generate_content(
                model='gemini-1.5-flash',
                contents=[
                    types.Part.from_bytes(data=image_bytes, mime_type='image/png'), 
                    self.extraction_prompt
                ]
            )
            response_text = response.text.strip().replace("```json", "").replace("```", "")
            
            extraction = self.DocumentModel.model_validate_json(response_text)
            
            logger.info(f"Extracted from page {page_num}: {len(extraction.rows)} rows")
            return extraction
            
        except (ValidationError, json.JSONDecodeError) as e:
            logger.error(f"Extraction failed on page {page_num}: {e}")
        except Exception as e:
            logger.error(f"Processing failed on page {page_num}: {e}")
        return None

    def extract_from_pdf(self, pdf_path: str, dpi: int = 200) -> Dict:
        """Sequential processing: scan pages, accumulate rows, write when letter ends."""
        pdf_path = Path(pdf_path)
        doc = None
        
        try:
            logger.info(f"Starting sequential extraction for: {pdf_path.name}")
            doc = fitz.open(str(pdf_path))
            
            image_dir = self.output_dir / self.config.document_type / pdf_path.stem
            image_dir.mkdir(parents=True, exist_ok=True)
            
            self.state_manager.setup_output(image_dir, pdf_path.stem)
            
            # Process pages sequentially
            for page_index in range(len(doc)):
                page_num = page_index + 1
                logger.info(f"=== Processing Page {page_num} ===")
                
                # Convert page to image
                page = doc.load_page(page_index)
                mat = fitz.Matrix(dpi / 72, dpi / 72)
                pix = page.get_pixmap(matrix=mat)
                pdf_name = pdf_path.stem
                image_path = image_dir / f"{pdf_name}_page_{page_num:03d}.png"
                pix.save(str(image_path))
                
                # Check if this is a healthcare contract page
                with open(image_path, 'rb') as f:
                    image_bytes = f.read()
                
                response = self.client.models.generate_content(
                    model='gemini-1.5-flash',
                    contents=[
                        types.Part.from_bytes(data=image_bytes, mime_type='image/png'), 
                        self.config.detection_prompt
                    ]
                )
                
                if 'YES' not in response.text.strip().upper():
                    logger.info(f"Page {page_num} is not a healthcare contract page - skipping")
                    continue
                
                logger.info(f"Page {page_num} is a healthcare contract page - processing")
                
                # Extract data from this page
                extraction = self.process_page(image_path, page_num, pdf_path.name)
                if not extraction:
                    continue
                
                # Add page data to current letter accumulation
                self.state_manager.add_page_data(extraction, pdf_path.name, page_num)
                
                # Check if this page ends a letter
                page_text = page.get_text()
                if self._is_letter_ending_page(page_text):
                    logger.info(f"Letter ending detected on page {page_num} - finalizing current letter")
                    self.state_manager.finalize_current_letter(extraction)
                    logger.info("Letter finalized. Ready for next letter.")
            
            # Handle case where PDF ends without explicit letter ending
            if self.state_manager.current_letter_rows:
                logger.info("PDF ended with pending letter data - finalizing last letter")
                # Use empty extraction for final letter (no document-level fields)
                empty_extraction = self.DocumentModel()
                self.state_manager.finalize_current_letter(empty_extraction)
            
            summary = self.state_manager.get_summary()
            
            result = {
                'pdf_name': pdf_path.name,
                'document_type': self.config.document_type,
                'total_pages_in_pdf': len(doc),
                'letters_processed': summary['total_letters_processed'],
                'total_rows': summary['total_rows_written']
            }
            
            logger.info(f"Sequential extraction completed for {pdf_path.name}")
            logger.info(f"Final result: {summary['total_letters_processed']} letters, {summary['total_rows_written']} total rows")
            return result
            
        except Exception as e:
            logger.error(f"Critical error during PDF extraction: {e}")
            return {'error': str(e), 'pdf_name': pdf_path.name}
        finally:
            if doc:
                try:
                    doc.close()
                    logger.info(f"Document {pdf_path.name} closed successfully")
                except Exception as e:
                    logger.error(f"Error closing document: {e}")logger.info(f"Letter {letter_id}: Processing page {page_index + 1}")
                    extraction = self.process_page(image_path, page_index + 1, f"{pdf_path.name}_Letter_{letter_id}")
                    if extraction:
                        successful_extractions += 1
                
                # Log current letter's document-level state before writing
                current_state = self.state_manager.get_current_letter_state()
                logger.info(f"Letter {letter_id} final document-level state: {current_state}")
                
                # PASS 2: Write all collected data for this letter with ITS OWN document-level information
                logger.info(f"PASS 2 - Letter {letter_id}: Writing collected data to CSV...")
                letter_rows = self.state_manager.write_all_collected_data()
                total_rows_all_letters += letter_rows
                
                self.state_manager.close_csv()
                
                letter_result = {
                    'letter_id': letter_id,
                    'pages': [p + 1 for p in page_indices],
                    'pages_processed': len(page_indices),
                    'successful_extractions': successful_extractions,
                    'total_rows': letter_rows,
                    'document_state': current_state,  # For debugging
                    'csv_path': str(csv_path)
                }
                all_results.append(letter_result)
                
                logger.info(f"Letter {letter_id} completed: {letter_rows} rows with state {current_state}")
                logger.info(f"=" * 50)
            
            # Summary result
            result = {
                'pdf_name': pdf_path.name,
                'document_type': self.config.document_type,
                'total_pages_in_pdf': len(doc),
                'letters_found': len(letter_groups),
                'letters_processed': len(all_results),
                'total_rows_all_letters': total_rows_all_letters,
                'letter_details': all_results
            }
            
            logger.info(f"Multi-letter extraction completed for {pdf_path.name}")
            logger.info(f"Final result: {len(letter_groups)} letters, {total_rows_all_letters} total rows")
            return result
            
        except Exception as e:
            logger.error(f"Critical error during PDF extraction: {e}")
            return {'error': str(e), 'pdf_name': pdf_path.name}
        finally:
            if doc:
                try:
                    doc.close()
                    logger.info(f"Document {pdf_path.name} closed successfully")
                except Exception as e:
                    logger.error(f"Error closing document: {e}")

# ==========================================
# CLIENT SETUP & MAIN FUNCTION
# ==========================================

def create_client():
    return genai.Client(vertexai=True, project="anbc-hcb-dev", location="us-central1")

def run_healthcare_document_extraction(pdf_path: str, output_dir: str = "healthcare_extraction_output") -> Dict:
    """Run healthcare contract extraction with sequential processing."""
    client = create_client()
    extractor = HealthcareDocumentExtractor(client=client, config=HEALTHCARE_CONTRACT_CONFIG, output_dir=output_dir)
    return extractor.extract_from_pdf(pdf_path)
