import fitz
import csv
import json
import logging
import os
import time
import threading
import signal
import sys
from pathlib import Path
from typing import List, Optional, Dict, Any, Tuple
from google.genai import types
import google.genai as genai
from datetime import datetime, date, timedelta
from dataclasses import dataclass, field
from dateutil import parser
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock, Event
from collections import defaultdict, deque
import gc
import psutil
from queue import Queue, Empty
import hashlib
import math

# Enhanced logging setup
from logging.handlers import RotatingFileHandler

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(threadName)-10s - %(levelname)s - %(message)s',
    handlers=[
        RotatingFileHandler('thorough_classification.log', maxBytes=50*1024*1024, backupCount=5, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ==========================================
# THOROUGH MODE DATA CLASSES
# ==========================================

@dataclass
class ThoroughModeStats:
    """Statistics for thorough mode processing."""
    total_documents: int = 0
    target_duration_hours: float = 18.0  # Target 15-20 hours (use middle)
    start_time: Optional[datetime] = None
    estimated_end_time: Optional[datetime] = None
    documents_processed: int = 0
    documents_per_hour_target: float = 0.0
    documents_per_hour_actual: float = 0.0
    processing_pace: str = "normal"  # "slow", "normal", "fast"
    time_adjustment_factor: float = 1.0
    early_exits_triggered: int = 0
    total_pages_analyzed: int = 0
    average_pages_per_doc: float = 0.0

@dataclass
class DocumentMetadata:
    """Enhanced document metadata for thorough mode."""
    source_path: str
    pdf_name: str
    state: str
    file_size: int
    last_modified: datetime
    classification_status: str = "pending"
    classification_result: str = "unknown"
    detected_on_page: Optional[int] = None
    effective_date: Optional[datetime] = None
    stage_failed: str = ""
    confidence_score: float = 0.0
    processing_time: float = 0.0
    error_message: str = ""
    pdf_complexity_level: str = ""
    pdf_issues: List[str] = field(default_factory=list)
    
    # Thorough mode specific
    pages_analyzed: int = 0
    early_exit_triggered: bool = False
    thread_id: str = ""
    processing_started_at: Optional[datetime] = None
    file_modification_date: date = None
    is_newly_modified: bool = False
    
    def __post_init__(self):
        if self.last_modified:
            self.file_modification_date = self.last_modified.date()

@dataclass
class ClassificationResult:
    """Enhanced classification results for thorough mode."""
    is_relevant: bool
    detected_on_page: Optional[int] = None
    effective_date: Optional[datetime] = None
    stage_failed: str = ""
    confidence: float = 0.0
    processing_time: float = 0.0
    pages_checked: int = 0
    early_exit_triggered: bool = False
    total_pages_available: int = 0
    pages_skipped: int = 0

# ==========================================
# THOROUGH MODE CONFIGURATION
# ==========================================

class ThoroughModeConfig:
    """Thorough mode specific configuration - exactly as requested."""
    def __init__(self):
        # THOROUGH MODE REQUIREMENTS
        self.pages_per_document = 10  # Exactly 10 pages as requested
        self.parallel_threads = 5     # Exactly 5 threads as requested
        self.enable_early_exit = True # Early exit on first match
        self.target_duration_hours = 18.0  # Target 15-20 hours (use middle: 18)
        
        # Core business logic
        self.agreements_folder = "AGREEMENTS-NON STANDARD"
        self.hospital_subfolder = "HOSPITAL"
        self.min_effective_year = 2020
        self.min_file_modification_date = date(2019, 1, 1)
        
        # Thorough mode page strategy
        self.page_selection_strategy = "thorough"  # "thorough", "smart", "sequential"
        self.first_pages_count = 6   # More thorough: first 6 pages
        self.last_pages_count = 4    # Last 4 pages
        self.analyze_middle_pages = True  # Also analyze some middle pages
        
        # Pacing control for 15-20 hour target
        self.enable_adaptive_pacing = True
        self.min_delay_between_docs = 0.5  # Minimum delay in seconds
        self.max_delay_between_docs = 10.0  # Maximum delay in seconds
        self.pacing_adjustment_interval = 100  # Adjust pacing every N documents
        
        # State management
        self.enable_incremental_processing = True
        self.checkpoint_interval = 50
        self.state_file_path = "thorough_mode_state.json"
        
        # Performance settings optimized for thoroughness
        self.batch_size = 25  # Smaller batches for better control
        self.memory_limit_mb = 1024
        self.api_timeout_seconds = 45  # Longer timeout for thorough processing
        
        # Monitoring
        self.progress_update_interval = 30  # More frequent updates
        self.enable_detailed_logging = True
        self.save_detailed_analytics = True
        
        # AI settings
        self.image_dpi = 150
        self.healthcare_contract_prompt = "Analyze the image carefully. Is this a formal letter from Aetna about a 'charge master increase' where they state 'we have received' a notification? Answer only YES or NO."
        self.effective_date_prompt = 'This is an Aetna charge master increase letter. Find the EFFECTIVE DATE mentioned in the letter (phrases like "effective January 1, 2024" or "to become effective 1/1/2025"). Respond with just the date found, or "NOT_FOUND" if no effective date exists.'
        
        # File filtering
        self.min_file_size_bytes = 1024
        self.max_file_size_mb = 200  # Allow larger files in thorough mode
        self.skip_file_patterns = ['.tmp', '.lock', '~$']
        
        # US states
        self.us_states = {
            'ALABAMA', 'ALASKA', 'ARIZONA', 'ARKANSAS', 'CALIFORNIA', 'COLORADO', 
            'CONNECTICUT', 'DELAWARE', 'FLORIDA', 'GEORGIA', 'HAWAII', 'IDAHO', 
            'ILLINOIS', 'INDIANA', 'IOWA', 'KANSAS', 'KENTUCKY', 'LOUISIANA', 
            'MAINE', 'MARYLAND', 'MASSACHUSETTS', 'MICHIGAN', 'MINNESOTA', 
            'MISSISSIPPI', 'MISSOURI', 'MONTANA', 'NEBRASKA', 'NEVADA', 
            'NEW HAMPSHIRE', 'NEW JERSEY', 'NEW MEXICO', 'NEW YORK', 
            'NORTH CAROLINA', 'NORTH DAKOTA', 'OHIO', 'OKLAHOMA', 'OREGON', 
            'PENNSYLVANIA', 'RHODE ISLAND', 'SOUTH CAROLINA', 'SOUTH DAKOTA', 
            'TENNESSEE', 'TEXAS', 'UTAH', 'VERMONT', 'VIRGINIA', 'WASHINGTON', 
            'WEST VIRGINIA', 'WISCONSIN', 'WYOMING', 'PUERTO RICO', 'DISTRICT OF COLUMBIA'
        }

# ==========================================
# ADAPTIVE PACING CONTROLLER
# ==========================================

class AdaptivePacingController:
    """Controls processing pace to target 15-20 hour completion time."""
    
    def __init__(self, config: ThoroughModeConfig, total_documents: int):
        self.config = config
        self.total_documents = total_documents
        self.target_hours = config.target_duration_hours
        self.start_time = datetime.now()
        
        # Calculate target rate
        self.target_docs_per_hour = total_documents / self.target_hours
        self.target_docs_per_minute = self.target_docs_per_hour / 60
        
        # Pacing control
        self.current_delay = config.min_delay_between_docs
        self.documents_processed = 0
        self.last_adjustment_time = datetime.now()
        self.last_adjustment_count = 0
        
        # Statistics
        self.pacing_history = deque(maxlen=50)
        self.rate_history = deque(maxlen=20)
        
        logger.info(f"Thorough Mode Pacing: Target {self.target_docs_per_hour:.1f} docs/hour over {self.target_hours} hours")
        logger.info(f"Total documents: {total_documents:,}")
    
    def calculate_adaptive_delay(self, documents_completed: int) -> float:
        """Calculate delay needed to maintain target pace."""
        if not self.config.enable_adaptive_pacing:
            return self.config.min_delay_between_docs
        
        current_time = datetime.now()
        elapsed_hours = (current_time - self.start_time).total_seconds() / 3600
        
        if elapsed_hours < 0.1:  # Too early to adjust
            return self.current_delay
        
        # Calculate current rate
        current_rate = documents_completed / elapsed_hours
        
        # Calculate how we're doing vs target
        rate_ratio = current_rate / self.target_docs_per_hour
        
        # Adjust delay based on rate
        if rate_ratio > 1.2:  # Too fast - slow down
            self.current_delay = min(self.current_delay * 1.1, self.config.max_delay_between_docs)
            pace_status = "slowing down"
        elif rate_ratio < 0.8:  # Too slow - speed up
            self.current_delay = max(self.current_delay * 0.9, self.config.min_delay_between_docs)
            pace_status = "speeding up"
        else:  # Just right
            pace_status = "on target"
        
        # Log pacing adjustments
        if documents_completed % self.config.pacing_adjustment_interval == 0:
            remaining_hours = (self.total_documents - documents_completed) / max(current_rate, 0.1)
            eta = current_time + timedelta(hours=remaining_hours)
            
            logger.info(f"Pacing Update: {current_rate:.1f} docs/hr (target: {self.target_docs_per_hour:.1f}) - {pace_status}")
            logger.info(f"ETA: {eta.strftime('%Y-%m-%d %H:%M')} ({remaining_hours:.1f} hours remaining)")
        
        return self.current_delay
    
    def apply_pacing_delay(self, documents_completed: int):
        """Apply adaptive delay to control processing pace."""
        delay = self.calculate_adaptive_delay(documents_completed)
        if delay > 0:
            time.sleep(delay)

# ==========================================
# THOROUGH MODE STATE MANAGER
# ==========================================

class ThoroughModeStateManager:
    """Enhanced state manager for thorough mode with detailed tracking."""
    
    def __init__(self, config: ThoroughModeConfig):
        self.config = config
        self.state_file_path = Path(config.state_file_path)
        self.processed_files = {}
        self.thorough_stats = ThoroughModeStats()
        self.last_run_date = None
        self._lock = Lock()
        
        # Enhanced tracking for thorough mode
        self.session_start_time = datetime.now()
        self.total_pages_analyzed = 0
        self.early_exits_count = 0
        self.detailed_results = []
        
        self.load_state()
    
    def load_state(self):
        """Load previous thorough mode state."""
        try:
            if self.state_file_path.exists():
                with open(self.state_file_path, 'r', encoding='utf-8') as f:
                    state_data = json.load(f)
                
                self.last_run_date = parser.parse(state_data.get('last_run_date')).date() if state_data.get('last_run_date') else None
                self.processed_files = state_data.get('processed_files', {})
                
                # Load thorough mode specific data
                thorough_data = state_data.get('thorough_mode_stats', {})
                self.total_pages_analyzed = thorough_data.get('total_pages_analyzed', 0)
                self.early_exits_count = thorough_data.get('early_exits_count', 0)
                
                logger.info(f"Loaded thorough mode state: {len(self.processed_files)} processed files")
                logger.info(f"Previous session analyzed {self.total_pages_analyzed:,} pages with {self.early_exits_count} early exits")
            else:
                logger.info("No previous thorough mode state found. Starting fresh.")
        except Exception as e:
            logger.warning(f"Could not load thorough mode state: {e}. Starting fresh.")
            self.processed_files = {}
    
    def should_process_file(self, file_path: str, file_modified_date: date) -> bool:
        """Determine if file needs thorough processing."""
        with self._lock:
            file_info = self.processed_files.get(file_path)
            
            if not file_info:
                return True
            
            # In thorough mode, we might want to reprocess some files
            last_processed_date = parser.parse(file_info['last_modified']).date()
            
            # Only skip if file is unchanged AND was processed in thorough mode
            was_thorough = file_info.get('thorough_mode', False)
            
            return not was_thorough or file_modified_date > last_processed_date
    
    def update_file_status(self, file_path: str, file_modified_date: date, 
                          classification_result: str, thread_id: str, 
                          pages_analyzed: int = 0, early_exit: bool = False):
        """Update file status with thorough mode details."""
        with self._lock:
            self.processed_files[file_path] = {
                'last_modified': file_modified_date.isoformat(),
                'classification_result': classification_result,
                'processed_on': datetime.now().isoformat(),
                'thread_id': thread_id,
                'thorough_mode': True,
                'pages_analyzed': pages_analyzed,
                'early_exit_triggered': early_exit
            }
            
            # Update session statistics
            self.total_pages_analyzed += pages_analyzed
            if early_exit:
                self.early_exits_count += 1
    
    def save_final_state(self, total_documents: int):
        """Save final thorough mode state with comprehensive statistics."""
        try:
            session_duration = datetime.now() - self.session_start_time
            
            state_data = {
                'last_run_date': date.today().isoformat(),
                'processed_files': self.processed_files,
                'thorough_mode_stats': {
                    'total_documents_processed': len(self.processed_files),
                    'total_pages_analyzed': self.total_pages_analyzed,
                    'early_exits_count': self.early_exits_count,
                    'session_duration_hours': session_duration.total_seconds() / 3600,
                    'average_pages_per_document': self.total_pages_analyzed / max(len(self.processed_files), 1),
                    'early_exit_rate': (self.early_exits_count / max(len(self.processed_files), 1)) * 100,
                    'processing_mode': 'thorough',
                    'target_duration_hours': self.config.target_duration_hours,
                    'actual_duration_hours': session_duration.total_seconds() / 3600
                },
                'final_save_timestamp': datetime.now().isoformat()
            }
            
            # Atomic write
            temp_path = self.state_file_path.with_suffix('.tmp')
            with open(temp_path, 'w', encoding='utf-8') as f:
                json.dump(state_data, f, indent=2, default=str)
            temp_path.replace(self.state_file_path)
            
            logger.info(f"Final thorough mode state saved: {len(self.processed_files)} files processed")
            logger.info(f"Total pages analyzed: {self.total_pages_analyzed:,}")
            logger.info(f"Early exits triggered: {self.early_exits_count}")
            
        except Exception as e:
            logger.error(f"Could not save thorough mode state: {e}")

# ==========================================
# THOROUGH MODE PROGRESS MONITOR
# ==========================================

class ThoroughModeProgressMonitor:
    """Enhanced progress monitor for thorough mode with detailed tracking."""
    
    def __init__(self, total_documents: int, config: ThoroughModeConfig):
        self.total_documents = total_documents
        self.config = config
        self._lock = Lock()
        self.start_time = datetime.now()
        
        # Thorough mode specific tracking
        self.target_end_time = self.start_time + timedelta(hours=config.target_duration_hours)
        self.documents_processed = 0
        self.total_pages_analyzed = 0
        self.early_exits_triggered = 0
        self.classification_results = defaultdict(int)
        
        # Thread tracking
        self.thread_metrics = {}
        self.active_threads = set()
        
        # Performance tracking
        self.processing_rate_history = deque(maxlen=20)
        self.pages_per_hour_history = deque(maxlen=20)
        self.last_update_time = datetime.now()
        self.last_completed_count = 0
        
        # Shutdown coordination
        self.shutdown_event = Event()
        
        # Start monitoring
        self.monitor_thread = threading.Thread(target=self._thorough_monitor_loop, daemon=True, name="ThoroughMonitor")
        self.monitor_thread.start()
        
        logger.info(f"Thorough Mode Monitor started: Target {config.target_duration_hours} hours")
        logger.info(f"Expected completion: {self.target_end_time.strftime('%Y-%m-%d %H:%M')}")
    
    def register_thread(self, thread_id: str):
        """Register worker thread for thorough mode."""
        with self._lock:
            self.thread_metrics[thread_id] = {
                'documents_processed': 0,
                'pages_analyzed': 0,
                'early_exits': 0,
                'total_time': 0.0,
                'current_document': 'Initializing...',
                'start_time': datetime.now(),
                'current_page': 0,
                'max_pages': 0
            }
            self.active_threads.add(thread_id)
    
    def update_thread_progress(self, thread_id: str, document_name: str, 
                             classification_result: str, processing_time: float,
                             pages_analyzed: int = 0, early_exit: bool = False):
        """Update thread progress with thorough mode details."""
        with self._lock:
            if thread_id not in self.thread_metrics:
                self.register_thread(thread_id)
            
            metrics = self.thread_metrics[thread_id]
            metrics['documents_processed'] += 1
            metrics['pages_analyzed'] += pages_analyzed
            metrics['total_time'] += processing_time
            
            if early_exit:
                metrics['early_exits'] += 1
                self.early_exits_triggered += 1
            
            self.documents_processed += 1
            self.total_pages_analyzed += pages_analyzed
            self.classification_results[classification_result] += 1
            
            # Update current status
            short_name = document_name[-40:] if len(document_name) > 40 else document_name
            exit_indicator = " [Early Exit]" if early_exit else ""
            metrics['current_document'] = f"{short_name} [{classification_result}]{exit_indicator}"
    
    def update_document_stage(self, thread_id: str, document_name: str, 
                            current_page: int, max_pages: int, stage: str):
        """Update document processing stage."""
        with self._lock:
            if thread_id not in self.thread_metrics:
                self.register_thread(thread_id)
            
            metrics = self.thread_metrics[thread_id]
            metrics['current_page'] = current_page
            metrics['max_pages'] = max_pages
            
            short_name = document_name[-30:] if len(document_name) > 30 else document_name
            metrics['current_document'] = f"{short_name} [{stage} {current_page}/{max_pages}]"
    
    def _thorough_monitor_loop(self):
        """Enhanced monitoring loop for thorough mode."""
        while not self.shutdown_event.is_set():
            try:
                self._update_thorough_metrics()
                self._print_thorough_progress()
                time.sleep(self.config.progress_update_interval)
            except Exception as e:
                logger.error(f"Thorough monitor error: {e}")
    
    def _update_thorough_metrics(self):
        """Update thorough mode specific metrics."""
        current_time = datetime.now()
        
        with self._lock:
            elapsed_time = current_time - self.start_time
            elapsed_hours = elapsed_time.total_seconds() / 3600
            
            # Calculate rates
            if elapsed_hours > 0:
                docs_per_hour = self.documents_processed / elapsed_hours
                pages_per_hour = self.total_pages_analyzed / elapsed_hours
                
                self.processing_rate_history.append(docs_per_hour)
                self.pages_per_hour_history.append(pages_per_hour)
    
    def _print_thorough_progress(self):
        """Print detailed thorough mode progress."""
        with self._lock:
            current_time = datetime.now()
            elapsed_time = current_time - self.start_time
            elapsed_hours = elapsed_time.total_seconds() / 3600
            
            # Calculate progress
            progress_pct = (self.documents_processed / max(self.total_documents, 1)) * 100
            
            # Calculate rates
            if elapsed_hours > 0:
                current_docs_per_hour = self.documents_processed / elapsed_hours
                current_pages_per_hour = self.total_pages_analyzed / elapsed_hours
                
                # Estimate completion
                remaining_docs = self.total_documents - self.documents_processed
                if current_docs_per_hour > 0:
                    remaining_hours = remaining_docs / current_docs_per_hour
                    estimated_completion = current_time + timedelta(hours=remaining_hours)
                else:
                    estimated_completion = self.target_end_time
            else:
                current_docs_per_hour = 0
                current_pages_per_hour = 0
                estimated_completion = self.target_end_time
            
            # Target vs actual comparison
            target_docs_per_hour = self.total_documents / self.config.target_duration_hours
            pace_comparison = "ON TARGET"
            if current_docs_per_hour > target_docs_per_hour * 1.2:
                pace_comparison = "AHEAD (may finish early)"
            elif current_docs_per_hour < target_docs_per_hour * 0.8:
                pace_comparison = "BEHIND (may need more time)"
            
            # Average pages per document
            avg_pages = self.total_pages_analyzed / max(self.documents_processed, 1)
            early_exit_rate = (self.early_exits_triggered / max(self.documents_processed, 1)) * 100
            
            # Clear and print status
            os.system('cls' if os.name == 'nt' else 'clear')
            
            print(f"\n{'='*120}")
            print(f"THOROUGH MODE HEALTHCARE CLASSIFICATION - {current_time.strftime('%H:%M:%S')}")
            print(f"{'='*120}")
            print(f"Progress: {self.documents_processed:,}/{self.total_documents:,} ({progress_pct:.1f}%) | Pages: {self.total_pages_analyzed:,} | Avg: {avg_pages:.1f} pages/doc")
            print(f"Elapsed: {str(elapsed_time).split('.')[0]} | Target: {self.config.target_duration_hours}h | ETA: {estimated_completion.strftime('%H:%M')}")
            print(f"Rate: {current_docs_per_hour:.1f} docs/hr (target: {target_docs_per_hour:.1f}) | Pages: {current_pages_per_hour:.0f}/hr | {pace_comparison}")
            print(f"Early Exits: {self.early_exits_triggered} ({early_exit_rate:.1f}%) | Threads: 5 parallel")
            print(f"-" * 120)
            
            # Thread details (show all 5 threads)
            print("THREAD STATUS (5 Parallel Threads):")
            for thread_id, metrics in list(self.thread_metrics.items()):
                docs_processed = metrics.get('documents_processed', 0)
                pages_analyzed = metrics.get('pages_analyzed', 0)
                early_exits = metrics.get('early_exits', 0)
                current_doc = metrics.get('current_document', 'Idle')
                
                print(f"  [{thread_id}] {docs_processed:,} docs | {pages_analyzed:,} pages | {early_exits} early exits")
                print(f"    └── {current_doc}")
            
            # Classification results
            if self.classification_results:
                print(f"-" * 120)
                result_summary = " | ".join([f"{k}: {v}" for k, v in sorted(self.classification_results.items())])
                print(f"RESULTS: {result_summary}")
            
            print(f"{'='*120}")
    
    def shutdown(self):
        """Graceful shutdown of thorough mode monitor."""
        self.shutdown_event.set()
        if hasattr(self, 'monitor_thread'):
            self.monitor_thread.join(timeout=5)

# ==========================================
# THOROUGH MODE PDF HANDLER
# ==========================================

class ThoroughModePDFHandler:
    """PDF handler optimized for thorough 10-page analysis."""
    
    def __init__(self, config: ThoroughModeConfig):
        self.config = config
        self._local = threading.local()
    
    def attempt_pdf_open_thorough(self, pdf_path: str) -> Optional[fitz.Document]:
        """Thorough PDF opening with multiple strategies."""
        # Standard strategies
        strategies = ['standard', 'stream', 'repair']
        
        for strategy in strategies:
            try:
                if strategy == 'standard':
                    doc = fitz.open(pdf_path)
                elif strategy == 'stream':
                    with open(pdf_path, 'rb') as f:
                        pdf_data = f.read()
                    doc = fitz.open(stream=pdf_data, filetype="pdf")
                else:  # repair
                    doc = fitz.open(pdf_path, filetype="pdf")
                
                if len(doc) > 0:
                    return doc
                else:
                    doc.close()
                    
            except Exception:
                continue
        
        return None
    
    def get_thorough_page_selection(self, pdf_doc: fitz.Document) -> List[int]:
        """Select exactly 10 pages using thorough strategy."""
        total_pages = len(pdf_doc)
        
        if total_pages <= self.config.pages_per_document:
            # If document has 10 or fewer pages, analyze all
            return list(range(total_pages))
        
        # For documents with more than 10 pages, use strategic selection
        pages_to_check = []
        
        if self.config.page_selection_strategy == "thorough":
            # Thorough strategy: First 6, middle 2, last 2
            # First 6 pages (healthcare contracts often have key info early)
            first_pages = list(range(min(6, total_pages)))
            pages_to_check.extend(first_pages)
            
            remaining_slots = self.config.pages_per_document - len(first_pages)
            
            if total_pages > 6 and remaining_slots > 0:
                # Add some middle pages
                middle_start = total_pages // 3
                middle_pages = list(range(middle_start, min(middle_start + 2, total_pages)))
                # Remove any overlap with first pages
                middle_pages = [p for p in middle_pages if p not in pages_to_check]
                pages_to_check.extend(middle_pages[:min(2, remaining_slots)])
                
                remaining_slots = self.config.pages_per_document - len(pages_to_check)
                
                # Fill remaining slots with last pages
                if remaining_slots > 0:
                    last_pages = list(range(max(total_pages - remaining_slots, len(pages_to_check)), total_pages))
                    # Remove any overlap
                    last_pages = [p for p in last_pages if p not in pages_to_check]
                    pages_to_check.extend(last_pages)
        
        # Ensure we have exactly 10 pages (or fewer if document is small)
        pages_to_check = sorted(list(set(pages_to_check)))[:self.config.pages_per_document]
        
        return pages_to_check
    
    def safe_page_access(self, doc: fitz.Document, page_idx: int) -> Optional[fitz.Page]:
        """Safe page access for thorough mode."""
        try:
            page = doc.load_page(page_idx)
            _ = page.rect  # Test accessibility
            return page
        except:
            return None
    
    def get_thorough_client(self):
        """Get thread-local client for thorough processing."""
        if not hasattr(self._local, 'client'):
            self._local.client = create_client()
        return self._local.client

# ==========================================
# FILE DISCOVERY (OPTIMIZED FOR THOROUGH MODE)
# ==========================================

class ThoroughModeFileDiscovery:
    """File discovery optimized for thorough mode processing."""
    
    def __init__(self, root_path: str, config: ThoroughModeConfig):
        self.root_path = Path(root_path)
        self.config = config
    
    def discover_all_documents(self) -> List[DocumentMetadata]:
        """Discover all documents for thorough mode processing."""
        logger.info(f"Starting thorough mode discovery from: {self.root_path}")
        
        agreements_path = self.root_path / self.config.agreements_folder
        if not agreements_path.exists():
            logger.error(f"'{self.config.agreements_folder}' not found")
            return []
        
        all_documents = []
        state_folders = [f for f in agreements_path.iterdir() 
                        if f.is_dir() and self._is_state_folder(f)]
        
        logger.info(f"Thorough mode processing {len(state_folders)} state folders...")
        
        for state_folder in state_folders:
            state_docs = self._process_state_thorough(state_folder)
            all_documents.extend(state_docs)
            
            if len(all_documents) % 500 == 0:
                logger.info(f"Discovered {len(all_documents):,} documents for thorough analysis...")
        
        # Apply thorough mode filters
        filtered_docs = self._apply_thorough_filters(all_documents)
        
        logger.info(f"Thorough mode discovery complete: {len(filtered_docs):,} documents ready for 10-page analysis")
        return filtered_docs
    
    def _process_state_thorough(self, state_folder: Path) -> List[DocumentMetadata]:
        """Process state folder for thorough mode."""
        hospital_path = state_folder / self.config.hospital_subfolder
        if not hospital_path.exists():
            return []
        
        documents = []
        
        try:
            pdf_paths = list(hospital_path.rglob("*.pdf"))
            
            for pdf_path in pdf_paths:
                try:
                    # Skip patterns
                    if any(pattern in str(pdf_path) for pattern in self.config.skip_file_patterns):
                        continue
                    
                    # Handle long paths
                    path_str = str(pdf_path)
                    if path_str.startswith('\\\\') and not path_str.startswith('\\\\?\\'):
                        path_str = '\\\\?\\UNC\\' + path_str[2:]
                    
                    if os.path.exists(path_str):
                        stat_info = os.stat(path_str)
                        
                        # Thorough mode size filters
                        if (stat_info.st_size < self.config.min_file_size_bytes or 
                            stat_info.st_size > self.config.max_file_size_mb * 1024 * 1024):
                            continue
                        
                        doc = DocumentMetadata(
                            source_path=path_str,
                            pdf_name=pdf_path.name,
                            state=state_folder.name,
                            file_size=stat_info.st_size,
                            last_modified=datetime.fromtimestamp(stat_info.st_mtime)
                        )
                        documents.append(doc)
                        
                except Exception as e:
                    logger.debug(f"Error processing {pdf_path}: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"Error processing state {state_folder.name}: {e}")
        
        return documents
    
    def _apply_thorough_filters(self, documents: List[DocumentMetadata]) -> List[DocumentMetadata]:
        """Apply filters optimized for thorough mode."""
        filtered = []
        
        for doc in documents:
            # File date filter
            if doc.file_modification_date < self.config.min_file_modification_date:
                continue
            
            # In thorough mode, we're more inclusive
            filtered.append(doc)
        
        return filtered
    
    def _is_state_folder(self, folder_path: Path) -> bool:
        """Check if folder represents a US state."""
        folder_name_upper = folder_path.name.upper()
        return any(state in folder_name_upper for state in self.config.us_states)

# ==========================================
# THOROUGH MODE CLASSIFIER
# ==========================================

class ThoroughModeHealthcareClassifier:
    """Healthcare classifier implementing exact thorough mode requirements."""
    
    def __init__(self, client, config: ThoroughModeConfig):
        self.client = client
        self.config = config
        self.pdf_handler = ThoroughModePDFHandler(config)
        self.state_manager = ThoroughModeStateManager(config)
        
        # Thorough mode tracking
        self.total_documents_discovered = 0
        self.api_calls_made = 0
        self.session_stats = ThoroughModeStats()
        
        # Pacing controller (will be initialized when we know total document count)
        self.pacing_controller = None
        
        # Shutdown handling
        self.shutdown_requested = Event()
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
    
    def _signal_handler(self, signum, frame):
        """Graceful shutdown for thorough mode."""
        logger.info("Thorough mode shutdown signal received...")
        self.shutdown_requested.set()
        self.state_manager.save_final_state(self.total_documents_discovered)
        sys.exit(0)
    
    def classify_all_documents_thorough(self, documents: List[DocumentMetadata]) -> Dict[str, Any]:
        """Execute thorough mode classification with exact requirements."""
        self.total_documents_discovered = len(documents)
        
        logger.info(f"Starting Thorough Mode Classification:")
        logger.info(f"  • Documents: {len(documents):,}")
        logger.info(f"  • Pages per document: {self.config.pages_per_document}")
        logger.info(f"  • Parallel threads: {self.config.parallel_threads}")
        logger.info(f"  • Early exit: {self.config.enable_early_exit}")
        logger.info(f"  • Target duration: {self.config.target_duration_hours} hours")
        
        # Apply thorough mode filtering
        documents_to_process = self._apply_thorough_filtering(documents)
        
        # Initialize pacing controller
        self.pacing_controller = AdaptivePacingController(self.config, len(documents_to_process))
        
        # Initialize progress monitor
        progress_monitor = ThoroughModeProgressMonitor(len(documents_to_process), self.config)
        
        # Results tracking
        results = defaultdict(list)
        results_lock = Lock()
        
        # Process in smaller batches for thorough mode
        batch_size = self.config.batch_size
        total_batches = (len(documents_to_process) + batch_size - 1) // batch_size
        
        logger.info(f"Thorough mode processing {len(documents_to_process)} documents in {total_batches} batches")
        
        documents_completed = 0
        
        for batch_idx in range(total_batches):
            if self.shutdown_requested.is_set():
                logger.info("Shutdown requested during thorough mode processing...")
                break
            
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, len(documents_to_process))
            batch_documents = documents_to_process[start_idx:end_idx]
            
            logger.info(f"Thorough batch {batch_idx + 1}/{total_batches} ({len(batch_documents)} documents)")
            
            # Process batch with exactly 5 threads
            batch_results = self._process_thorough_batch(
                batch_documents, progress_monitor, results_lock, results
            )
            
            documents_completed += len(batch_documents)
            
            # Apply adaptive pacing to meet 15-20 hour target
            if self.pacing_controller:
                self.pacing_controller.apply_pacing_delay(documents_completed)
            
            # Memory cleanup
            gc.collect()
            
            logger.info(f"Thorough batch {batch_idx + 1} completed. Total: {documents_completed:,}/{len(documents_to_process):,}")
        
        # Finalize results
        progress_monitor.shutdown()
        self.state_manager.save_final_state(len(documents_to_process))
        
        # Generate thorough mode results
        final_results = self._generate_thorough_results(results, documents, documents_completed)
        
        logger.info(f"Thorough Mode Classification completed: {documents_completed:,} documents processed")
        return final_results
    
    def _apply_thorough_filtering(self, documents: List[DocumentMetadata]) -> List[DocumentMetadata]:
        """Apply thorough mode specific filtering."""
        documents_to_process = []
        skipped_counts = defaultdict(int)
        
        for doc in documents:
            # Check if thorough processing needed
            should_process = self.state_manager.should_process_file(
                doc.source_path, doc.file_modification_date
            )
            
            if not should_process:
                skipped_counts['already_processed_thorough'] += 1
                doc.classification_result = "skipped_thorough_complete"
                continue
            
            doc.is_newly_modified = True
            documents_to_process.append(doc)
        
        # Sort by modification date (newest first for healthcare relevance)
        documents_to_process.sort(key=lambda x: x.last_modified, reverse=True)
        
        logger.info(f"Thorough mode filtering results:")
        logger.info(f"  • To process: {len(documents_to_process):,}")
        logger.info(f"  • Estimated pages to analyze: {len(documents_to_process) * self.config.pages_per_document:,}")
        for reason, count in skipped_counts.items():
            logger.info(f"  • Skipped ({reason}): {count:,}")
        
        return documents_to_process
    
    def _process_thorough_batch(self, batch_documents: List[DocumentMetadata], 
                               progress_monitor: ThoroughModeProgressMonitor,
                               results_lock: Lock, results: Dict) -> List[DocumentMetadata]:
        """Process batch using exactly 5 parallel threads as required."""
        
        def process_single_document_thorough(doc: DocumentMetadata) -> DocumentMetadata:
            """Process single document with thorough 10-page analysis."""
            thread_id = threading.current_thread().name
            progress_monitor.register_thread(thread_id)
            
            try:
                if self.shutdown_requested.is_set():
                    return doc
                
                doc.thread_id = thread_id
                doc.processing_started_at = datetime.now()
                
                # Execute thorough classification
                classification_result = self._classify_document_thorough_mode(doc, progress_monitor)
                
                # Update document with thorough results
                doc.processing_time = classification_result.processing_time
                doc.confidence_score = classification_result.confidence
                doc.detected_on_page = classification_result.detected_on_page
                doc.effective_date = classification_result.effective_date
                doc.stage_failed = classification_result.stage_failed
                doc.pages_analyzed = classification_result.pages_checked
                doc.early_exit_triggered = classification_result.early_exit_triggered
                doc.classification_status = "completed"
                
                # Determine result category
                is_successful = classification_result.is_relevant
                if is_successful:
                    doc.classification_result = "relevant"
                else:
                    doc.classification_result = self._categorize_thorough_failure(doc.stage_failed)
                
                # Update state with thorough mode details
                self.state_manager.update_file_status(
                    doc.source_path, doc.file_modification_date, 
                    doc.classification_result, thread_id,
                    doc.pages_analyzed, doc.early_exit_triggered
                )
                
                # Update progress monitor
                progress_monitor.update_thread_progress(
                    thread_id, doc.pdf_name, doc.classification_result, 
                    doc.processing_time, doc.pages_analyzed, doc.early_exit_triggered
                )
                
                # Update results
                with results_lock:
                    results[doc.classification_result].append(doc)
                
                return doc
                
            except Exception as e:
                logger.error(f"[{thread_id}] Thorough mode failed for {doc.pdf_name}: {e}")
                doc.classification_result = "failed"
                doc.error_message = str(e)
                
                with results_lock:
                    results['failed'].append(doc)
                
                return doc
        
        # Execute with exactly 5 threads as required
        with ThreadPoolExecutor(max_workers=self.config.parallel_threads, 
                               thread_name_prefix="ThoroughWorker") as executor:
            
            futures = {executor.submit(process_single_document_thorough, doc): doc 
                      for doc in batch_documents}
            
            completed_docs = []
            for future in as_completed(futures):
                if self.shutdown_requested.is_set():
                    break
                
                try:
                    doc = future.result(timeout=self.config.api_timeout_seconds)
                    completed_docs.append(doc)
                except Exception as e:
                    logger.error(f"Thorough mode task failed: {e}")
        
        return completed_docs
    
    def _classify_document_thorough_mode(self, doc: DocumentMetadata, 
                                       progress_monitor: ThoroughModeProgressMonitor) -> ClassificationResult:
        """Execute thorough mode classification - exactly 10 pages, early exit enabled."""
        start_time = time.time()
        pdf_doc = None
        thread_id = doc.thread_id
        
        try:
            # Open PDF with thorough strategies
            progress_monitor.update_document_stage(thread_id, doc.pdf_name, 0, 10, "Opening PDF")
            pdf_doc = self.pdf_handler.attempt_pdf_open_thorough(doc.source_path)
            
            if pdf_doc is None:
                doc.pdf_complexity_level = "problematic"
                doc.pdf_issues = ["unable_to_open"]
                return ClassificationResult(
                    False, stage_failed="Unable to open PDF for thorough analysis",
                    processing_time=time.time() - start_time, total_pages_available=0
                )
            
            total_pages_available = len(pdf_doc)
            if total_pages_available == 0:
                return ClassificationResult(
                    False, stage_failed="PDF has no pages for thorough analysis",
                    processing_time=time.time() - start_time, total_pages_available=0
                )
            
            # Get exactly 10 pages (or fewer if document is smaller) using thorough strategy
            pages_to_analyze = self.pdf_handler.get_thorough_page_selection(pdf_doc)
            
            if not pages_to_analyze:
                return ClassificationResult(
                    False, stage_failed="No accessible pages for thorough analysis",
                    processing_time=time.time() - start_time, total_pages_available=total_pages_available
                )
            
            logger.debug(f"[{thread_id}] Thorough analysis: {len(pages_to_analyze)} pages from {total_pages_available} total")
            
            pages_checked = 0
            
            # Thorough analysis: check each of the selected pages
            for i, page_idx in enumerate(pages_to_analyze):
                if self.shutdown_requested.is_set():
                    break
                
                progress_monitor.update_document_stage(
                    thread_id, doc.pdf_name, i + 1, len(pages_to_analyze), 
                    f"Analyzing page {page_idx + 1}"
                )
                
                page = self.pdf_handler.safe_page_access(pdf_doc, page_idx)
                if page is None:
                    continue
                
                pages_checked += 1
                
                # Stage 1: Healthcare contract detection
                if not self._is_healthcare_contract_thorough(page, thread_id):
                    continue
                
                # Stage 2: Date extraction with early exit
                progress_monitor.update_document_stage(
                    thread_id, doc.pdf_name, i + 1, len(pages_to_analyze), 
                    f"Extracting date from page {page_idx + 1}"
                )
                
                date_result = self._get_effective_date_thorough(page, thread_id)
                processing_time = time.time() - start_time
                
                if date_result and date_result.year >= self.config.min_effective_year:
                    # SUCCESS - Early exit triggered
                    logger.info(f"[{thread_id}] Thorough mode SUCCESS: {doc.pdf_name} - Early exit on page {page_idx + 1}")
                    return ClassificationResult(
                        True, page_idx + 1, date_result, confidence=0.95,
                        processing_time=processing_time, pages_checked=pages_checked,
                        early_exit_triggered=True, total_pages_available=total_pages_available,
                        pages_skipped=len(pages_to_analyze) - pages_checked
                    )
                elif date_result:
                    # Found date but too old - Early exit
                    logger.info(f"[{thread_id}] Thorough mode: {doc.pdf_name} - Date too old ({date_result.year})")
                    return ClassificationResult(
                        False, page_idx + 1, date_result,
                        stage_failed=f"Effective date before {self.config.min_effective_year} ({date_result.year})",
                        confidence=0.9, processing_time=processing_time,
                        pages_checked=pages_checked, early_exit_triggered=True,
                        total_pages_available=total_pages_available,
                        pages_skipped=len(pages_to_analyze) - pages_checked
                    )
                else:
                    # Healthcare contract found but no date - Early exit
                    logger.info(f"[{thread_id}] Thorough mode: {doc.pdf_name} - No effective date found")
                    return ClassificationResult(
                        False, page_idx + 1, None,
                        stage_failed="Healthcare contract found but no effective date",
                        confidence=0.8, processing_time=processing_time,
                        pages_checked=pages_checked, early_exit_triggered=True,
                        total_pages_available=total_pages_available,
                        pages_skipped=len(pages_to_analyze) - pages_checked
                    )
            
            # Completed full thorough analysis - no healthcare contract found
            logger.info(f"[{thread_id}] Thorough mode complete: {doc.pdf_name} - Not healthcare contract ({pages_checked} pages analyzed)")
            return ClassificationResult(
                False, stage_failed="Not a healthcare contract after thorough analysis",
                confidence=0.95, processing_time=time.time() - start_time,
                pages_checked=pages_checked, early_exit_triggered=False,
                total_pages_available=total_pages_available, pages_skipped=0
            )
            
        except Exception as e:
            logger.error(f"[{thread_id}] Critical error in thorough mode: {e}")
            raise RuntimeError(f"Thorough mode processing failed: {doc.pdf_name}") from e
        finally:
            if pdf_doc:
                try:
                    pdf_doc.close()
                except:
                    pass
    
    def _is_healthcare_contract_thorough(self, page: fitz.Page, thread_id: str) -> bool:
        """Healthcare contract detection optimized for thorough mode."""
        try:
            client = self.pdf_handler.get_thorough_client()
            
            # Create image for analysis
            mat = fitz.Matrix(self.config.image_dpi / 72, self.config.image_dpi / 72)
            pix = page.get_pixmap(matrix=mat)
            
            if pix.width == 0 or pix.height == 0:
                return False
            
            image_bytes = pix.tobytes("png")
            if not image_bytes:
                return False
            
            # API call with thorough mode timeout
            response = client.models.generate_content(
                model='gemini-1.5-flash',
                contents=[
                    types.Part.from_bytes(data=image_bytes, mime_type='image/png'),
                    self.config.healthcare_contract_prompt
                ]
            )
            
            self.api_calls_made += 1
            result = response.text.strip().upper()
            return 'YES' in result
            
        except Exception as e:
            logger.debug(f"[{thread_id}] Healthcare contract detection failed: {e}")
            return False
    
    def _get_effective_date_thorough(self, page: fitz.Page, thread_id: str) -> Optional[datetime]:
        """Effective date extraction optimized for thorough mode."""
        try:
            client = self.pdf_handler.get_thorough_client()
            
            # Create image for analysis
            mat = fitz.Matrix(self.config.image_dpi / 72, self.config.image_dpi / 72)
            pix = page.get_pixmap(matrix=mat)
            
            if pix.width == 0 or pix.height == 0:
                return None
            
            image_bytes = pix.tobytes("png")
            if not image_bytes:
                return None
            
            # API call with thorough mode timeout
            response = client.models.generate_content(
                model='gemini-1.5-flash',
                contents=[
                    types.Part.from_bytes(data=image_bytes, mime_type='image/png'),
                    self.config.effective_date_prompt
                ]
            )
            
            self.api_calls_made += 1
            date_text = response.text.strip()
            
            if not date_text or "NOT_FOUND" in date_text:
                return None
            
            return parser.parse(date_text)
            
        except (parser.ParserError, TypeError, Exception) as e:
            logger.debug(f"[{thread_id}] Date extraction failed: {e}")
            return None
    
    def _categorize_thorough_failure(self, stage_failed: str) -> str:
        """Categorize failure reasons for thorough mode."""
        stage_lower = stage_failed.lower()
        
        if "unable to open" in stage_lower or "no pages" in stage_lower:
            return "pdf_corrupted"
        elif "effective date before" in stage_lower:
            return "old_date"
        elif "no effective date" in stage_lower:
            return "no_date"
        elif "not a healthcare contract" in stage_lower:
            return "not_healthcare"
        else:
            return "other_failure"
    
    def _generate_thorough_results(self, results: Dict, all_documents: List[DocumentMetadata], 
                                 documents_processed: int) -> Dict[str, Any]:
        """Generate comprehensive thorough mode results."""
        
        # Calculate thorough mode statistics
        total_pages_analyzed = sum(doc.pages_analyzed for doc in all_documents if hasattr(doc, 'pages_analyzed'))
        early_exits = sum(1 for doc in all_documents if getattr(doc, 'early_exit_triggered', False))
        total_processing_time = sum(doc.processing_time for doc in all_documents if doc.processing_time > 0)
        
        # Performance metrics
        avg_processing_time = total_processing_time / max(documents_processed, 1)
        avg_pages_per_doc = total_pages_analyzed / max(documents_processed, 1)
        early_exit_rate = (early_exits / max(documents_processed, 1)) * 100
        
        # Session duration
        if hasattr(self, 'session_start_time'):
            session_duration = datetime.now() - self.session_start_time
        else:
            session_duration = timedelta(seconds=total_processing_time)
        
        actual_duration_hours = session_duration.total_seconds() / 3600
        target_vs_actual = actual_duration_hours / self.config.target_duration_hours
        
        # Generate comprehensive results
        thorough_results = {
            'classification_results': dict(results),
            'thorough_mode_summary': {
                'mode': 'THOROUGH_MODE',
                'pages_per_document': self.config.pages_per_document,
                'parallel_threads': self.config.parallel_threads,
                'early_exit_enabled': self.config.enable_early_exit,
                'target_duration_hours': self.config.target_duration_hours,
                'actual_duration_hours': actual_duration_hours,
                'target_vs_actual_ratio': target_vs_actual,
                'duration_status': 'ON_TARGET' if 0.8 <= target_vs_actual <= 1.2 else 'OFF_TARGET'
            },
            'processing_statistics': {
                'total_documents_discovered': len(all_documents),
                'documents_processed': documents_processed,
                'total_pages_analyzed': total_pages_analyzed,
                'average_pages_per_document': avg_pages_per_doc,
                'early_exits_triggered': early_exits,
                'early_exit_rate_percentage': early_exit_rate,
                'average_processing_time_per_document': avg_processing_time,
                'total_api_calls_made': self.api_calls_made,
                'successful_classifications': len(results.get('relevant', [])),
                'success_rate_percentage': (len(results.get('relevant', [])) / max(documents_processed, 1)) * 100
            },
            'performance_metrics': {
                'documents_per_hour': documents_processed / max(actual_duration_hours, 0.01),
                'pages_per_hour': total_pages_analyzed / max(actual_duration_hours, 0.01),
                'api_calls_per_hour': self.api_calls_made / max(actual_duration_hours, 0.01),
                'average_pages_analyzed_per_relevant_doc': total_pages_analyzed / max(len(results.get('relevant', [])), 1),
                'time_efficiency': 'EFFICIENT' if avg_processing_time < 30 else 'MODERATE' if avg_processing_time < 60 else 'SLOW'
            },
            'classification_breakdown': {
                category: len(docs) for category, docs in results.items()
            }
        }
        
        return thorough_results

# ==========================================
# THOROUGH MODE ORCHESTRATION SYSTEM
# ==========================================

class ThoroughModeHealthcareClassificationSystem:
    """Main orchestration system for thorough mode - implements exact requirements."""
    
    def __init__(self, client, output_dir: str = "thorough_mode_results"):
        self.client = client
        self.config = ThoroughModeConfig()
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.session_start_time = datetime.now()
    
    def run_thorough_mode_classification(self, root_folder_path: str) -> Dict[str, Any]:
        """Execute thorough mode classification with exact requirements."""
        
        logger.info("=" * 120)
        logger.info("THOROUGH MODE HEALTHCARE CLASSIFICATION SYSTEM")
        logger.info("=" * 120)
        logger.info(f"✓ Pages per document: {self.config.pages_per_document}")
        logger.info(f"✓ Parallel threads: {self.config.parallel_threads}")
        logger.info(f"✓ Early exit on first match: {self.config.enable_early_exit}")
        logger.info(f"✓ Target duration: {self.config.target_duration_hours} hours (15-20 hour range)")
        logger.info("=" * 120)
        
        try:
            # Step 1: Document discovery
            discovery = ThoroughModeFileDiscovery(root_folder_path, self.config)
            all_documents = discovery.discover_all_documents()
            
            if not all_documents:
                logger.error("No documents found for thorough mode processing")
                return {'success': False, 'error': 'No documents found'}
            
            # Step 2: Thorough mode classification
            classifier = ThoroughModeHealthcareClassifier(self.client, self.config)
            classifier.session_start_time = self.session_start_time
            results = classifier.classify_all_documents_thorough(all_documents)
            
            # Step 3: Save thorough mode reports
            output_files = self._save_thorough_reports(all_documents, results)
            
            end_time = datetime.now()
            total_duration = end_time - self.session_start_time
            total_hours = total_duration.total_seconds() / 3600
            
            logger.info("=" * 120)
            logger.info("THOROUGH MODE CLASSIFICATION COMPLETED")
            logger.info(f"Actual duration: {total_hours:.2f} hours (Target: {self.config.target_duration_hours}h)")
            logger.info("=" * 120)
            
            # Print thorough mode summary
            self._print_thorough_summary(results, total_hours)
            
            return {
                'success': True,
                'mode': 'THOROUGH_MODE',
                'output_directory': str(self.output_dir),
                'output_files': output_files,
                'results': results,
                'actual_duration_hours': total_hours,
                'target_duration_hours': self.config.target_duration_hours,
                'met_time_target': 15.0 <= total_hours <= 20.0
            }
            
        except Exception as e:
            logger.error(f"Critical error in thorough mode: {e}", exc_info=True)
            return {'success': False, 'error': str(e)}
    
    def _save_thorough_reports(self, all_docs: List[DocumentMetadata], 
                              results: Dict[str, Any]) -> Dict[str, str]:
        """Save comprehensive thorough mode reports."""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_files = {}
        
        # Main thorough mode report
        report_path = self.output_dir / f"thorough_mode_report_{timestamp}.csv"
        with open(report_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                'source_path', 'pdf_name', 'state', 'classification_result',
                'effective_date', 'detected_on_page', 'stage_failed', 'confidence_score',
                'processing_time', 'pages_analyzed', 'early_exit_triggered',
                'thread_id', 'file_size_mb'
            ])
            
            for doc in all_docs:
                writer.writerow([
                    doc.source_path, doc.pdf_name, doc.state, doc.classification_result,
                    doc.effective_date.strftime('%Y-%m-%d') if doc.effective_date else '',
                    doc.detected_on_page or '', doc.stage_failed, f"{doc.confidence_score:.3f}",
                    f"{doc.processing_time:.2f}", getattr(doc, 'pages_analyzed', 0),
                    getattr(doc, 'early_exit_triggered', False), doc.thread_id,
                    f"{doc.file_size / (1024*1024):.1f}"
                ])
        
        output_files['thorough_report'] = str(report_path)
        
        # Thorough mode analytics
        analytics_path = self.output_dir / f"thorough_mode_analytics_{timestamp}.json"
        with open(analytics_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, default=str)
        output_files['analytics'] = str(analytics_path)
        
        # Relevant documents for extraction pipeline
        relevant_docs = results['classification_results'].get('relevant', [])
        if relevant_docs:
            relevant_path = self.output_dir / f"thorough_mode_relevant_documents_{timestamp}.txt"
            with open(relevant_path, 'w', encoding='utf-8') as f:
                for doc in relevant_docs:
                    f.write(f"{doc.source_path}\n")
            output_files['relevant_list'] = str(relevant_path)
            
            # Detailed relevant documents report
            relevant_details_path = self.output_dir / f"thorough_mode_relevant_details_{timestamp}.csv"
            with open(relevant_details_path, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow([
                    'source_path', 'pdf_name', 'state', 'effective_date', 'detected_on_page',
                    'confidence_score', 'processing_time', 'pages_analyzed', 'early_exit_triggered'
                ])
                
                for doc in relevant_docs:
                    writer.writerow([
                        doc.source_path, doc.pdf_name, doc.state,
                        doc.effective_date.strftime('%Y-%m-%d') if doc.effective_date else '',
                        doc.detected_on_page or '', f"{doc.confidence_score:.3f}",
                        f"{doc.processing_time:.2f}", getattr(doc, 'pages_analyzed', 0),
                        getattr(doc, 'early_exit_triggered', False)
                    ])
            output_files['relevant_details'] = str(relevant_details_path)
        
        logger.info(f"Thorough mode reports saved to: {self.output_dir}")
        return output_files
    
    def _print_thorough_summary(self, results: Dict[str, Any], actual_hours: float):
        """Print comprehensive thorough mode summary."""
        summary = results.get('thorough_mode_summary', {})
        stats = results.get('processing_statistics', {})
        performance = results.get('performance_metrics', {})
        breakdown = results.get('classification_breakdown', {})
        
        print(f"\n{'='*120}")
        print("THOROUGH MODE CLASSIFICATION SUMMARY")
        print(f"{'='*120}")
        
        # Thorough mode verification
        print("THOROUGH MODE REQUIREMENTS:")
        print(f"  ✓ Pages per document: {summary.get('pages_per_document', 0)}")
        print(f"  ✓ Parallel threads: {summary.get('parallel_threads', 0)}")
        print(f"  ✓ Early exit enabled: {summary.get('early_exit_enabled', False)}")
        print(f"  ✓ Target duration: {summary.get('target_duration_hours', 0)} hours")
        
        # Time performance
        target_hours = summary.get('target_duration_hours', 18)
        time_status = "✓ ON TARGET" if 15 <= actual_hours <= 20 else "⚠ OFF TARGET"
        print(f"\nTIME PERFORMANCE:")
        print(f"  Target: {target_hours} hours (15-20 hour range)")
        print(f"  Actual: {actual_hours:.2f} hours")
        print(f"  Status: {time_status}")
        
        # Processing statistics
        print(f"\nPROCESSING STATISTICS:")
        print(f"  Documents Processed: {stats.get('documents_processed', 0):,}")
        print(f"  Total Pages Analyzed: {stats.get('total_pages_analyzed', 0):,}")
        print(f"  Average Pages/Document: {stats.get('average_pages_per_document', 0):.1f}")
        print(f"  Early Exits Triggered: {stats.get('early_exits_triggered', 0):,} ({stats.get('early_exit_rate_percentage', 0):.1f}%)")
        print(f"  Relevant Documents Found: {stats.get('successful_classifications', 0):,}")
        print(f"  Success Rate: {stats.get('success_rate_percentage', 0):.1f}%")
        
        # Performance metrics
        print(f"\nPERFORMANCE METRICS:")
        print(f"  Documents/Hour: {performance.get('documents_per_hour', 0):.1f}")
        print(f"  Pages/Hour: {performance.get('pages_per_hour', 0):.0f}")
        print(f"  API Calls/Hour: {performance.get('api_calls_per_hour', 0):.0f}")
        print(f"  Avg Processing Time: {stats.get('average_processing_time_per_document', 0):.1f} seconds")
        
        # Classification breakdown
        print(f"\nCLASSIFICATION BREAKDOWN:")
        for category, count in sorted(breakdown.items()):
            print(f"  {category.replace('_', ' ').title()}: {count:,}")
        
        # Efficiency assessment
        efficiency = performance.get('time_efficiency', 'UNKNOWN')
        early_exit_effectiveness = "HIGH" if stats.get('early_exit_rate_percentage', 0) > 20 else "MODERATE" if stats.get('early_exit_rate_percentage', 0) > 10 else "LOW"
        
        print(f"\nEFFICIENCY ASSESSMENT:")
        print(f"  Processing Efficiency: {efficiency}")
        print(f"  Early Exit Effectiveness: {early_exit_effectiveness}")
        print(f"  Thoroughness: {stats.get('average_pages_per_document', 0):.1f}/{summary.get('pages_per_document', 10)} pages analyzed per document")
        
        print(f"{'='*120}")

# ==========================================
# CLIENT AND MAIN EXECUTION
# ==========================================

def create_client():
    """Create Google AI client for thorough mode."""
    return genai.Client(vertexai=True, project="anbc-hcb-dev", location="us-central1")

def run_thorough_mode_healthcare_classification(
    root_folder_path: str,
    output_dir: str = "thorough_mode_results"
) -> Dict[str, Any]:
    """
    Execute Thorough Mode Healthcare Classification with exact requirements:
    - 10 pages per document
    - 5 parallel threads
    - Early exit on first match
    - Target: 15-20 hours
    
    Args:
        root_folder_path: Path to root folder containing documents
        output_dir: Output directory for results
    
    Returns:
        Dictionary with comprehensive results and thorough mode metrics
    """
    
    client = create_client()
    system = ThoroughModeHealthcareClassificationSystem(client=client, output_dir=output_dir)
    
    return system.run_thorough_mode_classification(root_folder_path)

# ==========================================
# MAIN EXECUTION BLOCK
# ==========================================

if __name__ == "__main__":
    # Thorough Mode Configuration
    ROOT_FOLDER_PATH = r"\\Network\midp-sfs-007\midamreprc"
    OUTPUT_DIR = "thorough_mode_results"
    
    try:
        print("=" * 120)
        print("THOROUGH MODE HEALTHCARE CLASSIFICATION SYSTEM")
        print("=" * 120)
        print("REQUIREMENTS:")
        print("  • 10 pages per document analysis")
        print("  • 5 parallel processing threads")
        print("  • Early exit on first match")
        print("  • Target completion: 15-20 hours")
        print("=" * 120)
        
        # Execute thorough mode
        results = run_thorough_mode_healthcare_classification(
            root_folder_path=ROOT_FOLDER_PATH,
            output_dir=OUTPUT_DIR
        )
        
        if results['success']:
            print(f"\n{'='*120}")
            print("THOROUGH MODE COMPLETED SUCCESSFULLY!")
            print(f"{'='*120}")
            print(f"Output Directory: {results['output_directory']}")
            print(f"Actual Duration: {results['actual_duration_hours']:.2f} hours")
            print(f"Target Met: {'YES' if results['met_time_target'] else 'NO'}")
            print(f"Mode: {results['mode']}")
            
            # Key statistics
            stats = results['results'].get('processing_statistics', {})
            print(f"\nKEY RESULTS:")
            print(f"  • Documents Processed: {stats.get('documents_processed', 0):,}")
            print(f"  • Pages Analyzed: {stats.get('total_pages_analyzed', 0):,}")
            print(f"  • Relevant Documents: {stats.get('successful_classifications', 0):,}")
            print(f"  • Early Exits: {stats.get('early_exits_triggered', 0):,} ({stats.get('early_exit_rate_percentage', 0):.1f}%)")
            
            print(f"\nUse the relevant documents list for your extraction pipeline")
            print(f"{'='*120}")
        else:
            print(f"\nERROR: {results.get('error', 'Unknown error occurred')}")
            
    except KeyboardInterrupt:
        print(f"\n{'='*120}")
        print("THOROUGH MODE GRACEFUL SHUTDOWN")
        print("Processing state has been saved and can be resumed")
        print("Session data preserved for incremental processing")
        print(f"{'='*120}")
        
    except Exception as e:
        print(f"\nCRITICAL ERROR IN THOROUGH MODE: {e}")
        print("\nTroubleshooting steps:")
        print("1. Verify network connectivity to document storage")
        print("2. Check Google AI credentials and project access")
        print("3. Review log file: thorough_classification.log")
        print("4. Ensure sufficient system resources for 5 parallel threads")
        print("5. Verify document folder structure exists")
        
    finally:
        print(f"\nTHOROUGH MODE SESSION COMPLETE")
        print(f"Log file: thorough_classification.log")
        print(f"State file: thorough_mode_state.json")
        print(f"Resume capability: Enabled")
        print(f"\nMode Specifications:")
        print(f"  ✓ 10 pages analyzed per document")
        print(f"  ✓ 5 parallel threads for processing")
        print(f"  ✓ Early exit strategy implemented")
        print(f"  ✓ 15-20 hour target duration with adaptive pacing")
        print(f"  ✓ Comprehensive thorough mode analytics")
