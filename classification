import fitz
import csv
import json
import logging
import os
import time
from pathlib import Path
from typing import List, Optional, Dict, Any, Tuple
from google.genai import types
import google.genai as genai
from datetime import datetime
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed
from dateutil import parser
import re

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('healthcare_classification.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ==========================================
# DATA STRUCTURES
# ==========================================

@dataclass
class DocumentMetadata:
    """Metadata for each discovered document."""
    file_path: Path
    source_path: str
    pdf_name: str
    state: str
    file_size: int
    last_modified: datetime
    classification_status: str = "pending"
    classification_result: str = "unknown"
    detected_on_page: Optional[int] = None
    effective_date: Optional[datetime] = None
    stage_failed: str = ""
    confidence_score: float = 0.0
    processing_time: float = 0.0
    error_message: str = ""

@dataclass
class HealthcareContractResult:
    """Result from Stage 1: Healthcare contract detection."""
    is_relevant: bool
    confidence: float
    ai_response: str

@dataclass
class EffectiveDateResult:
    """Result from Stage 2: Effective date validation."""
    is_valid: bool
    date: Optional[datetime]
    reason: str
    raw_date_text: str = ""

@dataclass
class ClassificationResult:
    """Final classification result."""
    is_relevant: bool
    detected_on_page: Optional[int] = None
    effective_date: Optional[datetime] = None
    stage_failed: str = ""
    stage_passed: str = ""
    confidence: float = 0.0
    processing_time: float = 0.0
    total_pages_checked: int = 0

# ==========================================
# CLASSIFICATION CONFIGURATION
# ==========================================

class ClassificationConfig:
    """Configuration for healthcare document classification."""
    
    def __init__(self):
        # File discovery settings
        self.agreements_folder = "AGREEMENTS-NON STANDARD"
        self.hospital_subfolder = "HOSPITAL"
        self.target_extensions = ['.pdf']
        
        # Page analysis settings
        self.first_pages_count = 10
        self.last_pages_count = 10
        self.image_dpi = 150
        
        # Date filtering
        self.min_effective_year = 2020
        
        # Processing settings
        self.max_workers = 3
        
        # US States list for folder identification
        self.us_states = {
            'ALABAMA', 'ALASKA', 'ARIZONA', 'ARKANSAS', 'CALIFORNIA',
            'COLORADO', 'CONNECTICUT', 'DELAWARE', 'FLORIDA', 'GEORGIA',
            'HAWAII', 'IDAHO', 'ILLINOIS', 'INDIANA', 'IOWA',
            'KANSAS', 'KENTUCKY', 'LOUISIANA', 'MAINE', 'MARYLAND',
            'MASSACHUSETTS', 'MICHIGAN', 'MINNESOTA', 'MISSISSIPPI', 'MISSOURI',
            'MONTANA', 'NEBRASKA', 'NEVADA', 'NEW HAMPSHIRE', 'NEW JERSEY',
            'NEW MEXICO', 'NEW YORK', 'NORTH CAROLINA', 'NORTH DAKOTA', 'OHIO',
            'OKLAHOMA', 'OREGON', 'PENNSYLVANIA', 'RHODE ISLAND', 'SOUTH CAROLINA',
            'SOUTH DAKOTA', 'TENNESSEE', 'TEXAS', 'UTAH', 'VERMONT',
            'VIRGINIA', 'WASHINGTON', 'WEST VIRGINIA', 'WISCONSIN', 'WYOMING',
            'PUERTO RICO', 'DISTRICT OF COLUMBIA'
        }
        
        # AI Prompts
        self.healthcare_contract_prompt = """Analyze the image. Is this a formal letter from Aetna about a 'charge master increase' where they state 'we have received' a notification? Answer only YES or NO."""
        
        self.effective_date_prompt = """This is an Aetna charge master increase letter. Find the EFFECTIVE DATE mentioned in the letter (phrases like "effective January 1, 2024" or "to become effective 1/1/2025").

Respond with just the date found, or "NOT_FOUND" if no effective date exists.

Examples:
- January 1, 2024
- 1/1/2025
- NOT_FOUND"""

# ==========================================
# FILE DISCOVERY SYSTEM
# ==========================================

class EnterpriseFileDiscovery:
    """Discovers all healthcare documents in enterprise folder structure."""
    
    def __init__(self, root_path: str, config: ClassificationConfig):
        self.root_path = Path(root_path)
        self.config = config
        self.discovered_documents: List[DocumentMetadata] = []
        
    def discover_all_documents(self) -> List[DocumentMetadata]:
        """Discover all PDFs following systematic state-by-state, hospital-by-hospital approach."""
        logger.info(f"Starting systematic document discovery from: {self.root_path}")
        
        agreements_path = self.root_path / self.config.agreements_folder
        if not agreements_path.exists():
            logger.error(f"{self.config.agreements_folder} folder not found at: {agreements_path}")
            return []
        
        logger.info(f"Found AGREEMENTS-NON STANDARD folder")
        
        # Get all state folders first
        state_folders = []
        for folder in agreements_path.iterdir():
            if folder.is_dir() and self._is_state_folder(folder):
                state_folders.append(folder)
        
        logger.info(f"Found {len(state_folders)} state folders to process")
        
        # Process each state sequentially
        for state_index, state_folder in enumerate(state_folders, 1):
            logger.info(f"\n{'='*60}")
            logger.info(f"PROCESSING STATE {state_index}/{len(state_folders)}: {state_folder.name}")
            logger.info(f"{'='*60}")
            
            self._process_single_state(state_folder, state_index, len(state_folders))
        
        logger.info(f"\n{'='*80}")
        logger.info(f"DISCOVERY COMPLETED")
        logger.info(f"Total states processed: {len(state_folders)}")
        logger.info(f"Total documents discovered: {len(self.discovered_documents)}")
        logger.info(f"{'='*80}")
        
        return self.discovered_documents
    
    def _process_single_state(self, state_folder: Path, state_index: int, total_states: int):
        """Process a single state folder completely before moving to next state."""
        state_name = state_folder.name
        
        try:
            # Check for HOSPITAL subfolder with better error handling
            hospital_folder_path = state_folder / self.config.hospital_subfolder
            
            # Try different approaches to handle problematic folder names
            if not hospital_folder_path.exists():
                logger.warning(f"No HOSPITAL folder found in {state_name}")
                
                # Try alternative: look for any folder containing "HOSPITAL" in name
                try:
                    for item in state_folder.iterdir():
                        if item.is_dir() and "HOSPITAL" in item.name.upper():
                            logger.info(f"Found alternative hospital folder: {item.name}")
                            hospital_folder_path = item
                            break
                    else:
                        logger.warning(f"No hospital-related folders found in {state_name}")
                        return
                except Exception as e:
                    logger.error(f"Error scanning state folder {state_name}: {e}")
                    return
            
            logger.info(f"Found HOSPITAL folder in {state_name}")
            
            # Get all hospital subfolders with robust error handling
            try:
                hospital_subfolders = []
                
                for item in hospital_folder_path.iterdir():
                    if item.is_dir():
                        hospital_subfolders.append(item)
                        logger.debug(f"  Found subfolder: {item.name}")
                
                logger.info(f"Found {len(hospital_subfolders)} subfolders in {state_name}/HOSPITAL")
                
                if not hospital_subfolders:
                    logger.warning(f"No subfolders found in {state_name}/HOSPITAL")
                    return
                
                # Process each hospital subfolder sequentially
                for hospital_index, hospital_subfolder in enumerate(hospital_subfolders, 1):
                    try:
                        logger.info(f"\n  Processing Hospital {hospital_index}/{len(hospital_subfolders)}: {hospital_subfolder.name}")
                        
                        docs_before = len(self.discovered_documents)
                        
                        # Process this hospital folder with better error handling
                        try:
                            self._process_single_hospital_folder(hospital_subfolder, state_name, hospital_index, len(hospital_subfolders))
                        except Exception as processing_error:
                            logger.error(f"    Error during processing: {processing_error}")
                            continue
                        
                        docs_after = len(self.discovered_documents)
                        docs_found = docs_after - docs_before
                        
                        if docs_found > 0:
                            logger.info(f"    Successfully processed {docs_found} PDFs from {hospital_subfolder.name}")
                        else:
                            logger.warning(f"    No PDFs were successfully processed from {hospital_subfolder.name}")
                    
                    except Exception as hospital_error:
                        logger.error(f"    Error accessing hospital {hospital_subfolder.name}: {hospital_error}")
                        continue
                
                # Summary for this state
                total_state_docs = sum(1 for doc in self.discovered_documents if doc.state == state_name)
                logger.info(f"\nSTATE {state_index} SUMMARY:")
                logger.info(f"  State: {state_name}")
                logger.info(f"  Hospital subfolders processed: {len(hospital_subfolders)}")
                logger.info(f"  Total PDFs found in state: {total_state_docs}")
                
            except PermissionError as pe:
                logger.error(f"Permission denied accessing HOSPITAL folder for {state_name}: {pe}")
            except OSError as ose:
                logger.error(f"OS Error accessing HOSPITAL folder for {state_name}: {ose}")
            except Exception as e:
                logger.error(f"Error listing hospital subfolders for {state_name}: {e}")
                
        except Exception as state_error:
            logger.error(f"Critical error processing state {state_name}: {state_error}")
    
    def _process_single_hospital_folder(self, hospital_subfolder: Path, state_name: str, hospital_index: int, total_hospitals: int):
        """Process all PDFs in a single hospital subfolder completely - GUARANTEED to find all PDFs."""
        hospital_name = hospital_subfolder.name
        logger.debug(f"    Starting COMPREHENSIVE processing of: {hospital_name}")
        
        try:
            # Check if we can access the folder first
            if not hospital_subfolder.exists():
                logger.warning(f"    Hospital folder does not exist: {hospital_name}")
                return
            
            if not hospital_subfolder.is_dir():
                logger.warning(f"    Hospital path is not a directory: {hospital_name}")
                return
            
            # STRATEGY: Use completely separate, independent methods to ensure we don't miss anything
            all_pdf_files = []
            processed_paths = set()
            
            # METHOD 1: Use glob to find ALL PDFs (most reliable)
            method1_pdfs = []
            try:
                logger.debug(f"    METHOD 1: Using glob to find direct PDFs")
                direct_glob_pdfs = list(hospital_subfolder.glob("*.pdf"))
                for pdf in direct_glob_pdfs:
                    if str(pdf) not in processed_paths:
                        method1_pdfs.append(pdf)
                        processed_paths.add(str(pdf))
                logger.info(f"    METHOD 1 found {len(method1_pdfs)} direct PDFs")
            except Exception as e:
                logger.debug(f"    METHOD 1 failed: {e}")
            
            # METHOD 2: Use rglob to find ALL PDFs recursively (backup)
            method2_pdfs = []
            try:
                logger.debug(f"    METHOD 2: Using rglob to find all PDFs recursively")
                recursive_pdfs = list(hospital_subfolder.rglob("*.pdf"))
                for pdf in recursive_pdfs:
                    if str(pdf) not in processed_paths:
                        method2_pdfs.append(pdf)
                        processed_paths.add(str(pdf))
                logger.info(f"    METHOD 2 found {len(method2_pdfs)} additional PDFs")
            except Exception as e:
                logger.debug(f"    METHOD 2 failed: {e}")
            
            # METHOD 3: Manual iteration (absolutely guaranteed)
            method3_pdfs = []
            try:
                logger.debug(f"    METHOD 3: Manual iteration to catch anything missed")
                all_items = []
                
                # Force complete directory listing
                for item in hospital_subfolder.iterdir():
                    all_items.append(item)
                
                logger.debug(f"    Found {len(all_items)} total items in directory")
                
                # Process ONLY PDF files first
                pdf_files_found = []
                folders_found = []
                
                for item in all_items:
                    try:
                        if item.is_file():
                            if item.suffix.lower() == '.pdf':
                                pdf_files_found.append(item)
                                logger.debug(f"      Found PDF file: {item.name}")
                        elif item.is_dir():
                            folders_found.append(item)
                            logger.debug(f"      Found folder: {item.name}")
                    except Exception as item_error:
                        logger.debug(f"      Error checking item {item}: {item_error}")
                        continue
                
                logger.info(f"    Manual method found {len(pdf_files_found)} direct PDF files")
                logger.info(f"    Manual method found {len(folders_found)} folders")
                
                # Add PDFs that weren't found by other methods
                for pdf in pdf_files_found:
                    if str(pdf) not in processed_paths:
                        method3_pdfs.append(pdf)
                        processed_paths.add(str(pdf))
                
                # Now process folders for additional PDFs
                for folder in folders_found:
                    try:
                        folder_pdfs = list(folder.rglob("*.pdf"))
                        for pdf in folder_pdfs:
                            if str(pdf) not in processed_paths:
                                method3_pdfs.append(pdf)
                                processed_paths.add(str(pdf))
                        if len(folder_pdfs) > 0:
                            logger.info(f"      Folder {folder.name} contains {len(folder_pdfs)} PDFs")
                    except Exception as folder_error:
                        logger.debug(f"      Error processing folder {folder.name}: {folder_error}")
                        continue
                
                logger.info(f"    METHOD 3 found {len(method3_pdfs)} additional PDFs")
                
            except Exception as e:
                logger.debug(f"    METHOD 3 failed: {e}")
            
            # Combine all methods - but convert to string paths immediately
            all_pdf_paths = []
            
            # Convert all Path objects to strings to avoid Windows network path issues
            for pdf in method1_pdfs:
                all_pdf_paths.append(str(pdf))
            for pdf in method2_pdfs:
                all_pdf_paths.append(str(pdf))
            for pdf in method3_pdfs:
                all_pdf_paths.append(str(pdf))
            
            # Remove any duplicates based on string path
            unique_pdf_paths = list(set(all_pdf_paths))
            
            total_found = len(unique_pdf_paths)
            logger.info(f"    TOTAL UNIQUE PDFs found in {hospital_name}: {total_found}")
            logger.info(f"      Method 1 (direct glob): {len(method1_pdfs)}")
            logger.info(f"      Method 2 (recursive): {len(method2_pdfs)}")
            logger.info(f"      Method 3 (manual): {len(method3_pdfs)}")
            
            if total_found == 0:
                logger.warning(f"    NO PDFs found anywhere in {hospital_name} using any method!")
                return
            
            # Process all found PDFs using string paths
            processed_count = 0
            failed_count = 0
            
            for pdf_index, pdf_path_str in enumerate(unique_pdf_paths, 1):
                logger.debug(f"      Processing PDF {pdf_index}/{total_found}: {Path(pdf_path_str).name}")
                
                try:
                    # Step 1: Create fresh Path object from string
                    logger.debug(f"        Step 1: Creating Path object from string")
                    pdf_file = Path(pdf_path_str)
                    logger.debug(f"        Path created: {pdf_path_str[:100]}...")
                    
                    # Step 2: Verify file exists using both Path and os.path methods
                    logger.debug(f"        Step 2: Checking if file exists")
                    
                    # Try multiple existence checks
                    exists_via_path = pdf_file.exists()
                    exists_via_os = os.path.exists(pdf_path_str)
                    
                    logger.debug(f"        Path.exists(): {exists_via_path}")
                    logger.debug(f"        os.path.exists(): {exists_via_os}")
                    
                    if not exists_via_path and not exists_via_os:
                        logger.error(f"        FAILED Step 2: File does not exist via any method: {pdf_file.name}")
                        failed_count += 1
                        continue
                    
                    # Use os.path methods for network paths (more reliable)
                    if exists_via_os:
                        logger.debug(f"        Step 3: Using os.path methods for file operations")
                        
                        # Step 3a: Get file size using os.path
                        try:
                            import os
                            file_size = os.path.getsize(pdf_path_str)
                            logger.debug(f"        File size: {file_size} bytes")
                        except Exception as size_error:
                            logger.error(f"        FAILED Step 3a: Cannot get file size: {size_error}")
                            failed_count += 1
                            continue
                        
                        # Step 3b: Get modification time using os.path
                        try:
                            import os
                            mtime = os.path.getmtime(pdf_path_str)
                            last_modified = datetime.fromtimestamp(mtime)
                            logger.debug(f"        Last modified: {last_modified}")
                        except Exception as time_error:
                            logger.error(f"        FAILED Step 3b: Cannot get modification time: {time_error}")
                            failed_count += 1
                            continue
                    
                    else:
                        # Fallback to Path methods
                        logger.debug(f"        Step 3: Using Path methods as fallback")
                        try:
                            file_size = pdf_file.stat().st_size
                            last_modified = datetime.fromtimestamp(pdf_file.stat().st_mtime)
                            logger.debug(f"        File size: {file_size}, Modified: {last_modified}")
                        except Exception as stat_error:
                            logger.error(f"        FAILED Step 3: Cannot get file stats: {stat_error}")
                            failed_count += 1
                            continue
                    
                    # Step 4: Create document metadata
                    logger.debug(f"        Step 4: Creating DocumentMetadata object")
                    try:
                        doc_metadata = DocumentMetadata(
                            file_path=pdf_file,
                            source_path=pdf_path_str,
                            pdf_name=pdf_file.name,
                            state=state_name,
                            file_size=file_size,
                            last_modified=last_modified
                        )
                        logger.debug(f"        DocumentMetadata created successfully")
                    except Exception as metadata_error:
                        logger.error(f"        FAILED Step 4: Cannot create metadata: {metadata_error}")
                        failed_count += 1
                        continue
                    
                    # Step 5: Add to discovered documents list
                    logger.debug(f"        Step 5: Adding to discovered documents list")
                    try:
                        self.discovered_documents.append(doc_metadata)
                        processed_count += 1
                        logger.debug(f"        SUCCESS: Added {pdf_file.name} to discovered documents")
                    except Exception as append_error:
                        logger.error(f"        FAILED Step 5: Cannot add to list: {append_error}")
                        failed_count += 1
                        continue
                
                except Exception as pdf_error:
                    logger.error(f"        CRITICAL ERROR processing {Path(pdf_path_str).name}: {pdf_error}")
                    logger.error(f"        Error type: {type(pdf_error).__name__}")
                    failed_count += 1
                    continue
            
            # Final summary for this hospital folder
            logger.info(f"    FINAL RESULT for {hospital_name}:")
            logger.info(f"      Total unique PDFs discovered: {total_found}")
            logger.info(f"      PDFs processed successfully: {processed_count}")
            logger.info(f"      PDFs failed: {failed_count}")
            
            if processed_count == 0 and total_found > 0:
                logger.error(f"    CRITICAL: Found {total_found} PDFs but processed 0 successfully!")
            elif processed_count < total_found:
                logger.warning(f"    WARNING: Only processed {processed_count}/{total_found} PDFs successfully")
            else:
                logger.info(f"    SUCCESS: All {processed_count} PDFs processed successfully")
            
        except Exception as e:
            logger.error(f"    CRITICAL error processing hospital folder {hospital_name}: {e}")
            logger.error(f"    Error type: {type(e).__name__}")
            return
    
    def _collect_pdfs_from_subfolder(self, subfolder_path: Path, processed_paths: set, pdf_list: List[Path]) -> int:
        """Recursively collect all PDFs from a subfolder and its subdirectories."""
        pdf_count = 0
        
        try:
            logger.debug(f"        Entering subfolder: {subfolder_path.name}")
            
            # Get all items in this subfolder
            try:
                subfolder_items = list(subfolder_path.iterdir())
                logger.debug(f"        Found {len(subfolder_items)} items in {subfolder_path.name}")
            except Exception as e:
                logger.debug(f"        Cannot list contents of {subfolder_path.name}: {e}")
                return 0
            
            # Process each item in the subfolder
            for item in subfolder_items:
                try:
                    if item.is_file() and item.suffix.lower() == '.pdf':
                        if str(item) not in processed_paths:
                            pdf_list.append(item)
                            processed_paths.add(str(item))
                            pdf_count += 1
                            logger.debug(f"          Added PDF: {item.name}")
                    elif item.is_dir():
                        # Recursively process subdirectories
                        logger.debug(f"          Entering subdirectory: {item.name}")
                        sub_count = self._collect_pdfs_from_subfolder(item, processed_paths, pdf_list)
                        pdf_count += sub_count
                        if sub_count > 0:
                            logger.debug(f"          Subdirectory {item.name} contributed {sub_count} PDFs")
                
                except Exception as item_error:
                    logger.debug(f"          Error processing item {item}: {item_error}")
                    continue
            
            logger.debug(f"        Subfolder {subfolder_path.name} total: {pdf_count} PDFs")
                    
        except Exception as e:
            logger.debug(f"        Error processing subfolder {subfolder_path.name}: {e}")
        
        return pdf_count
    
    def _manual_pdf_search(self, folder_path: Path, processed_paths: set) -> List[Path]:
        """Manually search for PDFs with better error handling for problematic paths."""
        pdf_files = []
        
        try:
            # First, get all PDFs directly in this folder
            for item in folder_path.iterdir():
                try:
                    if item.is_file() and item.suffix.lower() == '.pdf':
                        if str(item) not in processed_paths:
                            pdf_files.append(item)
                            processed_paths.add(str(item))
                    elif item.is_dir():
                        # Recursively search subfolders
                        subfolder_pdfs = self._manual_pdf_search(item, processed_paths)
                        pdf_files.extend(subfolder_pdfs)
                except Exception as item_error:
                    logger.debug(f"        Error processing item {item}: {item_error}")
                    continue
        except Exception as folder_error:
            logger.debug(f"      Error listing folder {folder_path}: {folder_error}")
        
        return pdf_files
    
    def _process_potential_hospital_directory(self, directory_path: Path, state_name: str, subfolder_name: str):
        """Process a directory that might contain hospital PDFs."""
        try:
            pdf_count = 0
            # Look for PDFs in this directory and subdirectories
            for pdf_file in directory_path.rglob("*.pdf"):
                try:
                    if pdf_file.exists() and pdf_file.is_file():
                        doc_metadata = DocumentMetadata(
                            file_path=pdf_file,
                            source_path=str(pdf_file),
                            pdf_name=pdf_file.name,
                            state=state_name,
                            file_size=pdf_file.stat().st_size,
                            last_modified=datetime.fromtimestamp(pdf_file.stat().st_mtime)
                        )
                        
                        self.discovered_documents.append(doc_metadata)
                        pdf_count += 1
                        logger.debug(f"Found PDF in {subfolder_name}: {pdf_file.name}")
                
                except Exception as file_error:
                    logger.debug(f"Error processing file {pdf_file}: {file_error}")
                    continue
            
            if pdf_count > 0:
                logger.info(f"  SUCCESS: Found {pdf_count} PDFs in {state_name}/{subfolder_name}")
            
        except Exception as e:
            logger.error(f"Error processing potential hospital directory {directory_path}: {e}")
    
    def _is_state_folder(self, folder_path: Path) -> bool:
        """Identify state folders by checking for US state names."""
        folder_name = folder_path.name.upper()
        
        # Check if any US state name appears in folder name
        return any(state in folder_name for state in self.config.us_states)
    
    def _process_hospital_directory(self, hospital_path: Path, state_name: str):
        """Process all PDF files in HOSPITAL directory and subdirectories."""
        try:
            # First, check if we can access the hospital directory
            if not hospital_path.exists():
                logger.warning(f"Hospital directory does not exist: {hospital_path}")
                return
                
            if not hospital_path.is_dir():
                logger.warning(f"Hospital path is not a directory: {hospital_path}")
                return
            
            # Try to list the contents first with shorter path handling
            try:
                # Use a shorter path representation for Windows long path issues
                hospital_path_str = str(hospital_path)
                if len(hospital_path_str) > 200:
                    logger.warning(f"Very long path detected for {state_name}: {len(hospital_path_str)} characters")
                
                items = list(hospital_path.iterdir())
                logger.info(f"Hospital directory {state_name} contains {len(items)} items")
                
                # Log first few items to see what's there
                for i, item in enumerate(items[:5]):
                    item_type = "DIR" if item.is_dir() else "FILE"
                    logger.info(f"  {item_type}: {item.name}")
                if len(items) > 5:
                    logger.info(f"  ... and {len(items) - 5} more items")
                    
            except PermissionError as pe:
                logger.error(f"Permission denied accessing hospital directory for {state_name}: {pe}")
                return
            except OSError as ose:
                logger.error(f"OS Error accessing hospital directory for {state_name}: {ose}")
                return
            except Exception as e:
                logger.error(f"Cannot list contents of hospital directory for {state_name}: {e}")
                return
            
            pdf_count = 0
            processed_folders = 0
            
            # Process each item in the hospital directory
            for item in items:
                try:
                    if item.is_dir():
                        processed_folders += 1
                        logger.debug(f"Processing hospital subfolder: {item.name}")
                        
                        # Look for PDFs in this hospital subfolder
                        try:
                            # Use non-recursive approach first for better compatibility
                            subfolder_pdfs = list(item.glob("*.pdf"))
                            if subfolder_pdfs:
                                logger.info(f"  Found {len(subfolder_pdfs)} PDFs in {item.name}")
                                
                                for pdf_file in subfolder_pdfs:
                                    try:
                                        if pdf_file.exists() and pdf_file.is_file():
                                            doc_metadata = DocumentMetadata(
                                                file_path=pdf_file,
                                                source_path=str(pdf_file),
                                                pdf_name=pdf_file.name,
                                                state=state_name,
                                                file_size=pdf_file.stat().st_size,
                                                last_modified=datetime.fromtimestamp(pdf_file.stat().st_mtime)
                                            )
                                            
                                            self.discovered_documents.append(doc_metadata)
                                            pdf_count += 1
                                            logger.debug(f"Added PDF: {pdf_file.name}")
                                    
                                    except Exception as pdf_error:
                                        logger.debug(f"Error processing PDF {pdf_file}: {pdf_error}")
                                        continue
                            
                            # Also try recursive search in this subfolder (if non-recursive found some)
                            if subfolder_pdfs:
                                try:
                                    recursive_pdfs = list(item.rglob("*.pdf"))
                                    additional_pdfs = [p for p in recursive_pdfs if p not in subfolder_pdfs]
                                    
                                    if additional_pdfs:
                                        logger.info(f"  Found {len(additional_pdfs)} additional PDFs in subfolders of {item.name}")
                                        
                                        for pdf_file in additional_pdfs:
                                            try:
                                                if pdf_file.exists() and pdf_file.is_file():
                                                    doc_metadata = DocumentMetadata(
                                                        file_path=pdf_file,
                                                        source_path=str(pdf_file),
                                                        pdf_name=pdf_file.name,
                                                        state=state_name,
                                                        file_size=pdf_file.stat().st_size,
                                                        last_modified=datetime.fromtimestamp(pdf_file.stat().st_mtime)
                                                    )
                                                    
                                                    self.discovered_documents.append(doc_metadata)
                                                    pdf_count += 1
                                            
                                            except Exception as pdf_error:
                                                logger.debug(f"Error processing recursive PDF {pdf_file}: {pdf_error}")
                                                continue
                                except Exception as recursive_error:
                                    logger.debug(f"Recursive search failed for {item.name}: {recursive_error}")
                        
                        except Exception as subfolder_error:
                            logger.debug(f"Error processing subfolder {item.name}: {subfolder_error}")
                            continue
                    
                    elif item.is_file() and item.suffix.lower() == '.pdf':
                        # Direct PDF in hospital folder
                        try:
                            doc_metadata = DocumentMetadata(
                                file_path=item,
                                source_path=str(item),
                                pdf_name=item.name,
                                state=state_name,
                                file_size=item.stat().st_size,
                                last_modified=datetime.fromtimestamp(item.stat().st_mtime)
                            )
                            
                            self.discovered_documents.append(doc_metadata)
                            pdf_count += 1
                            logger.debug(f"Added direct PDF: {item.name}")
                        
                        except Exception as pdf_error:
                            logger.debug(f"Error processing direct PDF {item}: {pdf_error}")
                            continue
                
                except Exception as item_error:
                    logger.debug(f"Error processing item {item}: {item_error}")
                    continue
            
            logger.info(f"Successfully processed {state_name}: {processed_folders} hospital folders, {pdf_count} PDFs found")
                
        except Exception as e:
            logger.error(f"Critical error processing hospital directory for {state_name}: {e}")
            logger.error(f"Hospital path: {hospital_path}")
            logger.error(f"Error type: {type(e).__name__}")
            logger.error(f"Path length: {len(str(hospital_path))} characters")

# ==========================================
# TWO-STAGE DOCUMENT CLASSIFIER
# ==========================================

class HealthcareDocumentClassifier:
    """Two-stage AI-powered healthcare document classifier."""
    
    def __init__(self, client, config: ClassificationConfig):
        self.client = client
        self.config = config
        self.classification_cache = {}
        
    def classify_all_documents(self, documents: List[DocumentMetadata], 
                             max_workers: int = 3) -> Dict[str, List[DocumentMetadata]]:
        """Classify all documents using systematic sequential processing."""
        logger.info(f"Starting systematic two-stage classification of {len(documents)} documents")
        logger.info(f"Stage 1: Healthcare contract detection")
        logger.info(f"Stage 2: Effective date validation (2020+)")
        
        # Group documents by state for systematic processing
        documents_by_state = {}
        for doc in documents:
            if doc.state not in documents_by_state:
                documents_by_state[doc.state] = []
            documents_by_state[doc.state].append(doc)
        
        logger.info(f"Documents grouped into {len(documents_by_state)} states")
        
        relevant_docs = []
        not_healthcare_docs = []
        old_date_docs = []
        no_date_docs = []
        failed_docs = []
        
        total_processed = 0
        
        # Process each state systematically
        for state_index, (state_name, state_docs) in enumerate(documents_by_state.items(), 1):
            logger.info(f"\n{'='*60}")
            logger.info(f"CLASSIFYING STATE {state_index}/{len(documents_by_state)}: {state_name}")
            logger.info(f"Documents in this state: {len(state_docs)}")
            logger.info(f"{'='*60}")
            
            # Group by hospital within the state
            docs_by_hospital = {}
            for doc in state_docs:
                hospital_folder = doc.file_path.parent.name
                if hospital_folder not in docs_by_hospital:
                    docs_by_hospital[hospital_folder] = []
                docs_by_hospital[hospital_folder].append(doc)
            
            # Process each hospital folder systematically
            for hospital_index, (hospital_name, hospital_docs) in enumerate(docs_by_hospital.items(), 1):
                logger.info(f"\n  Processing Hospital {hospital_index}/{len(docs_by_hospital)}: {hospital_name}")
                logger.info(f"  Documents in this hospital: {len(hospital_docs)}")
                
                # Process each document in this hospital folder
                for doc_index, doc in enumerate(hospital_docs, 1):
                    total_processed += 1
                    logger.info(f"    Classifying PDF {doc_index}/{len(hospital_docs)}: {doc.pdf_name}")
                    logger.info(f"    Overall progress: {total_processed}/{len(documents)}")
                    
                    try:
                        # Classify this single document
                        classification_result = self._classify_single_document(doc)
                        
                        # Update document metadata
                        doc.classification_status = "completed"
                        doc.confidence_score = classification_result.confidence
                        doc.detected_on_page = classification_result.detected_on_page
                        doc.effective_date = classification_result.effective_date
                        doc.stage_failed = classification_result.stage_failed
                        doc.processing_time = classification_result.processing_time
                        
                        if classification_result.is_relevant:
                            doc.classification_result = "relevant"
                            relevant_docs.append(doc)
                            logger.info(f"      RELEVANT: {doc.pdf_name} (Effective: {doc.effective_date.strftime('%Y-%m-%d') if doc.effective_date else 'N/A'})")
                        else:
                            # Determine specific rejection reason
                            if "Stage 1" in classification_result.stage_failed:
                                doc.classification_result = "not_healthcare"
                                not_healthcare_docs.append(doc)
                                logger.info(f"      Not healthcare contract: {doc.pdf_name}")
                            elif "before 2020" in classification_result.stage_failed:
                                doc.classification_result = "old_date"
                                old_date_docs.append(doc)
                                logger.info(f"      Old date: {doc.pdf_name} ({doc.effective_date.strftime('%Y-%m-%d') if doc.effective_date else 'N/A'})")
                            elif "No effective date" in classification_result.stage_failed:
                                doc.classification_result = "no_date"
                                no_date_docs.append(doc)
                                logger.info(f"      No effective date: {doc.pdf_name}")
                            else:
                                doc.classification_result = "not_relevant"
                                not_healthcare_docs.append(doc)
                                logger.info(f"      Not relevant: {doc.pdf_name}")
                            
                    except Exception as e:
                        logger.error(f"      Classification failed: {doc.pdf_name} - {e}")
                        doc.classification_status = "failed"
                        doc.classification_result = "failed"
                        doc.error_message = str(e)
                        failed_docs.append(doc)
                
                # Hospital summary
                hospital_relevant = sum(1 for doc in hospital_docs if doc.classification_result == "relevant")
                logger.info(f"  Hospital Summary: {hospital_relevant}/{len(hospital_docs)} relevant documents")
            
            # State summary
            state_relevant = sum(1 for doc in state_docs if doc.classification_result == "relevant")
            logger.info(f"\nSTATE {state_index} SUMMARY:")
            logger.info(f"  Relevant documents: {state_relevant}/{len(state_docs)}")
        
        logger.info(f"\n{'='*80}")
        logger.info(f"SYSTEMATIC CLASSIFICATION COMPLETED")
        logger.info(f"{'='*80}")
        logger.info(f"Total documents processed: {len(documents)}")
        logger.info(f"Relevant (healthcare + 2020+ date): {len(relevant_docs)}")
        logger.info(f"Not healthcare contracts: {len(not_healthcare_docs)}")
        logger.info(f"Old dates (before 2020): {len(old_date_docs)}")
        logger.info(f"No effective date found: {len(no_date_docs)}")
        logger.info(f"Failed classifications: {len(failed_docs)}")
        logger.info(f"{'='*80}")
        
        return {
            'relevant': relevant_docs,
            'not_healthcare': not_healthcare_docs,
            'old_date': old_date_docs,
            'no_date': no_date_docs,
            'failed': failed_docs
        }
    
    def _classify_single_document(self, doc: DocumentMetadata) -> ClassificationResult:
        """Two-stage classification of a single document."""
        start_time = time.time()
        
        try:
            # Check cache first
            cache_key = f"{doc.source_path}_{doc.last_modified.timestamp()}"
            if cache_key in self.classification_cache:
                cached_result = self.classification_cache[cache_key]
                cached_result.processing_time = time.time() - start_time
                return cached_result
            
            # Open PDF and determine pages to analyze
            pdf_doc = fitz.open(str(doc.file_path))
            total_pages = len(pdf_doc)
            pages_to_check = self._get_pages_to_analyze(total_pages)
            
            # STAGE 1: Healthcare contract detection
            for page_idx in pages_to_check:
                page = pdf_doc.load_page(page_idx)
                
                # Check if this page contains healthcare contract
                healthcare_result = self._check_healthcare_contract(page, page_idx + 1)
                
                if healthcare_result.is_relevant:
                    # Found healthcare contract - proceed to Stage 2
                    logger.debug(f"Healthcare contract found on page {page_idx + 1} of {doc.pdf_name}")
                    
                    # STAGE 2: Effective date validation
                    date_result = self._check_effective_date_2020_plus(page, page_idx + 1)
                    
                    pdf_doc.close()
                    
                    if date_result.is_valid:
                        # Both stages passed
                        result = ClassificationResult(
                            is_relevant=True,
                            detected_on_page=page_idx + 1,
                            effective_date=date_result.date,
                            stage_passed="Both stages passed",
                            confidence=0.85,
                            processing_time=time.time() - start_time,
                            total_pages_checked=pages_to_check.index(page_idx) + 1
                        )
                    else:
                        # Stage 1 passed, Stage 2 failed
                        result = ClassificationResult(
                            is_relevant=False,
                            detected_on_page=page_idx + 1,
                            effective_date=date_result.date,
                            stage_failed=f"Stage 2: {date_result.reason}",
                            confidence=0.85,
                            processing_time=time.time() - start_time,
                            total_pages_checked=pages_to_check.index(page_idx) + 1
                        )
                    
                    # Cache and return result
                    self.classification_cache[cache_key] = result
                    return result
            
            # Stage 1 failed - not a healthcare contract
            pdf_doc.close()
            result = ClassificationResult(
                is_relevant=False,
                stage_failed="Stage 1: Not a healthcare contract",
                confidence=0.9,
                processing_time=time.time() - start_time,
                total_pages_checked=len(pages_to_check)
            )
            
            self.classification_cache[cache_key] = result
            return result
            
        except Exception as e:
            logger.error(f"Classification error for {doc.pdf_name}: {e}")
            return ClassificationResult(
                is_relevant=False,
                stage_failed=f"Processing error: {str(e)}",
                confidence=0.0,
                processing_time=time.time() - start_time
            )
    
    def _get_pages_to_analyze(self, total_pages: int) -> List[int]:
        """Determine which pages to analyze (first 10 + last 10)."""
        if total_pages <= 20:
            # Small document - check all pages
            return list(range(total_pages))
        else:
            # Large document - first 10 + last 10
            first_pages = list(range(self.config.first_pages_count))
            last_pages = list(range(total_pages - self.config.last_pages_count, total_pages))
            return first_pages + last_pages
    
    def _check_healthcare_contract(self, page, page_number: int) -> HealthcareContractResult:
        """Stage 1: Check if page contains healthcare contract."""
        try:
            # Convert page to image
            mat = fitz.Matrix(self.config.image_dpi / 72, self.config.image_dpi / 72)
            pix = page.get_pixmap(matrix=mat)
            image_bytes = pix.tobytes("png")
            
            # AI classification
            response = self.client.models.generate_content(
                model='gemini-1.5-flash',
                contents=[
                    types.Part.from_bytes(data=image_bytes, mime_type='image/png'),
                    self.config.healthcare_contract_prompt
                ]
            )
            
            ai_answer = response.text.strip().upper()
            
            return HealthcareContractResult(
                is_relevant='YES' in ai_answer,
                confidence=0.85 if 'YES' in ai_answer else 0.8,
                ai_response=ai_answer
            )
            
        except Exception as e:
            logger.error(f"Healthcare contract check failed on page {page_number}: {e}")
            return HealthcareContractResult(
                is_relevant=False,
                confidence=0.0,
                ai_response=f"Error: {str(e)}"
            )
    
    def _check_effective_date_2020_plus(self, page, page_number: int) -> EffectiveDateResult:
        """Stage 2: Extract effective date and validate it's 2020+."""
        try:
            # Convert page to image
            mat = fitz.Matrix(self.config.image_dpi / 72, self.config.image_dpi / 72)
            pix = page.get_pixmap(matrix=mat)
            image_bytes = pix.tobytes("png")
            
            # AI date extraction
            response = self.client.models.generate_content(
                model='gemini-1.5-flash',
                contents=[
                    types.Part.from_bytes(data=image_bytes, mime_type='image/png'),
                    self.config.effective_date_prompt
                ]
            )
            
            date_text = response.text.strip()
            
            if date_text == "NOT_FOUND":
                return EffectiveDateResult(
                    is_valid=False,
                    date=None,
                    reason="No effective date found in document",
                    raw_date_text=date_text
                )
            
            # Parse the extracted date
            parsed_date = self._parse_date_string(date_text)
            
            if parsed_date is None:
                return EffectiveDateResult(
                    is_valid=False,
                    date=None,
                    reason=f"Could not parse date: {date_text}",
                    raw_date_text=date_text
                )
            
            if parsed_date.year >= self.config.min_effective_year:
                return EffectiveDateResult(
                    is_valid=True,
                    date=parsed_date,
                    reason=f"Valid effective date: {parsed_date.year}",
                    raw_date_text=date_text
                )
            else:
                return EffectiveDateResult(
                    is_valid=False,
                    date=parsed_date,
                    reason=f"Effective date before 2020: {parsed_date.year}",
                    raw_date_text=date_text
                )
                
        except Exception as e:
            logger.error(f"Effective date check failed on page {page_number}: {e}")
            return EffectiveDateResult(
                is_valid=False,
                date=None,
                reason=f"Date extraction error: {str(e)}",
                raw_date_text=""
            )
    
    def _parse_date_string(self, date_str: str) -> Optional[datetime]:
        """Parse various date formats to datetime object."""
        try:
            # Try dateutil parser first (handles most formats)
            return parser.parse(date_str)
        except:
            try:
                # Try common manual patterns
                patterns = [
                    (r'(\d{1,2})/(\d{1,2})/(\d{4})', '%m/%d/%Y'),      # MM/DD/YYYY
                    (r'(\d{4})-(\d{1,2})-(\d{1,2})', '%Y-%m-%d'),      # YYYY-MM-DD
                    (r'(\d{1,2})-(\d{1,2})-(\d{4})', '%m-%d-%Y'),      # MM-DD-YYYY
                ]
                
                for pattern, format_str in patterns:
                    match = re.search(pattern, date_str)
                    if match:
                        return datetime.strptime(match.group(), format_str)
                
                return None
            except:
                return None

# ==========================================
# ENTERPRISE HEALTHCARE CLASSIFICATION SYSTEM
# ==========================================

class EnterpriseHealthcareClassificationSystem:
    """Main system for enterprise healthcare document classification."""
    
    def __init__(self, client, output_dir: str = "healthcare_classification_results"):
        self.client = client
        self.config = ClassificationConfig()
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Processing statistics
        self.stats = {
            'start_time': None,
            'end_time': None,
            'total_documents_found': 0,
            'total_states_processed': 0,
            'relevant_documents_found': 0,
            'classification_failures': 0,
            'total_processing_time': 0.0
        }
    
    def classify_enterprise_healthcare_documents(self, root_folder_path: str, 
                                               max_workers: int = 3) -> Dict[str, Any]:
        """Main method to discover and classify all healthcare documents."""
        
        self.stats['start_time'] = datetime.now()
        logger.info("ENTERPRISE HEALTHCARE DOCUMENT CLASSIFICATION STARTED")
        
        try:
            # PHASE 1: Document Discovery
            logger.info("PHASE 1: Discovering healthcare documents...")
            file_discovery = EnterpriseFileDiscovery(root_folder_path, self.config)
            all_documents = file_discovery.discover_all_documents()
            
            if not all_documents:
                logger.error("No documents found! Check the folder structure.")
                return self.stats
            
            self.stats['total_documents_found'] = len(all_documents)
            self.stats['total_states_processed'] = len(set(doc.state for doc in all_documents))
            
            # PHASE 2: Two-Stage AI Classification
            logger.info("PHASE 2: Two-stage healthcare document classification...")
            classifier = HealthcareDocumentClassifier(self.client, self.config)
            classification_results = classifier.classify_all_documents(
                all_documents, max_workers=max_workers
            )
            
            relevant_docs = classification_results['relevant']
            self.stats['relevant_documents_found'] = len(relevant_docs)
            self.stats['classification_failures'] = len(classification_results['failed'])
            
            # PHASE 3: Save Results
            logger.info("PHASE 3: Saving classification results...")
            
            # Save detailed classification report
            self._save_detailed_classification_report(all_documents)
            
            # Save relevant documents list
            relevant_files_list = self._save_relevant_documents_list(relevant_docs)
            
            # Generate summary statistics
            self._generate_classification_summary(all_documents, classification_results)
            
            self.stats['end_time'] = datetime.now()
            self.stats['total_processing_time'] = (
                self.stats['end_time'] - self.stats['start_time']
            ).total_seconds()
            
            logger.info("ENTERPRISE HEALTHCARE CLASSIFICATION COMPLETED")
            self._print_final_statistics()
            
            return {
                **self.stats,
                'relevant_documents_list': relevant_files_list,
                'classification_results': classification_results,
                'output_directory': str(self.output_dir)
            }
            
        except Exception as e:
            logger.error(f"Critical error in healthcare classification system: {e}")
            return {**self.stats, 'critical_error': str(e)}
    
    def _save_detailed_classification_report(self, all_documents: List[DocumentMetadata]):
        """Save comprehensive classification report as CSV."""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_path = self.output_dir / f"healthcare_classification_report_{timestamp}.csv"
        
        with open(report_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            
            # CSV Headers
            writer.writerow([
                'source_path', 'pdf_name', 'state', 'file_size_mb', 'last_modified',
                'classification_result', 'confidence_score', 'detected_on_page',
                'effective_date', 'stage_failed', 'processing_time_seconds', 'error_message'
            ])
            
            # Write data for each document
            for doc in all_documents:
                writer.writerow([
                    doc.source_path,
                    doc.pdf_name,
                    doc.state,
                    round(doc.file_size / (1024*1024), 2),  # Convert to MB
                    doc.last_modified.strftime('%Y-%m-%d %H:%M:%S'),
                    doc.classification_result,
                    doc.confidence_score,
                    doc.detected_on_page or '',
                    doc.effective_date.strftime('%Y-%m-%d') if doc.effective_date else '',
                    doc.stage_failed,
                    round(doc.processing_time, 2),
                    doc.error_message
                ])
        
        logger.info(f"Detailed classification report saved: {report_path}")
        return report_path
    
    def _save_relevant_documents_list(self, relevant_docs: List[DocumentMetadata]) -> str:
        """Save list of relevant document paths for extraction processing."""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # Save as simple text file (one path per line) - for your extraction code
        txt_path = self.output_dir / f"relevant_healthcare_documents_{timestamp}.txt"
        with open(txt_path, 'w', encoding='utf-8') as f:
            for doc in relevant_docs:
                f.write(f"{doc.source_path}\n")
        
        # Save as JSON with metadata
        json_path = self.output_dir / f"relevant_healthcare_detailed_{timestamp}.json"
        relevant_data = []
        for doc in relevant_docs:
            relevant_data.append({
                'source_path': doc.source_path,
                'pdf_name': doc.pdf_name,
                'state': doc.state,
                'effective_date': doc.effective_date.strftime('%Y-%m-%d') if doc.effective_date else None,
                'detected_on_page': doc.detected_on_page,
                'confidence_score': doc.confidence_score
            })
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(relevant_data, f, indent=2)
        
        logger.info(f"Relevant healthcare documents saved:")
        logger.info(f"  Text file (for extraction): {txt_path}")
        logger.info(f"  JSON file (with metadata): {json_path}")
        
        return str(txt_path)
    
    def _generate_classification_summary(self, all_documents: List[DocumentMetadata], 
                                       classification_results: Dict[str, List[DocumentMetadata]]):
        """Generate summary statistics by state and classification result."""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        summary_path = self.output_dir / f"healthcare_classification_summary_{timestamp}.json"
        
        # Aggregate statistics
        state_stats = {}
        year_stats = {}
        
        for doc in all_documents:
            # State-level stats
            if doc.state not in state_stats:
                state_stats[doc.state] = {
                    'total_documents': 0,
                    'relevant_documents': 0,
                    'not_healthcare': 0,
                    'old_date': 0,
                    'no_date': 0,
                    'failed': 0
                }
            
            state_stats[doc.state]['total_documents'] += 1
            state_stats[doc.state][doc.classification_result] += 1
            
            # Year-level stats for relevant documents
            if doc.effective_date and doc.classification_result == 'relevant':
                year = doc.effective_date.year
                if year not in year_stats:
                    year_stats[year] = 0
                year_stats[year] += 1
        
        summary_data = {
            'classification_timestamp': timestamp,
            'overall_stats': {
                'total_documents_processed': len(all_documents),
                'relevant_healthcare_documents': len(classification_results['relevant']),
                'not_healthcare_contracts': len(classification_results['not_healthcare']),
                'old_dates_before_2020': len(classification_results['old_date']),
                'no_effective_date_found': len(classification_results['no_date']),
                'classification_failures': len(classification_results['failed']),
                'total_states': len(state_stats),
                'success_rate_percent': round(
                    (len(all_documents) - len(classification_results['failed'])) / len(all_documents) * 100, 2
                ) if all_documents else 0
            },
            'state_breakdown': state_stats,
            'effective_date_years': dict(sorted(year_stats.items()))
        }
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, indent=2)
        
        logger.info(f"Classification summary saved: {summary_path}")
    
    def _print_final_statistics(self):
        """Print comprehensive final statistics."""
        print("ENTERPRISE HEALTHCARE DOCUMENT CLASSIFICATION - FINAL RESULTS")
        print("="*80)
        print(f"Total Documents Found: {self.stats['total_documents_found']:,}")
        print(f"States Processed: {self.stats['total_states_processed']:,}")
        print(f"Relevant Healthcare Documents (2020+): {self.stats['relevant_documents_found']:,}")
        print(f"Classification Failures: {self.stats['classification_failures']:,}")
        print(f"Total Processing Time: {self.stats['total_processing_time']:.2f} seconds")
        print(f"Success Rate: {((self.stats['total_documents_found'] - self.stats['classification_failures']) / self.stats['total_documents_found'] * 100):.1f}%" if self.stats['total_documents_found'] > 0 else "N/A")
        print(f"Output Directory: {self.output_dir}")
        print("="*80)

# ==========================================
# CLIENT SETUP & MAIN EXECUTION
# ==========================================

def create_client():
    """Create Google AI client."""
    return genai.Client(vertexai=True, project="anbc-hcb-dev", location="us-central1")

def run_enterprise_healthcare_classification(
    root_folder_path: str,
    output_dir: str = "healthcare_classification_results",
    max_workers: int = 3
) -> Dict[str, Any]:
    """
    Main function to run enterprise healthcare document classification.
    
    Args:
        root_folder_path: Path to root folder containing AGREEMENTS-NON STANDARD
        output_dir: Directory to save classification results
        max_workers: Number of parallel workers for classification
    
    Returns:
        Dictionary with classification results and statistics
    """
    
    client = create_client()
    classification_system = EnterpriseHealthcareClassificationSystem(
        client=client,
        output_dir=output_dir
    )
    
    return classification_system.classify_enterprise_healthcare_documents(
        root_folder_path=root_folder_path,
        max_workers=max_workers
    )

# ==========================================
# MAIN EXECUTION
# ==========================================

if __name__ == "__main__":
    # Configure for your enterprise environment
    ROOT_FOLDER_PATH = r"\\Network\midp-sfs-007\midamreprc"  # Your network path
    OUTPUT_DIR = "healthcare_classification_results"
    MAX_WORKERS = 3  # Adjust based on your system capacity
    
    try:
        # Run the healthcare document classification
        results = run_enterprise_healthcare_classification(
            root_folder_path=ROOT_FOLDER_PATH,
            output_dir=OUTPUT_DIR,
            max_workers=MAX_WORKERS
        )
        
        print("\nHEALTHCARE CLASSIFICATION COMPLETED SUCCESSFULLY!")
        print(f"Check the output directory: {results.get('output_directory')}")
        print(f"Use 'relevant_healthcare_documents_*.txt' with your extraction code")
        print(f"Found {results.get('relevant_documents_found', 0)} relevant healthcare documents with 2020+ effective dates")
        
    except Exception as e:
        logger.error(f"Critical error in main execution: {e}")
        print(f"Error: {e}")
