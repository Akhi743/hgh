import fitz
import csv
import json
import logging
import os
import time
from pathlib import Path
from typing import List, Optional, Dict, Any
from google.genai import types
import google.genai as genai
from datetime import datetime, date
from dataclasses import dataclass
from dateutil import parser

# Enhanced logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('healthcare_classification.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class DocumentMetadata:
    """Represents a single PDF, holding all its metadata and final classification details."""
    source_path: str
    pdf_name: str
    state: str
    file_size: int
    last_modified: datetime
    classification_status: str = "pending"
    classification_result: str = "unknown"
    detected_on_page: Optional[int] = None
    effective_date: Optional[datetime] = None
    stage_failed: str = ""
    confidence_score: float = 0.0
    processing_time: float = 0.0
    error_message: str = ""
    pdf_complexity_level: str = ""
    pdf_issues: List[str] = None
    
    # NEW: Enhanced file date tracking
    file_modification_date: date = None
    file_date_meets_criteria: bool = False
    is_newly_modified: bool = False  # For incremental processing

    def __post_init__(self):
        if self.pdf_issues is None:
            self.pdf_issues = []
        # Set file modification date for easy access
        self.file_modification_date = self.last_modified.date()

@dataclass
class ClassificationResult:
    """A simple structure to pass results between classification methods."""
    is_relevant: bool
    detected_on_page: Optional[int] = None
    effective_date: Optional[datetime] = None
    stage_failed: str = ""
    confidence: float = 0.0
    processing_time: float = 0.0

@dataclass
class PDFComplexityInfo:
    """Structure to hold PDF complexity analysis results."""
    has_form_fields: bool = False
    has_digital_signatures: bool = False
    has_encryption: bool = False
    has_embedded_files: bool = False
    page_count: int = 0
    complexity_level: str = 'simple'
    potential_issues: List[str] = None
    analysis_successful: bool = True
    
    def __post_init__(self):
        if self.potential_issues is None:
            self.potential_issues = []

class ClassificationConfig:
    """Enhanced configuration class with incremental processing capabilities."""
    def __init__(self):
        # Folder names the script will look for.
        self.agreements_folder = "AGREEMENTS-NON STANDARD"
        self.hospital_subfolder = "HOSPITAL"
        
        # Key business logic settings
        self.min_effective_year = 2020  # Minimum year for effective date to be considered relevant
        
        # NEW: Enhanced file modification date filtering with incremental processing
        self.min_file_modification_date = date(2019, 1, 1)  # Only process files modified from 01/01/2019
        self.enable_file_date_prefilter = True  # Can be disabled for testing/debugging
        self.enable_incremental_processing = True  # Only process files modified after last run
        
        # NEW: State persistence for incremental processing
        self.state_file_path = "classification_state.json"
        self.last_run_date = None  # Will be loaded from state file
        
        # Settings for AI processing.
        self.image_dpi = 150
        self.first_pages_count = 10
        self.last_pages_count = 10
        
        # PDF error handling settings
        self.max_repair_attempts = 3
        self.skip_corrupted_pages = True
        self.enable_complexity_analysis = True  # Can be disabled for performance
        
        # Prompts are carefully worded to guide the AI's response for each task.
        self.healthcare_contract_prompt = "Analyze the image. Is this a formal letter from Aetna about a 'charge master increase' where they state 'we have received' a notification? Answer only YES or NO."
        self.effective_date_prompt = 'This is an Aetna charge master increase letter. Find the EFFECTIVE DATE mentioned in the letter (phrases like "effective January 1, 2024" or "to become effective 1/1/2025"). Respond with just the date found, or "NOT_FOUND" if no effective date exists.'
        
        # A static set of US states used to identify state-level folders.
        self.us_states = {
            'ALABAMA', 'ALASKA', 'ARIZONA', 'ARKANSAS', 'CALIFORNIA', 'COLORADO', 
            'CONNECTICUT', 'DELAWARE', 'FLORIDA', 'GEORGIA', 'HAWAII', 'IDAHO', 
            'ILLINOIS', 'INDIANA', 'IOWA', 'KANSAS', 'KENTUCKY', 'LOUISIANA', 
            'MAINE', 'MARYLAND', 'MASSACHUSETTS', 'MICHIGAN', 'MINNESOTA', 
            'MISSISSIPPI', 'MISSOURI', 'MONTANA', 'NEBRASKA', 'NEVADA', 
            'NEW HAMPSHIRE', 'NEW JERSEY', 'NEW MEXICO', 'NEW YORK', 
            'NORTH CAROLINA', 'NORTH DAKOTA', 'OHIO', 'OKLAHOMA', 'OREGON', 
            'PENNSYLVANIA', 'RHODE ISLAND', 'SOUTH CAROLINA', 'SOUTH DAKOTA', 
            'TENNESSEE', 'TEXAS', 'UTAH', 'VERMONT', 'VIRGINIA', 'WASHINGTON', 
            'WEST VIRGINIA', 'WISCONSIN', 'WYOMING', 'PUERTO RICO', 'DISTRICT OF COLUMBIA'
        }

class IncrementalStateManager:
    """Manages state for incremental processing to avoid reprocessing unchanged files."""
    
    def __init__(self, state_file_path: str):
        self.state_file_path = Path(state_file_path)
        self.processed_files = {}  # file_path -> {last_modified, classification_result}
        self.last_run_date = None
        self.load_state()
    
    def load_state(self):
        """Load the previous run state from disk."""
        try:
            if self.state_file_path.exists():
                with open(self.state_file_path, 'r', encoding='utf-8') as f:
                    state_data = json.load(f)
                    
                self.last_run_date = parser.parse(state_data.get('last_run_date')).date() if state_data.get('last_run_date') else None
                self.processed_files = state_data.get('processed_files', {})
                
                logger.info(f"Loaded state: Last run = {self.last_run_date}, {len(self.processed_files)} previously processed files")
            else:
                logger.info("No previous state found. Will process all files.")
        except Exception as e:
            logger.warning(f"Could not load previous state: {e}. Starting fresh.")
            self.processed_files = {}
            self.last_run_date = None
    
    def save_state(self, current_run_date: date):
        """Save the current run state to disk."""
        try:
            state_data = {
                'last_run_date': current_run_date.isoformat(),
                'processed_files': self.processed_files,
                'total_files_processed': len(self.processed_files)
            }
            
            with open(self.state_file_path, 'w', encoding='utf-8') as f:
                json.dump(state_data, f, indent=2, default=str)
                
            logger.info(f"Saved state: {len(self.processed_files)} processed files as of {current_run_date}")
        except Exception as e:
            logger.error(f"Could not save state: {e}")
    
    def should_process_file(self, file_path: str, file_modified_date: date) -> bool:
        """Determine if a file needs processing based on modification date."""
        file_info = self.processed_files.get(file_path)
        
        if not file_info:
            # File never processed before
            return True
        
        # Check if file was modified since last processing
        last_processed_date = parser.parse(file_info['last_modified']).date()
        return file_modified_date > last_processed_date
    
    def update_file_status(self, file_path: str, file_modified_date: date, classification_result: str):
        """Update the processing status of a file."""
        self.processed_files[file_path] = {
            'last_modified': file_modified_date.isoformat(),
            'classification_result': classification_result,
            'processed_on': datetime.now().isoformat()
        }
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """Get statistics about incremental processing."""
        if not self.processed_files:
            return {'total_files': 0, 'classification_breakdown': {}}
        
        classification_counts = {}
        for file_info in self.processed_files.values():
            result = file_info.get('classification_result', 'unknown')
            classification_counts[result] = classification_counts.get(result, 0) + 1
        
        return {
            'total_files': len(self.processed_files),
            'last_run_date': self.last_run_date.isoformat() if self.last_run_date else None,
            'classification_breakdown': classification_counts
        }

class EnhancedPDFHandler:
    """
    Enhanced PDF handler that deals with form fields, digital signatures, 
    and complex PDF structures that cause xref errors.
    """
    
    @staticmethod
    def quick_complexity_check(pdf_doc: fitz.Document) -> PDFComplexityInfo:
        """
        Performs complexity analysis on an already-opened document.
        This avoids redundant file operations.
        """
        complexity_info = PDFComplexityInfo()
        
        try:
            complexity_info.page_count = len(pdf_doc)
            
            # Check for form fields (sample first few pages for performance)
            pages_to_check = min(5, len(pdf_doc))
            for page_num in range(pages_to_check):
                try:
                    page = pdf_doc.load_page(page_num)
                    if page.get_widgets():  # Form fields/widgets detected
                        complexity_info.has_form_fields = True
                        complexity_info.potential_issues.append('form_fields')
                        break
                except Exception:
                    continue
            
            # Check for encryption
            if pdf_doc.needs_pass:
                complexity_info.has_encryption = True
                complexity_info.potential_issues.append('password_protected')
            
            # Check for embedded files
            if pdf_doc.embfile_count() > 0:
                complexity_info.has_embedded_files = True
                complexity_info.potential_issues.append('embedded_files')
            
            # Check metadata for signatures
            try:
                metadata = pdf_doc.metadata
                if metadata and any(sig_indicator in str(metadata).lower() 
                                  for sig_indicator in ['signature', 'signed', 'certif']):
                    complexity_info.has_digital_signatures = True
                    complexity_info.potential_issues.append('digital_signatures')
            except Exception:
                pass
            
            # Determine complexity level
            issue_count = len(complexity_info.potential_issues)
            if issue_count == 0:
                complexity_info.complexity_level = 'simple'
            elif issue_count <= 2:
                complexity_info.complexity_level = 'moderate'
            else:
                complexity_info.complexity_level = 'complex'
                
        except Exception as e:
            complexity_info.potential_issues.append(f'analysis_error: {str(e)}')
            complexity_info.complexity_level = 'problematic'
            complexity_info.analysis_successful = False
        
        return complexity_info

    @staticmethod
    def attempt_pdf_open_with_strategies(pdf_path: str) -> Optional[fitz.Document]:
        """
        Multiple strategies specifically designed for problematic enterprise PDFs.
        """
        logger.debug(f"Attempting to open PDF: {os.path.basename(pdf_path)}")
        
        # Strategy 1: Standard open (fastest)
        try:
            doc = fitz.open(pdf_path)
            logger.debug("Standard open successful")
            return doc
        except Exception as e:
            logger.debug(f"Standard open failed: {e}")
        
        # Strategy 2: Open with repair flag
        try:
            doc = fitz.open(pdf_path, filetype="pdf")
            logger.debug("Repair mode open successful")
            return doc
        except Exception as e:
            logger.debug(f"Repair mode open failed: {e}")
        
        # Strategy 3: Memory-based opening (handles file locking issues)
        try:
            with open(pdf_path, 'rb') as f:
                pdf_data = f.read()
            doc = fitz.open(stream=pdf_data, filetype="pdf")
            logger.debug("Memory stream open successful")
            return doc
        except Exception as e:
            logger.debug(f"Memory stream open failed: {e}")
        
        # Strategy 4: Proper visual content preservation
        try:
            source_doc = fitz.open(pdf_path)
            clean_doc = fitz.open()
            
            successful_pages = 0
            for page_num in range(min(len(source_doc), 20)):  # Limit for safety
                try:
                    # Properly copy the entire page including visual content
                    clean_doc.insert_pdf(source_doc, from_page=page_num, to_page=page_num)
                    successful_pages += 1
                except Exception as page_error:
                    logger.debug(f"Skipped page {page_num + 1}: {page_error}")
                    continue
            
            source_doc.close()
            
            if successful_pages > 0:
                logger.debug(f"Page reconstruction successful ({successful_pages} pages)")
                return clean_doc
            else:
                clean_doc.close()
                
        except Exception as e:
            logger.debug(f"Page reconstruction failed: {e}")
        
        # Strategy 5: Form field flattening (convert to images)
        try:
            source_doc = fitz.open(pdf_path)
            temp_doc = fitz.open()
            
            successful_pages = 0
            for page_num in range(min(len(source_doc), 10)):  # Limit for performance
                try:
                    page = source_doc.load_page(page_num)
                    # Create high-res image to preserve visual fidelity
                    mat = fitz.Matrix(2.0, 2.0)  # 2x scale for AI readability
                    pix = page.get_pixmap(matrix=mat)
                    
                    # Create new page from image
                    img_pdf = fitz.open("pdf", pix.tobytes("pdf"))
                    temp_doc.insert_pdf(img_pdf, from_page=0, to_page=0)
                    img_pdf.close()
                    successful_pages += 1
                    
                except Exception as page_error:
                    logger.debug(f"Flattening failed for page {page_num + 1}: {page_error}")
                    continue
            
            source_doc.close()
            
            if successful_pages > 0:
                logger.debug(f"Form field flattening successful ({successful_pages} pages)")
                return temp_doc
            else:
                temp_doc.close()
                
        except Exception as e:
            logger.debug(f"Form field flattening failed: {e}")
        
        logger.warning(f"All repair strategies failed for: {os.path.basename(pdf_path)}")
        return None

    @staticmethod
    def safe_page_access(doc: fitz.Document, page_idx: int) -> Optional[fitz.Page]:
        """
        Safely access a page, handling corruption at the page level.
        """
        try:
            page = doc.load_page(page_idx)
            # Test if page can be rendered by trying to get its rect
            _ = page.rect
            return page
        except Exception as e:
            logger.debug(f"Page {page_idx + 1} corrupted or inaccessible: {e}")
            return None

class EnterpriseFileDiscovery:
    """Handles the logic of finding all target PDF documents within the specified folder structure."""
    def __init__(self, root_path: str, config: ClassificationConfig):
        self.root_path = Path(root_path)
        self.config = config

    def discover_all_documents(self) -> List[DocumentMetadata]:
        """Runs the discovery process by scanning through each state folder."""
        logger.info(f"Starting document discovery from: {self.root_path}")
        agreements_path = self.root_path / self.config.agreements_folder
        if not agreements_path.exists():
            logger.error(f"'{self.config.agreements_folder}' folder not found at: {agreements_path}")
            return []

        all_documents = []
        state_folders = [f for f in agreements_path.iterdir() if f.is_dir() and self._is_state_folder(f)]
        logger.info(f"Found {len(state_folders)} state folders to process.")

        for state_folder in state_folders:
            all_documents.extend(self._process_single_state(state_folder))

        logger.info(f"Discovery complete. Total documents found: {len(all_documents)}")
        return all_documents

    def _process_single_state(self, state_folder: Path) -> List[DocumentMetadata]:
        """Processes a single state folder to find its hospital subfolders and their PDFs."""
        logger.info(f"Processing State: {state_folder.name}")
        hospital_base_path = state_folder / self.config.hospital_subfolder
        if not hospital_base_path.exists():
            logger.warning(f"No '{self.config.hospital_subfolder}' folder found in {state_folder.name}.")
            return []

        state_documents = []
        hospital_subfolders = [d for d in hospital_base_path.iterdir() if d.is_dir()]
        logger.info(f"Found {len(hospital_subfolders)} hospital subfolders in {state_folder.name}.")

        for hospital_folder in hospital_subfolders:
            try:
                logger.info(f"  Scanning hospital: {hospital_folder.name}")
                pdf_paths = hospital_folder.rglob("*.pdf")
                
                for pdf_path in pdf_paths:
                    path_str = str(pdf_path)
                    if path_str.startswith('\\\\') and not path_str.startswith('\\\\?\\'):
                        path_str = '\\\\?\\UNC\\' + path_str[2:]
                    
                    if os.path.exists(path_str):
                        stats = os.stat(path_str)
                        doc = DocumentMetadata(
                            source_path=path_str,
                            pdf_name=pdf_path.name,
                            state=state_folder.name,
                            file_size=stats.st_size,
                            last_modified=datetime.fromtimestamp(stats.st_mtime)
                        )
                        state_documents.append(doc)
                    else:
                        logger.warning(f"    Path discovered but not found: {pdf_path.name}")

            except Exception as e:
                logger.error(f"  Could not process hospital folder {hospital_folder.name}: {e}")
        
        logger.info(f"  Found {len(state_documents)} documents in {state_folder.name}.")
        return state_documents

    def _is_state_folder(self, folder_path: Path) -> bool:
        """A helper to identify state folders based on the configured list of names."""
        return any(state in folder_path.name.upper() for state in self.config.us_states)

class HealthcareDocumentClassifier:
    """Enhanced classifier with incremental processing and file date pre-filtering."""
    def __init__(self, client, config: ClassificationConfig):
        self.client = client
        self.config = config
        self.pdf_handler = EnhancedPDFHandler()
        
        # NEW: Initialize incremental state manager
        self.state_manager = IncrementalStateManager(config.state_file_path)
        if self.config.enable_incremental_processing:
            self.config.last_run_date = self.state_manager.last_run_date
        
        # Statistics tracking
        self.complexity_stats = {
            'simple': 0,
            'moderate': 0, 
            'complex': 0,
            'problematic': 0
        }
        self.issue_frequency = {}
        
        # NEW: Incremental processing stats
        self.incremental_stats = {
            'total_discovered': 0,
            'skipped_old_files': 0,
            'skipped_unchanged': 0,
            'newly_processed': 0
        }

    def classify_all_documents(self, documents: List[DocumentMetadata]) -> Dict[str, List[DocumentMetadata]]:
        """
        Enhanced classification with incremental processing and file date pre-filtering.
        """
        logger.info(f"Starting enhanced classification of {len(documents)} documents...")
        self.incremental_stats['total_discovered'] = len(documents)
        
        # NEW: Apply file date and incremental filtering
        documents_to_process = self._apply_pre_filtering(documents)
        
        results = {
            'relevant': [], 
            'not_healthcare': [], 
            'old_date': [], 
            'no_date': [], 
            'failed': [],
            'pdf_corrupted': [],
            'form_field_issues': [],
            'old_file_date': [],        # NEW: Files modified before min_file_modification_date
            'skipped_unchanged': []      # NEW: Files skipped due to incremental processing
        }
        
        logger.info(f"After pre-filtering: {len(documents_to_process)} documents to process")
        
        for i, doc in enumerate(documents_to_process, 1):
            logger.info(f"--> Processing {i}/{len(documents_to_process)}: {doc.pdf_name} (State: {doc.state})")
            
            try:
                # Enhanced classification with pre-filtering
                classification_result = self._classify_single_document_enhanced(doc)
                
                # Update the original document object with the classification outcome
                doc.processing_time = classification_result.processing_time
                doc.confidence_score = classification_result.confidence
                doc.detected_on_page = classification_result.detected_on_page
                doc.effective_date = classification_result.effective_date
                doc.stage_failed = classification_result.stage_failed
                doc.classification_status = "completed"
                
                # Update complexity statistics
                if doc.pdf_complexity_level:
                    self.complexity_stats[doc.pdf_complexity_level] += 1
                for issue in doc.pdf_issues:
                    self.issue_frequency[issue] = self.issue_frequency.get(issue, 0) + 1

                if classification_result.is_relevant:
                    doc.classification_result = "relevant"
                    results['relevant'].append(doc)
                    logger.info(f"    └── SUCCESS: Document is relevant. Effective Date: {doc.effective_date.strftime('%Y-%m-%d') if doc.effective_date else 'N/A'}")
                else:
                    # Enhanced categorization
                    if "old_file_date" in doc.stage_failed.lower():
                        reason_key = 'old_file_date'
                    elif "form field" in doc.stage_failed.lower():
                        reason_key = 'form_field_issues'
                    elif "Stage 2: No effective date" in doc.stage_failed:
                        reason_key = 'no_date'
                    elif "Stage 2: Effective date before" in doc.stage_failed:
                        reason_key = 'old_date'
                    elif "PDF corruption" in doc.stage_failed or "Unable to open" in doc.stage_failed:
                        reason_key = 'pdf_corrupted'
                    else:
                        reason_key = 'not_healthcare'
                    
                    doc.classification_result = reason_key
                    results[reason_key].append(doc)
                    logger.info(f"    └── REJECTED: {doc.stage_failed}")

                # NEW: Update incremental state
                if self.config.enable_incremental_processing:
                    self.state_manager.update_file_status(
                        doc.source_path, 
                        doc.file_modification_date, 
                        doc.classification_result
                    )

            except Exception as e:
                logger.error(f"    └── CLASSIFICATION FAILED for {doc.pdf_name}: {e}")
                doc.classification_result = "failed"
                doc.error_message = str(e)
                doc.pdf_complexity_level = "problematic"
                self.complexity_stats["problematic"] += 1
                results['failed'].append(doc)
        
        # NEW: Add skipped documents to results for reporting
        skipped_docs = [doc for doc in documents if not doc.is_newly_modified and doc not in documents_to_process]
        results['skipped_unchanged'].extend(skipped_docs)
        
        # Add comprehensive analysis summary to results
        total_docs = len(documents)
        results['complexity_analysis'] = {
            'total_analyzed': total_docs,
            'incremental_stats': self.incremental_stats,
            'complexity_distribution': {
                level: {'count': count, 'percentage': (count/total_docs)*100 if total_docs > 0 else 0}
                for level, count in self.complexity_stats.items()
            },
            'common_issues': dict(sorted(self.issue_frequency.items(), 
                                       key=lambda x: x[1], reverse=True)),
            'recommendations': self._generate_recommendations()
        }
        
        # NEW: Save state after processing
        if self.config.enable_incremental_processing:
            self.state_manager.save_state(date.today())
        
        return results

    def _apply_pre_filtering(self, documents: List[DocumentMetadata]) -> List[DocumentMetadata]:
        """
        Apply file date pre-filtering and incremental processing logic.
        """
        documents_to_process = []
        
        for doc in documents:
            # Check file modification date criteria
            doc.file_date_meets_criteria = doc.file_modification_date >= self.config.min_file_modification_date
            
            # Check if file needs processing (incremental logic)
            if self.config.enable_incremental_processing:
                doc.is_newly_modified = self.state_manager.should_process_file(
                    doc.source_path, 
                    doc.file_modification_date
                )
            else:
                doc.is_newly_modified = True  # Process all if incremental disabled
            
            # Apply filtering logic
            if not doc.file_date_meets_criteria:
                self.incremental_stats['skipped_old_files'] += 1
                doc.classification_result = "old_file_date"
                doc.stage_failed = f"File modified before {self.config.min_file_modification_date} ({doc.file_modification_date})"
                continue
            
            if not doc.is_newly_modified:
                self.incremental_stats['skipped_unchanged'] += 1
                doc.classification_result = "skipped_unchanged"
                doc.stage_failed = f"File unchanged since last run ({self.config.last_run_date})"
                continue
            
            documents_to_process.append(doc)
            self.incremental_stats['newly_processed'] += 1
        
        logger.info(f"Pre-filtering results:")
        logger.info(f"  Total discovered: {self.incremental_stats['total_discovered']}")
        logger.info(f"  Skipped (old file date): {self.incremental_stats['skipped_old_files']}")
        logger.info(f"  Skipped (unchanged): {self.incremental_stats['skipped_unchanged']}")
        logger.info(f"  To process: {self.incremental_stats['newly_processed']}")
        
        return documents_to_process

    def _classify_single_document_enhanced(self, doc: DocumentMetadata) -> ClassificationResult:
        """
        Enhanced single document classification with all pre-filtering already applied.
        """
        start_time = time.time()
        pdf_doc = None
        
        try:
            # Step 1: Attempt to open PDF with repair strategies
            pdf_doc = self.pdf_handler.attempt_pdf_open_with_strategies(doc.source_path)
            
            if pdf_doc is None:
                doc.pdf_complexity_level = "problematic"
                doc.pdf_issues = ["unable_to_open"]
                return ClassificationResult(
                    False, 
                    stage_failed="PDF corruption: Unable to open PDF after all repair attempts",
                    processing_time=time.time() - start_time
                )
            
            # Step 2: Analyze complexity on the already-opened document
            if self.config.enable_complexity_analysis:
                complexity_info = self.pdf_handler.quick_complexity_check(pdf_doc)
                doc.pdf_complexity_level = complexity_info.complexity_level
                doc.pdf_issues = complexity_info.potential_issues.copy()
            else:
                doc.pdf_complexity_level = "simple"
                doc.pdf_issues = []
            
            # Check if PDF has any accessible pages
            if len(pdf_doc) == 0:
                doc.pdf_complexity_level = "problematic"
                doc.pdf_issues.append("no_pages")
                return ClassificationResult(
                    False, 
                    stage_failed="PDF corruption: No accessible pages found",
                    processing_time=time.time() - start_time
                )
            
            # Step 3: Process based on complexity level
            pages_to_check = self._get_pages_to_analyze_safely(pdf_doc)
            
            if not pages_to_check:
                doc.pdf_issues.append("no_accessible_pages")
                return ClassificationResult(
                    False, 
                    stage_failed="PDF corruption: No accessible pages for analysis",
                    processing_time=time.time() - start_time
                )

            accessible_pages_count = 0
            for page_idx in pages_to_check:
                page = self.pdf_handler.safe_page_access(pdf_doc, page_idx)
                if page is None:
                    logger.debug(f"Skipping corrupted page {page_idx + 1} in {doc.pdf_name}")
                    continue
                
                accessible_pages_count += 1
                
                # Stage 1: Check if the page contains a relevant healthcare contract
                if not self._is_healthcare_contract_safe(page):
                    continue

                # Stage 2: If it is a contract, check for a valid effective date
                date_result = self._get_effective_date_safe(page)
                processing_time = time.time() - start_time
                
                if date_result and date_result.year >= self.config.min_effective_year:
                    confidence = 0.9 if doc.pdf_complexity_level == 'simple' else 0.8
                    return ClassificationResult(True, page_idx + 1, date_result, confidence=confidence, processing_time=processing_time)
                elif date_result:
                    confidence = 0.9 if doc.pdf_complexity_level == 'simple' else 0.8
                    stage_failed_msg = f"Stage 2: Effective date before {self.config.min_effective_year} ({date_result.year})"
                    if doc.pdf_complexity_level in ['complex', 'moderate']:
                        stage_failed_msg += f" (complex PDF: {', '.join(doc.pdf_issues)})"
                    return ClassificationResult(False, page_idx + 1, date_result, stage_failed=stage_failed_msg, confidence=confidence, processing_time=processing_time)
                else: 
                    confidence = 0.8 if doc.pdf_complexity_level == 'simple' else 0.7
                    stage_failed_msg = "Stage 2: No effective date found on relevant page"
                    if doc.pdf_complexity_level in ['complex', 'moderate']:
                        stage_failed_msg += f" (complex PDF: {', '.join(doc.pdf_issues)})"
                    return ClassificationResult(False, page_idx + 1, None, stage_failed=stage_failed_msg, confidence=confidence, processing_time=processing_time)

            # If we processed some pages but found no contracts
            if accessible_pages_count > 0:
                confidence = 0.9 if doc.pdf_complexity_level == 'simple' else 0.8
                stage_failed_msg = "Stage 1: Not a healthcare contract"
                if doc.pdf_complexity_level in ['complex', 'moderate']:
                    stage_failed_msg += f" (complex PDF: {', '.join(doc.pdf_issues)})"
                return ClassificationResult(False, stage_failed=stage_failed_msg, confidence=confidence, processing_time=time.time() - start_time)
            else:
                doc.pdf_issues.append("no_processable_pages")
                return ClassificationResult(False, stage_failed="PDF corruption: No pages could be processed", processing_time=time.time() - start_time)
                
        except Exception as e:
            raise RuntimeError(f"Failed to process PDF {doc.pdf_name}") from e
        finally:
            if pdf_doc:
                try:
                    pdf_doc.close()
                except:
                    pass

    def _get_pages_to_analyze_safely(self, pdf_doc: fitz.Document) -> List[int]:
        """
        Enhanced page selection that prioritizes accessible pages.
        """
        total_pages = len(pdf_doc)
        
        if total_pages <= (self.config.first_pages_count + self.config.last_pages_count):
            candidate_pages = list(range(total_pages))
        else:
            first = list(range(self.config.first_pages_count))
            last = list(range(total_pages - self.config.last_pages_count, total_pages))
            candidate_pages = sorted(list(set(first + last)))
        
        # Filter out pages that can't be accessed
        accessible_pages = []
        for page_idx in candidate_pages:
            if self.pdf_handler.safe_page_access(pdf_doc, page_idx) is not None:
                accessible_pages.append(page_idx)
        
        logger.debug(f"Selected {len(accessible_pages)} accessible pages out of {len(candidate_pages)} candidates")
        return accessible_pages

    def _call_gemini_safe(self, page: fitz.Page, prompt: str) -> Optional[str]:
        """Enhanced Gemini API call with better error handling."""
        try:
            mat = fitz.Matrix(self.config.image_dpi / 72, self.config.image_dpi / 72)
            pix = page.get_pixmap(matrix=mat)
            
            # Verify the pixmap is valid
            if pix.width == 0 or pix.height == 0:
                logger.debug("Generated pixmap has zero dimensions")
                return None
                
            image_bytes = pix.tobytes("png")
            
            if len(image_bytes) == 0:
                logger.debug("Generated image bytes are empty")
                return None
            
            response = self.client.models.generate_content(
                model='gemini-1.5-flash',
                contents=[types.Part.from_bytes(data=image_bytes, mime_type='image/png'), prompt]
            )
            return response.text.strip()
            
        except Exception as e:
            logger.debug(f"Gemini API call failed: {e}")
            return None

    def _is_healthcare_contract_safe(self, page: fitz.Page) -> bool:
        """Safe healthcare contract detection with error handling."""
        try:
            answer = self._call_gemini_safe(page, self.config.healthcare_contract_prompt)
            return answer and 'YES' in answer.upper()
        except Exception as e:
            logger.debug(f"Healthcare contract check failed: {e}")
            return False

    def _get_effective_date_safe(self, page: fitz.Page) -> Optional[datetime]:
        """Safe effective date extraction with error handling."""
        try:
            date_text = self._call_gemini_safe(page, self.config.effective_date_prompt)
            if not date_text or "NOT_FOUND" in date_text:
                return None
            return parser.parse(date_text)
        except (parser.ParserError, TypeError, Exception) as e:
            logger.debug(f"Could not parse date from AI response '{date_text}': {e}")
            return None

    def _generate_recommendations(self) -> List[str]:
        """Generate processing recommendations based on complexity analysis."""
        recommendations = []
        
        if self.issue_frequency.get('form_fields', 0) > 0:
            recommendations.append(
                f"Form fields detected in {self.issue_frequency['form_fields']} documents. "
                "Consider implementing form field flattening for these documents."
            )
        
        if self.issue_frequency.get('digital_signatures', 0) > 0:
            recommendations.append(
                f"Digital signatures detected in {self.issue_frequency['digital_signatures']} documents. "
                "These may cause xref errors during processing."
            )
        
        if self.complexity_stats['problematic'] > self.complexity_stats['simple']:
            recommendations.append(
                "High proportion of problematic PDFs detected. "
                "Consider implementing additional PDF repair strategies."
            )
        
        total_complex = self.complexity_stats['complex'] + self.complexity_stats['moderate']
        total_processed = sum(self.complexity_stats.values())
        if total_processed > 0 and (total_complex / total_processed) > 0.3:
            recommendations.append(
                f"30%+ of documents are moderately to highly complex. "
                f"Consider enabling specialized processing pipelines."
            )
        
        # NEW: Incremental processing recommendations
        if self.incremental_stats['skipped_unchanged'] > 0:
            recommendations.append(
                f"Incremental processing skipped {self.incremental_stats['skipped_unchanged']} unchanged files, "
                f"saving significant processing time and API costs."
            )
        
        if self.incremental_stats['skipped_old_files'] > 0:
            recommendations.append(
                f"Pre-filtering excluded {self.incremental_stats['skipped_old_files']} files "
                f"modified before {self.config.min_file_modification_date}, optimizing processing efficiency."
            )
        
        return recommendations

class EnterpriseHealthcareClassificationSystem:
    """Enhanced main class with incremental processing and optimized workflows."""
    def __init__(self, client, output_dir: str = "classification_results"):
        self.client = client
        self.config = ClassificationConfig()
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def run_classification(self, root_folder_path: str) -> Dict[str, Any]:
        """
        Executes the enhanced end-to-end process with incremental processing.
        """
        start_time = time.time()
        logger.info("STARTING ENHANCED ENTERPRISE CLASSIFICATION WITH INCREMENTAL PROCESSING")
        logger.info(f"Configuration:")
        logger.info(f"  Min file modification date: {self.config.min_file_modification_date}")
        logger.info(f"  Min effective year: {self.config.min_effective_year}")
        logger.info(f"  Incremental processing: {'ENABLED' if self.config.enable_incremental_processing else 'DISABLED'}")
        logger.info(f"  File date pre-filtering: {'ENABLED' if self.config.enable_file_date_prefilter else 'DISABLED'}")

        # Step 1: Discover all potential documents
        discovery = EnterpriseFileDiscovery(root_folder_path, self.config)
        all_documents = discovery.discover_all_documents()
        if not all_documents:
            logger.error("No documents were found. Exiting.")
            return {
                'success': False,
                'error': 'No documents found',
                'output_directory': str(self.output_dir)
            }

        # Step 2: Enhanced classification with incremental processing
        classifier = HealthcareDocumentClassifier(self.client, self.config)
        results = classifier.classify_all_documents(all_documents)

        # Step 3: Save the enhanced results to output files
        output_files = self._save_enhanced_reports(all_documents, results)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        logger.info("ENHANCED CLASSIFICATION COMPLETE")
        logger.info(f"Total time: {total_time:.2f} seconds")
        self._print_enhanced_summary(results)
        
        # Return comprehensive results for the caller
        incremental_stats = results.get('complexity_analysis', {}).get('incremental_stats', {})
        return {
            'success': True,
            'output_directory': str(self.output_dir),
            'total_documents_discovered': len(all_documents),
            'total_documents_processed': incremental_stats.get('newly_processed', 0),
            'relevant_documents_found': len(results['relevant']),
            'processing_time_seconds': total_time,
            'output_files': output_files,
            'classification_breakdown': {
                'relevant': len(results['relevant']),
                'not_healthcare': len(results['not_healthcare']),
                'old_date': len(results['old_date']),
                'no_date': len(results['no_date']),
                'form_field_issues': len(results['form_field_issues']),
                'pdf_corrupted': len(results['pdf_corrupted']),
                'failed': len(results['failed']),
                'old_file_date': len(results['old_file_date']),  # NEW
                'skipped_unchanged': len(results['skipped_unchanged'])  # NEW
            },
            'incremental_processing_stats': incremental_stats,
            'complexity_analysis': results.get('complexity_analysis', {}),
            'success_rate_percentage': (len(results['relevant']) / len(all_documents)) * 100 if all_documents else 0,
            'processing_efficiency': {
                'documents_skipped': incremental_stats.get('skipped_old_files', 0) + incremental_stats.get('skipped_unchanged', 0),
                'api_calls_saved': incremental_stats.get('skipped_old_files', 0) + incremental_stats.get('skipped_unchanged', 0),
                'time_saved_percentage': ((incremental_stats.get('skipped_old_files', 0) + incremental_stats.get('skipped_unchanged', 0)) / len(all_documents)) * 100 if all_documents else 0
            }
        }

    def run(self, root_folder_path: str):
        """Legacy method for backwards compatibility."""
        return self.run_classification(root_folder_path)

    def _save_enhanced_reports(self, all_docs: List[DocumentMetadata], results: Dict[str, List[DocumentMetadata]]) -> Dict[str, str]:
        """Enhanced reporting with incremental processing tracking."""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_files = {}
        
        # Enhanced detailed report with incremental processing information
        report_path = self.output_dir / f"full_classification_report_{timestamp}.csv"
        with open(report_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                'source_path', 'pdf_name', 'state', 'classification_result', 
                'effective_date', 'detected_on_page', 'stage_failed', 'error_message', 
                'complexity_level', 'pdf_issues', 'processing_time', 'confidence_score',
                'file_modification_date', 'file_date_meets_criteria', 'is_newly_modified'  # NEW columns
            ])
            for doc in all_docs:
                writer.writerow([
                    doc.source_path, 
                    doc.pdf_name, 
                    doc.state, 
                    doc.classification_result, 
                    doc.effective_date.strftime('%Y-%m-%d') if doc.effective_date else '', 
                    doc.detected_on_page or '', 
                    doc.stage_failed, 
                    doc.error_message,
                    doc.pdf_complexity_level,
                    '; '.join(doc.pdf_issues) if doc.pdf_issues else '',
                    f"{doc.processing_time:.2f}s",
                    f"{doc.confidence_score:.2f}",
                    doc.file_modification_date.strftime('%Y-%m-%d') if doc.file_modification_date else '',  # NEW
                    doc.file_date_meets_criteria,  # NEW
                    doc.is_newly_modified  # NEW
                ])
        output_files['detailed_report'] = str(report_path)
        logger.info(f"Enhanced CSV report saved to: {report_path}")

        # Enhanced complexity analysis report with incremental stats
        complexity_analysis = results.get('complexity_analysis', {})
        complexity_report_path = self.output_dir / f"processing_analysis_{timestamp}.json"
        with open(complexity_report_path, 'w', encoding='utf-8') as f:
            json.dump(complexity_analysis, f, indent=2, default=str)
        output_files['processing_analysis'] = str(complexity_report_path)
        logger.info(f"Processing analysis saved to: {complexity_report_path}")

        # Incremental processing summary
        incremental_summary_path = self.output_dir / f"incremental_summary_{timestamp}.json"
        incremental_data = {
            'run_date': datetime.now().isoformat(),
            'configuration': {
                'min_file_modification_date': self.config.min_file_modification_date.isoformat(),
                'min_effective_year': self.config.min_effective_year,
                'incremental_enabled': self.config.enable_incremental_processing,
                'file_date_prefilter_enabled': self.config.enable_file_date_prefilter
            },
            'incremental_stats': complexity_analysis.get('incremental_stats', {}),
            'efficiency_gains': {
                'total_discovered': complexity_analysis.get('incremental_stats', {}).get('total_discovered', 0),
                'actually_processed': complexity_analysis.get('incremental_stats', {}).get('newly_processed', 0),
                'processing_reduction_percentage': (
                    (complexity_analysis.get('incremental_stats', {}).get('skipped_old_files', 0) + 
                     complexity_analysis.get('incremental_stats', {}).get('skipped_unchanged', 0)) / 
                    max(complexity_analysis.get('incremental_stats', {}).get('total_discovered', 1), 1)
                ) * 100
            }
        }
        with open(incremental_summary_path, 'w', encoding='utf-8') as f:
            json.dump(incremental_data, f, indent=2, default=str)
        output_files['incremental_summary'] = str(incremental_summary_path)
        logger.info(f"Incremental processing summary saved to: {incremental_summary_path}")

        # Separate report for problematic PDFs
        problematic_pdfs = [doc for doc in all_docs 
                           if doc.pdf_complexity_level in ['problematic', 'complex'] 
                           or doc.classification_result in ['pdf_corrupted', 'form_field_issues']]
        
        if problematic_pdfs:
            problematic_report_path = self.output_dir / f"problematic_pdfs_report_{timestamp}.csv"
            with open(problematic_report_path, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(['source_path', 'pdf_name', 'state', 'complexity_level', 
                               'issues', 'classification_result', 'file_size'])
                for doc in problematic_pdfs:
                    writer.writerow([
                        doc.source_path, 
                        doc.pdf_name, 
                        doc.state, 
                        doc.pdf_complexity_level,
                        '; '.join(doc.pdf_issues) if doc.pdf_issues else '', 
                        doc.classification_result, 
                        doc.file_size
                    ])
            output_files['problematic_pdfs'] = str(problematic_report_path)
            logger.info(f"Problematic PDFs report saved to: {problematic_report_path}")

        # Simple text file for relevant documents (for extraction pipeline)
        relevant_paths = self.output_dir / f"relevant_healthcare_documents_{timestamp}.txt"
        with open(relevant_paths, 'w', encoding='utf-8') as f:
            for doc in results['relevant']:
                f.write(f"{doc.source_path}\n")
        output_files['relevant_documents'] = str(relevant_paths)
        logger.info(f"Relevant healthcare documents list saved to: {relevant_paths}")

        # NEW: Files to process next time (for monitoring)
        if results.get('old_file_date') or results.get('skipped_unchanged'):
            skipped_files_report = self.output_dir / f"skipped_files_report_{timestamp}.csv"
            with open(skipped_files_report, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(['source_path', 'pdf_name', 'state', 'skip_reason', 'file_modification_date'])
                
                for doc in results.get('old_file_date', []):
                    writer.writerow([doc.source_path, doc.pdf_name, doc.state, 'old_file_date', doc.file_modification_date])
                
                for doc in results.get('skipped_unchanged', []):
                    writer.writerow([doc.source_path, doc.pdf_name, doc.state, 'unchanged_since_last_run', doc.file_modification_date])
            
            output_files['skipped_files'] = str(skipped_files_report)
            logger.info(f"Skipped files report saved to: {skipped_files_report}")

        return output_files

    def _print_enhanced_summary(self, results: Dict[str, List[DocumentMetadata]]):
        """Enhanced summary with incremental processing statistics."""
        print("\n" + "="*70)
        print("ENHANCED CLASSIFICATION SUMMARY WITH INCREMENTAL PROCESSING")
        print("="*70)
        print(f"  Relevant Documents (2020+):     {len(results['relevant'])}")
        print(f"  Not Healthcare Contracts:       {len(results['not_healthcare'])}")
        print(f"  Old Effective Dates (< 2020):   {len(results['old_date'])}")
        print(f"  No Date Found:                  {len(results['no_date'])}")
        print(f"  Form Field Issues:              {len(results['form_field_issues'])}")
        print(f"  PDF Corrupted/Unreadable:       {len(results['pdf_corrupted'])}")
        print(f"  Failed Processing:              {len(results['failed'])}")
        print("-" * 70)
        print("EFFICIENCY GAINS:")
        print(f"  Old File Dates (< 2019):        {len(results['old_file_date'])}")
        print(f"  Skipped (Unchanged):            {len(results['skipped_unchanged'])}")
        print("-" * 70)
        
        # Incremental processing statistics
        complexity_analysis = results.get('complexity_analysis', {})
        incremental_stats = complexity_analysis.get('incremental_stats', {})
        
        if incremental_stats:
            print("\nINCREMENTAL PROCESSING EFFICIENCY:")
            print(f"  Total Documents Discovered:     {incremental_stats.get('total_discovered', 0)}")
            print(f"  Skipped (Old File Date):        {incremental_stats.get('skipped_old_files', 0)}")
            print(f"  Skipped (Unchanged):            {incremental_stats.get('skipped_unchanged', 0)}")
            print(f"  Actually Processed:             {incremental_stats.get('newly_processed', 0)}")
            
            total_discovered = incremental_stats.get('total_discovered', 1)
            total_skipped = incremental_stats.get('skipped_old_files', 0) + incremental_stats.get('skipped_unchanged', 0)
            efficiency_percentage = (total_skipped / total_discovered) * 100 if total_discovered > 0 else 0
            
            print(f"  Processing Efficiency:          {efficiency_percentage:.1f}% reduction")
            print(f"  API Calls Saved:                {total_skipped}")
        
        # Complexity distribution
        if complexity_analysis:
            print(f"\nCOMPLEXITY DISTRIBUTION:")
            distribution = complexity_analysis.get('complexity_distribution', {})
            for level, stats in distribution.items():
                count = stats.get('count', 0)
                percentage = stats.get('percentage', 0)
                print(f"  {level.capitalize():<12}: {count:>4} ({percentage:>5.1f}%)")
            
            # Common issues
            common_issues = complexity_analysis.get('common_issues', {})
            if common_issues:
                print(f"\nCOMMON ISSUES:")
                for issue, count in list(common_issues.items())[:5]:  # Top 5 issues
                    print(f"  {issue.replace('_', ' ').title():<20}: {count}")
            
            # Recommendations
            recommendations = complexity_analysis.get('recommendations', [])
            if recommendations:
                print(f"\nRECOMMENDATIONS:")
                for i, rec in enumerate(recommendations, 1):
                    print(f"  {i}. {rec}")
        
        print("="*70)
        
        # Performance metrics
        total_docs = sum(len(docs) for category, docs in results.items() 
                        if category not in ['complexity_analysis', 'skipped_unchanged', 'old_file_date'])
        if total_docs > 0:
            success_rate = (len(results['relevant']) / total_docs) * 100
            print(f"\nSUCCESS RATE: {success_rate:.1f}% ({len(results['relevant'])}/{total_docs})")
            
            problematic_count = (len(results['pdf_corrupted']) + 
                               len(results['form_field_issues']) + 
                               len(results['failed']))
            if problematic_count > 0:
                print(f"ATTENTION NEEDED: {problematic_count} documents require manual review")

# ==========================================
# CLIENT SETUP & MAIN EXECUTION
# ==========================================

def create_client():
    """Create Google AI client."""
    return genai.Client(vertexai=True, project="anbc-hcb-dev", location="us-central1")

def run_enhanced_healthcare_classification(
    root_folder_path: str, 
    output_dir: str = "enhanced_healthcare_classification_results",
    min_file_date: str = "2019-01-01",
    enable_incremental: bool = True
) -> Dict[str, Any]:
    """
    Main function to run enhanced healthcare document classification with incremental processing.
    
    Args:
        root_folder_path: Path to root folder containing AGREEMENTS-NON STANDARD
        output_dir: Directory to save classification results
        min_file_date: Minimum file modification date (YYYY-MM-DD format)
        enable_incremental: Enable incremental processing to skip unchanged files
        
    Returns:
        Dictionary with classification results and statistics
    """
    client = create_client()
    classification_system = EnterpriseHealthcareClassificationSystem(
        client=client, 
        output_dir=output_dir
    )
    
    # Configure the system based on parameters
    if min_file_date:
        try:
            classification_system.config.min_file_modification_date = parser.parse(min_file_date).date()
        except Exception as e:
            logger.warning(f"Invalid date format '{min_file_date}', using default 2019-01-01: {e}")
            classification_system.config.min_file_modification_date = date(2019, 1, 1)
    
    classification_system.config.enable_incremental_processing = enable_incremental
    
    # Execute the enhanced classification process
    return classification_system.run_classification(
        root_folder_path=root_folder_path
    )

# ==========================================
# MAIN EXECUTION
# ==========================================

if __name__ == "__main__":
    # Configure for your enterprise environment
    ROOT_FOLDER_PATH = r"\\Network\midp-sfs-007\midamreprc"  # Your network path
    OUTPUT_DIR = "enhanced_healthcare_classification_results"
    MIN_FILE_DATE = "2019-01-01"  # Only process files modified from this date
    ENABLE_INCREMENTAL = True     # Enable incremental processing

    try:
        # Run the enhanced healthcare document classification
        results = run_enhanced_healthcare_classification(
            root_folder_path=ROOT_FOLDER_PATH,
            output_dir=OUTPUT_DIR,
            min_file_date=MIN_FILE_DATE,
            enable_incremental=ENABLE_INCREMENTAL
        )
        
        print("\nENHANCED HEALTHCARE CLASSIFICATION COMPLETED SUCCESSFULLY!")
        print(f"Check the output directory: {results.get('output_directory')}")
        print(f"Use 'relevant_healthcare_documents_*.txt' with your extraction code")
        print(f"Found {results.get('relevant_documents_found', 0)} relevant healthcare documents")
        
        # Display efficiency gains
        efficiency_stats = results.get('processing_efficiency', {})
        if efficiency_stats:
            print(f"\nEFFICIENCY GAINS:")
            print(f"  Documents skipped: {efficiency_stats.get('documents_skipped', 0)}")
            print(f"  API calls saved: {efficiency_stats.get('api_calls_saved', 0)}")
            print(f"  Time saved: {efficiency_stats.get('time_saved_percentage', 0):.1f}%")
        
    except Exception as e:
        logger.error(f"Critical error in main execution: {e}")
        print(f"Error: {e}")
