import fitz
import csv
import json
import logging
import os
import time
import threading
import signal
import sys
from pathlib import Path
from typing import List, Optional, Dict, Any
from google.genai import types
import google.genai as genai
from datetime import datetime, date
from dataclasses import dataclass, field
from dateutil import parser
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock, Event
from collections import defaultdict

# Enhanced logging setup with thread safety
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(threadName)-10s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('healthcare_classification.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class DocumentMetadata:
    """Represents a single PDF, holding all its metadata and final classification details."""
    source_path: str
    pdf_name: str
    state: str
    file_size: int
    last_modified: datetime
    classification_status: str = "pending"
    classification_result: str = "unknown"
    detected_on_page: Optional[int] = None
    effective_date: Optional[datetime] = None
    stage_failed: str = ""
    confidence_score: float = 0.0
    processing_time: float = 0.0
    error_message: str = ""
    pdf_complexity_level: str = ""
    pdf_issues: List[str] = field(default_factory=list)
    
    # Enhanced file date tracking
    file_modification_date: date = None
    file_date_meets_criteria: bool = False
    is_newly_modified: bool = False
    
    # Thread processing tracking
    thread_id: str = ""
    retry_count: int = 0
    processing_started_at: Optional[datetime] = None

    def __post_init__(self):
        if self.pdf_issues is None:
            self.pdf_issues = []
        # Set file modification date for easy access
        if self.last_modified:
            self.file_modification_date = self.last_modified.date()

@dataclass
class ClassificationResult:
    """A simple structure to pass results between classification methods."""
    is_relevant: bool
    detected_on_page: Optional[int] = None
    effective_date: Optional[datetime] = None
    stage_failed: str = ""
    confidence: float = 0.0
    processing_time: float = 0.0

@dataclass
class PDFComplexityInfo:
    """Structure to hold PDF complexity analysis results."""
    has_form_fields: bool = False
    has_digital_signatures: bool = False
    has_encryption: bool = False
    has_embedded_files: bool = False
    page_count: int = 0
    complexity_level: str = 'simple'
    potential_issues: List[str] = field(default_factory=list)
    analysis_successful: bool = True

class ClassificationConfig:
    """EXACT configuration from working project knowledge code."""
    def __init__(self):
        # Folder names the script will look for.
        self.agreements_folder = "AGREEMENTS-NON STANDARD"
        self.hospital_subfolder = "HOSPITAL"
        
        # Key business logic setting: the minimum year for a document to be considered relevant.
        self.min_effective_year = 2020
        
        # Settings for AI processing - EXACT FROM WORKING CODE
        self.image_dpi = 150
        self.first_pages_count = 10
        self.last_pages_count = 10
        
        # PDF error handling settings
        self.max_repair_attempts = 3
        self.skip_corrupted_pages = True
        self.enable_complexity_analysis = True
        
        # Threading configuration
        self.max_threads = 5
        self.enable_threading = True
        self.thread_timeout_minutes = 10
        
        # State management
        self.enable_incremental_processing = True
        self.min_file_modification_date = date(2019, 1, 1)
        self.state_file_path = "classification_state.json"
        
        # EXACT prompts from working code
        self.healthcare_contract_prompt = "Analyze the image. Is this a formal letter from Aetna about a 'charge master increase' where they state 'we have received' a notification? Answer only YES or NO."
        self.effective_date_prompt = 'This is an Aetna charge master increase letter. Find the EFFECTIVE DATE mentioned in the letter (phrases like "effective January 1, 2024" or "to become effective 1/1/2025"). Respond with just the date found, or "NOT_FOUND" if no effective date exists.'
        
        # US states for folder identification
        self.us_states = {
            'ALABAMA', 'ALASKA', 'ARIZONA', 'ARKANSAS', 'CALIFORNIA', 'COLORADO', 
            'CONNECTICUT', 'DELAWARE', 'FLORIDA', 'GEORGIA', 'HAWAII', 'IDAHO', 
            'ILLINOIS', 'INDIANA', 'IOWA', 'KANSAS', 'KENTUCKY', 'LOUISIANA', 
            'MAINE', 'MARYLAND', 'MASSACHUSETTS', 'MICHIGAN', 'MINNESOTA', 
            'MISSISSIPPI', 'MISSOURI', 'MONTANA', 'NEBRASKA', 'NEVADA', 
            'NEW HAMPSHIRE', 'NEW JERSEY', 'NEW MEXICO', 'NEW YORK', 
            'NORTH CAROLINA', 'NORTH DAKOTA', 'OHIO', 'OKLAHOMA', 'OREGON', 
            'PENNSYLVANIA', 'RHODE ISLAND', 'SOUTH CAROLINA', 'SOUTH DAKOTA', 
            'TENNESSEE', 'TEXAS', 'UTAH', 'VERMONT', 'VIRGINIA', 'WASHINGTON', 
            'WEST VIRGINIA', 'WISCONSIN', 'WYOMING', 'PUERTO RICO', 'DISTRICT OF COLUMBIA'
        }

class StateManager:
    """Thread-safe state manager for incremental processing."""
    
    def __init__(self, config: ClassificationConfig):
        self.config = config
        self.state_file_path = Path(config.state_file_path)
        self.processed_files = {}
        self.last_run_date = None
        self._lock = Lock()
        self.load_state()
    
    def load_state(self):
        """Load the previous run state from disk."""
        try:
            if self.state_file_path.exists():
                with open(self.state_file_path, 'r', encoding='utf-8') as f:
                    state_data = json.load(f)
                    
                self.last_run_date = parser.parse(state_data.get('last_run_date')).date() if state_data.get('last_run_date') else None
                self.processed_files = state_data.get('processed_files', {})
                
                logger.info(f"Loaded state: Last run = {self.last_run_date}, {len(self.processed_files)} previously processed files")
            else:
                logger.info("No previous state found. Starting fresh run.")
        except Exception as e:
            logger.warning(f"Could not load previous state: {e}. Starting fresh.")
            self.processed_files = {}
            self.last_run_date = None
    
    def save_state(self, current_run_date: date):
        """Save the current run state to disk with thread safety."""
        with self._lock:
            try:
                state_data = {
                    'last_run_date': current_run_date.isoformat(),
                    'processed_files': self.processed_files,
                    'total_files_processed': len(self.processed_files),
                    'final_save_timestamp': datetime.now().isoformat()
                }
                
                # Atomic write
                temp_path = self.state_file_path.with_suffix('.tmp')
                with open(temp_path, 'w', encoding='utf-8') as f:
                    json.dump(state_data, f, indent=2, default=str)
                temp_path.replace(self.state_file_path)
                
                logger.info(f"Saved final state: {len(self.processed_files)} processed files as of {current_run_date}")
                    
            except Exception as e:
                logger.error(f"Could not save state: {e}")
    
    def should_process_file(self, file_path: str, file_modified_date: date) -> bool:
        """Thread-safe determination if a file needs processing."""
        with self._lock:
            file_info = self.processed_files.get(file_path)
            
            if not file_info:
                return True
            
            # Check if file was modified since last processing
            last_processed_date = parser.parse(file_info['last_modified']).date()
            return file_modified_date > last_processed_date
    
    def update_file_status(self, file_path: str, file_modified_date: date, classification_result: str, thread_id: str):
        """Thread-safe update of file processing status."""
        with self._lock:
            self.processed_files[file_path] = {
                'last_modified': file_modified_date.isoformat(),
                'classification_result': classification_result,
                'processed_on': datetime.now().isoformat(),
                'thread_id': thread_id
            }

class ProgressMonitor:
    """Thread-safe progress monitoring."""
    
    def __init__(self, total_documents: int, config: ClassificationConfig):
        self.total_documents = total_documents
        self.config = config
        self._lock = Lock()
        self.start_time = datetime.now()
        
        # Overall statistics
        self.completed_documents = 0
        self.successful_documents = 0
        self.failed_documents = 0
        self.documents_by_result = defaultdict(int)
        
        # Performance tracking
        self.last_update_time = datetime.now()
        self.last_completed_count = 0
        
        # Graceful shutdown
        self.shutdown_event = Event()
        
        # Start monitoring thread
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True, name="ProgressMonitor")
        self.monitor_thread.start()
    
    def update_progress(self, document_name: str, is_successful: bool, result_type: str, processing_time: float):
        """Update progress for a completed document."""
        with self._lock:
            if is_successful:
                self.successful_documents += 1
            else:
                self.failed_documents += 1
            
            self.completed_documents += 1
            self.documents_by_result[result_type] += 1
    
    def _monitor_loop(self):
        """Background monitoring loop."""
        while not self.shutdown_event.is_set():
            try:
                self._print_progress_update()
                time.sleep(10)  # Update every 10 seconds
            except Exception as e:
                logger.error(f"Error in progress monitor: {e}")
    
    def _print_progress_update(self):
        """Print progress update."""
        with self._lock:
            current_time = datetime.now()
            elapsed_time = current_time - self.start_time
            
            # Calculate overall rate
            current_completed = self.completed_documents
            time_since_last = (current_time - self.last_update_time).total_seconds()
            
            if time_since_last > 0:
                recent_rate = (current_completed - self.last_completed_count) / (time_since_last / 60)
            else:
                recent_rate = 0
            
            # Calculate ETA
            if current_completed > 0 and recent_rate > 0:
                remaining_docs = self.total_documents - current_completed
                eta_minutes = remaining_docs / recent_rate
                eta_hours = eta_minutes / 60
            else:
                eta_hours = 0
            
            # Success rate
            success_rate = (self.successful_documents / max(current_completed, 1)) * 100
            
            # Clear console for clean update
            os.system('cls' if os.name == 'nt' else 'clear')
            
            print(f"\n{'='*80}")
            print(f"HEALTHCARE CLASSIFICATION PROGRESS - {current_time.strftime('%H:%M:%S')}")
            print(f"{'='*80}")
            print(f"Overall: {current_completed:,}/{self.total_documents:,} ({(current_completed/max(self.total_documents, 1))*100:.1f}%) | Success Rate: {success_rate:.1f}%")
            print(f"Elapsed: {str(elapsed_time).split('.')[0]} | ETA: {eta_hours:.1f} hours | Rate: {recent_rate:.1f} docs/min")
            print(f"-" * 80)
            
            # Result breakdown
            if self.documents_by_result:
                print("RESULTS: ", end="")
                result_parts = []
                for result_type, count in sorted(self.documents_by_result.items()):
                    result_parts.append(f"{result_type}: {count}")
                print(" | ".join(result_parts))
            
            print(f"{'='*80}")
            
            # Update tracking variables
            self.last_update_time = current_time
            self.last_completed_count = current_completed
    
    def shutdown(self):
        """Gracefully shutdown the monitor."""
        self.shutdown_event.set()
        if hasattr(self, 'monitor_thread'):
            self.monitor_thread.join(timeout=5)

class EnhancedPDFHandler:
    """EXACT PDF handler from working project knowledge code."""
    
    @staticmethod
    def quick_complexity_check(pdf_doc: fitz.Document) -> PDFComplexityInfo:
        """EXACT complexity analysis from working code."""
        complexity_info = PDFComplexityInfo()
        
        try:
            complexity_info.page_count = len(pdf_doc)
            
            # Check for form fields (sample first few pages for performance)
            pages_to_check = min(5, len(pdf_doc))
            for page_num in range(pages_to_check):
                try:
                    page = pdf_doc.load_page(page_num)
                    if page.get_widgets():  # Form fields/widgets detected
                        complexity_info.has_form_fields = True
                        complexity_info.potential_issues.append('form_fields')
                        break
                except Exception:
                    continue
            
            # Check for encryption
            if pdf_doc.needs_pass:
                complexity_info.has_encryption = True
                complexity_info.potential_issues.append('password_protected')
            
            # Check for embedded files
            if pdf_doc.embfile_count() > 0:
                complexity_info.has_embedded_files = True
                complexity_info.potential_issues.append('embedded_files')
            
            # Check metadata for signatures
            try:
                metadata = pdf_doc.metadata
                if metadata and any(sig_indicator in str(metadata).lower() 
                                  for sig_indicator in ['signature', 'signed', 'certif']):
                    complexity_info.has_digital_signatures = True
                    complexity_info.potential_issues.append('digital_signatures')
            except Exception:
                pass
            
            # Determine complexity level
            issue_count = len(complexity_info.potential_issues)
            if issue_count == 0:
                complexity_info.complexity_level = 'simple'
            elif issue_count <= 2:
                complexity_info.complexity_level = 'moderate'
            else:
                complexity_info.complexity_level = 'complex'
                
        except Exception as e:
            complexity_info.potential_issues.append(f'analysis_error: {str(e)}')
            complexity_info.complexity_level = 'problematic'
            complexity_info.analysis_successful = False
        
        return complexity_info

    @staticmethod
    def attempt_pdf_open_with_strategies(pdf_path: str) -> Optional[fitz.Document]:
        """EXACT PDF opening strategies from working code."""
        logger.debug(f"Attempting to open PDF: {os.path.basename(pdf_path)}")
        
        # Strategy 1: Standard open (fastest)
        try:
            doc = fitz.open(pdf_path)
            logger.debug("Standard open successful")
            return doc
        except Exception as e:
            logger.debug(f"Standard open failed: {e}")
        
        # Strategy 2: Open with repair flag
        try:
            doc = fitz.open(pdf_path, filetype="pdf")
            logger.debug("Repair mode open successful")
            return doc
        except Exception as e:
            logger.debug(f"Repair mode open failed: {e}")
        
        # Strategy 3: Memory-based opening (handles file locking issues)
        try:
            with open(pdf_path, 'rb') as f:
                pdf_data = f.read()
            doc = fitz.open(stream=pdf_data, filetype="pdf")
            logger.debug("Memory stream open successful")
            return doc
        except Exception as e:
            logger.debug(f"Memory stream open failed: {e}")
        
        # Strategy 4: Proper visual content preservation
        try:
            source_doc = fitz.open(pdf_path)
            clean_doc = fitz.open()
            
            successful_pages = 0
            for page_num in range(min(len(source_doc), 20)):  # Limit for safety
                try:
                    # Properly copy the entire page including visual content
                    clean_doc.insert_pdf(source_doc, from_page=page_num, to_page=page_num)
                    successful_pages += 1
                except Exception as page_error:
                    logger.debug(f"Skipped page {page_num + 1}: {page_error}")
                    continue
            
            source_doc.close()
            
            if successful_pages > 0:
                logger.debug(f"Page reconstruction successful ({successful_pages} pages)")
                return clean_doc
            else:
                clean_doc.close()
                
        except Exception as e:
            logger.debug(f"Page reconstruction failed: {e}")
        
        logger.warning(f"All repair strategies failed for: {os.path.basename(pdf_path)}")
        return None

    @staticmethod
    def safe_page_access(doc: fitz.Document, page_idx: int) -> Optional[fitz.Page]:
        """EXACT safe page access from working code."""
        try:
            page = doc.load_page(page_idx)
            # Test if page can be rendered by trying to get its rect
            _ = page.rect
            return page
        except Exception as e:
            logger.debug(f"Page {page_idx + 1} corrupted or inaccessible: {e}")
            return None

class HealthcareDocumentClassifier:
    """EXACT classification logic from working project knowledge code."""
    
    def __init__(self, client, config: ClassificationConfig):
        self.client = client
        self.config = config
        self.pdf_handler = EnhancedPDFHandler()
        
        # State management
        self.state_manager = StateManager(config)
        self.progress_monitor = None
        
        # Thread-safe statistics
        self._stats_lock = Lock()
        self.complexity_stats = defaultdict(int)
        self.issue_frequency = defaultdict(int)
        
        # Graceful shutdown handling
        self.shutdown_requested = Event()
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
    
    def _signal_handler(self, signum, frame):
        """Handle graceful shutdown on Ctrl+C or terminate signal."""
        logger.info("Shutdown signal received. Initiating graceful shutdown...")
        self.shutdown_requested.set()
        
        if self.progress_monitor:
            self.progress_monitor.shutdown()
        
        # Save current state
        self.state_manager.save_state(date.today())
        
        logger.info("Graceful shutdown completed.")
        sys.exit(0)
    
    def classify_all_documents(self, documents: List[DocumentMetadata]) -> Dict[str, List[DocumentMetadata]]:
        """Main classification method with parallel processing."""
        logger.info(f"Starting classification of {len(documents)} documents with {self.config.max_threads} threads...")
        
        # Apply pre-filtering
        documents_to_process = self._apply_pre_filtering(documents)
        
        # Initialize progress monitoring
        self.progress_monitor = ProgressMonitor(len(documents_to_process), self.config)
        
        # Initialize results structure
        results = {
            'relevant': [], 'not_healthcare': [], 'old_date': [], 'no_date': [], 
            'failed': [], 'pdf_corrupted': [], 'form_field_issues': [],
            'old_file_date': [], 'skipped_unchanged': []
        }
        
        # Thread-safe result collection
        results_lock = Lock()
        
        def process_document_wrapper(doc: DocumentMetadata) -> DocumentMetadata:
            """Wrapper function for thread processing."""
            thread_id = threading.current_thread().name
            
            try:
                # Check for shutdown
                if self.shutdown_requested.is_set():
                    return doc
                
                doc.processing_started_at = datetime.now()
                doc.thread_id = thread_id
                
                # EXACT classification from working code
                classification_result = self._classify_single_document_exact(doc, thread_id)
                
                # Update document with results
                doc.processing_time = classification_result.processing_time
                doc.confidence_score = classification_result.confidence
                doc.detected_on_page = classification_result.detected_on_page
                doc.effective_date = classification_result.effective_date
                doc.stage_failed = classification_result.stage_failed
                doc.classification_status = "completed"
                
                # Determine result category
                if classification_result.is_relevant:
                    doc.classification_result = "relevant"
                    is_successful = True
                else:
                    if "old_file_date" in doc.stage_failed.lower():
                        doc.classification_result = 'old_file_date'
                    elif "form field" in doc.stage_failed.lower():
                        doc.classification_result = 'form_field_issues'
                    elif "Stage 2: No effective date" in doc.stage_failed:
                        doc.classification_result = 'no_date'
                    elif "Stage 2: Effective date before" in doc.stage_failed:
                        doc.classification_result = 'old_date'
                    elif "PDF corruption" in doc.stage_failed or "Unable to open" in doc.stage_failed:
                        doc.classification_result = 'pdf_corrupted'
                    else:
                        doc.classification_result = 'not_healthcare'
                    is_successful = False
                
                # Update thread-safe statistics
                with self._stats_lock:
                    if doc.pdf_complexity_level:
                        self.complexity_stats[doc.pdf_complexity_level] += 1
                    for issue in doc.pdf_issues:
                        self.issue_frequency[issue] += 1
                
                # Update state management
                self.state_manager.update_file_status(
                    doc.source_path, 
                    doc.file_modification_date, 
                    doc.classification_result,
                    thread_id
                )
                
                # Update progress monitoring
                self.progress_monitor.update_progress(
                    doc.pdf_name, 
                    is_successful, 
                    doc.classification_result, 
                    doc.processing_time
                )
                
                # Thread-safe result collection
                with results_lock:
                    results[doc.classification_result].append(doc)
                
                return doc
                
            except Exception as e:
                logger.error(f"[{thread_id}] Failed to process {doc.pdf_name}: {e}")
                doc.classification_result = "failed"
                doc.error_message = str(e)
                doc.pdf_complexity_level = "problematic"
                
                with results_lock:
                    results['failed'].append(doc)
                
                return doc
        
        # Execute with ThreadPoolExecutor
        logger.info(f"Processing {len(documents_to_process)} documents with {self.config.max_threads} threads...")
        
        completed_count = 0
        with ThreadPoolExecutor(max_workers=self.config.max_threads, thread_name_prefix="ClassifierWorker") as executor:
            # Submit all tasks
            future_to_doc = {executor.submit(process_document_wrapper, doc): doc for doc in documents_to_process}
            
            # Process completed tasks
            for future in as_completed(future_to_doc):
                if self.shutdown_requested.is_set():
                    logger.info("Shutdown requested, cancelling remaining tasks...")
                    for remaining_future in future_to_doc:
                        remaining_future.cancel()
                    break
                
                try:
                    doc = future.result(timeout=self.config.thread_timeout_minutes * 60)
                    completed_count += 1
                    
                    if completed_count % 100 == 0:
                        logger.info(f"Completed {completed_count}/{len(documents_to_process)} documents")
                        
                except Exception as e:
                    logger.error(f"Task execution failed: {e}")
        
        # Add skipped documents to results
        skipped_docs = [doc for doc in documents if not doc.is_newly_modified and doc not in documents_to_process]
        results['skipped_unchanged'].extend(skipped_docs)
        
        # Generate comprehensive analysis
        total_docs = len(documents)
        results['complexity_analysis'] = {
            'total_analyzed': total_docs,
            'complexity_distribution': {
                level: {'count': count, 'percentage': (count/total_docs)*100 if total_docs > 0 else 0}
                for level, count in self.complexity_stats.items()
            },
            'common_issues': dict(sorted(self.issue_frequency.items(), key=lambda x: x[1], reverse=True))
        }
        
        # Shutdown progress monitor
        if self.progress_monitor:
            self.progress_monitor.shutdown()
        
        # Save final state
        self.state_manager.save_state(date.today())
        
        logger.info(f"Classification completed. Processed {completed_count} documents.")
        return results
    
    def _apply_pre_filtering(self, documents: List[DocumentMetadata]) -> List[DocumentMetadata]:
        """Apply intelligent pre-filtering."""
        documents_to_process = []
        
        for doc in documents:
            # Check file modification date criteria
            doc.file_date_meets_criteria = doc.file_modification_date >= self.config.min_file_modification_date
            
            # Check incremental processing
            if self.config.enable_incremental_processing:
                doc.is_newly_modified = self.state_manager.should_process_file(
                    doc.source_path, 
                    doc.file_modification_date
                )
            else:
                doc.is_newly_modified = True
            
            # Apply filtering logic
            if not doc.file_date_meets_criteria:
                doc.classification_result = "old_file_date"
                doc.stage_failed = f"File modified before {self.config.min_file_modification_date} ({doc.file_modification_date})"
                continue
            
            if not doc.is_newly_modified:
                doc.classification_result = "skipped_unchanged"
                doc.stage_failed = f"File unchanged since last run"
                continue
            
            documents_to_process.append(doc)
        
        logger.info(f"Pre-filtering results:")
        logger.info(f"  Total discovered: {len(documents)}")
        logger.info(f"  To process: {len(documents_to_process)}")
        
        return documents_to_process
    
    def _classify_single_document_exact(self, doc: DocumentMetadata, thread_id: str) -> ClassificationResult:
        """EXACT single document classification from working project knowledge code."""
        start_time = time.time()
        pdf_doc = None
        
        try:
            # Step 1: Attempt to open PDF with repair strategies
            pdf_doc = self.pdf_handler.attempt_pdf_open_with_strategies(doc.source_path)
            
            if pdf_doc is None:
                doc.pdf_complexity_level = "problematic"
                doc.pdf_issues = ["unable_to_open"]
                return ClassificationResult(
                    False, 
                    stage_failed="PDF corruption: Unable to open PDF after all repair attempts",
                    processing_time=time.time() - start_time
                )
            
            # Step 2: Analyze complexity on the already-opened document
            if self.config.enable_complexity_analysis:
                complexity_info = self.pdf_handler.quick_complexity_check(pdf_doc)
                doc.pdf_complexity_level = complexity_info.complexity_level
                doc.pdf_issues = complexity_info.potential_issues.copy()
            else:
                doc.pdf_complexity_level = "simple"
                doc.pdf_issues = []
            
            # Check if PDF has any accessible pages
            if len(pdf_doc) == 0:
                doc.pdf_complexity_level = "problematic"
                doc.pdf_issues.append("no_pages")
                return ClassificationResult(
                    False, 
                    stage_failed="PDF corruption: No accessible pages found",
                    processing_time=time.time() - start_time
                )
            
            # Step 3: EXACT page selection from working code
            pages_to_check = self._get_pages_to_analyze_safely(pdf_doc)
            
            if not pages_to_check:
                doc.pdf_issues.append("no_accessible_pages")
                return ClassificationResult(
                    False, 
                    stage_failed="PDF corruption: No accessible pages for analysis",
                    processing_time=time.time() - start_time
                )

            accessible_pages_count = 0
            for page_idx in pages_to_check:
                # Use the handler's safe_page_access method
                page = self.pdf_handler.safe_page_access(pdf_doc, page_idx)
                if page is None:
                    logger.debug(f"[{thread_id}] Skipping corrupted page {page_idx + 1} in {doc.pdf_name}")
                    continue
                
                accessible_pages_count += 1
                
                # Stage 1: EXACT healthcare contract check from working code
                if not self._is_healthcare_contract_safe(page):
                    continue

                # Stage 2: EXACT effective date check from working code
                date_result = self._get_effective_date_safe(page)
                processing_time = time.time() - start_time
                
                if date_result and date_result.year >= self.config.min_effective_year:
                    confidence = 0.9 if doc.pdf_complexity_level == 'simple' else 0.8
                    return ClassificationResult(True, page_idx + 1, date_result, confidence=confidence, processing_time=processing_time)
                elif date_result:
                    confidence = 0.9 if doc.pdf_complexity_level == 'simple' else 0.8
                    stage_failed_msg = f"Stage 2: Effective date before {self.config.min_effective_year} ({date_result.year})"
                    if doc.pdf_complexity_level in ['complex', 'moderate']:
                        stage_failed_msg += f" (complex PDF: {', '.join(doc.pdf_issues)})"
                    return ClassificationResult(False, page_idx + 1, date_result, stage_failed=stage_failed_msg, confidence=confidence, processing_time=processing_time)
                else: 
                    confidence = 0.8 if doc.pdf_complexity_level == 'simple' else 0.7
                    stage_failed_msg = "Stage 2: No effective date found on relevant page"
                    if doc.pdf_complexity_level in ['complex', 'moderate']:
                        stage_failed_msg += f" (complex PDF: {', '.join(doc.pdf_issues)})"
                    return ClassificationResult(False, page_idx + 1, None, stage_failed=stage_failed_msg, confidence=confidence, processing_time=processing_time)

            # If we processed some pages but found no contracts
            if accessible_pages_count > 0:
                confidence = 0.9 if doc.pdf_complexity_level == 'simple' else 0.8
                stage_failed_msg = "Stage 1: Not a healthcare contract"
                if doc.pdf_complexity_level in ['complex', 'moderate']:
                    stage_failed_msg += f" (complex PDF: {', '.join(doc.pdf_issues)})"
                return ClassificationResult(False, stage_failed=stage_failed_msg, confidence=confidence, processing_time=time.time() - start_time)
            else:
                doc.pdf_issues.append("no_processable_pages")
                return ClassificationResult(False, stage_failed="PDF corruption: No pages could be processed", processing_time=time.time() - start_time)
                
        except Exception as e:
            raise RuntimeError(f"Failed to process PDF {doc.pdf_name}") from e
        finally:
            if pdf_doc:
                try:
                    pdf_doc.close()
                except:
                    pass

    def _get_pages_to_analyze_safely(self, pdf_doc: fitz.Document) -> List[int]:
        """EXACT page selection from working code."""
        total_pages = len(pdf_doc)
        
        if total_pages <= (self.config.first_pages_count + self.config.last_pages_count):
            candidate_pages = list(range(total_pages))
        else:
            first = list(range(self.config.first_pages_count))
            last = list(range(total_pages - self.config.last_pages_count, total_pages))
            candidate_pages = sorted(list(set(first + last)))
        
        # Filter out pages that can't be accessed
        accessible_pages = []
        for page_idx in candidate_pages:
            if self.pdf_handler.safe_page_access(pdf_doc, page_idx) is not None:
                accessible_pages.append(page_idx)
        
        logger.debug(f"Selected {len(accessible_pages)} accessible pages out of {len(candidate_pages)} candidates")
        return accessible_pages

    def _call_gemini_safe(self, page: fitz.Page, prompt: str) -> Optional[str]:
        """EXACT Gemini API call from working code."""
        try:
            mat = fitz.Matrix(self.config.image_dpi / 72, self.config.image_dpi / 72)
            pix = page.get_pixmap(matrix=mat)
            
            # Verify the pixmap is valid
            if pix.width == 0 or pix.height == 0:
                logger.debug("Generated pixmap has zero dimensions")
                return None
                
            image_bytes = pix.tobytes("png")
            
            if len(image_bytes) == 0:
                logger.debug("Generated image bytes are empty")
                return None
            
            response = self.client.models.generate_content(
                model='gemini-1.5-flash',
                contents=[types.Part.from_bytes(data=image_bytes, mime_type='image/png'), prompt]
            )
            return response.text.strip()
            
        except Exception as e:
            logger.debug(f"Gemini API call failed: {e}")
            return None

    def _is_healthcare_contract_safe(self, page: fitz.Page) -> bool:
        """EXACT healthcare contract detection from working code."""
        try:
            answer = self._call_gemini_safe(page, self.config.healthcare_contract_prompt)
            return answer and 'YES' in answer.upper()
        except Exception as e:
            logger.debug(f"Healthcare contract check failed: {e}")
            return False

    def _get_effective_date_safe(self, page: fitz.Page) -> Optional[datetime]:
        """EXACT effective date extraction from working code."""
        try:
            date_text = self._call_gemini_safe(page, self.config.effective_date_prompt)
            if not date_text or "NOT_FOUND" in date_text:
                return None
            return parser.parse(date_text)
        except (parser.ParserError, TypeError, Exception) as e:
            logger.debug(f"Could not parse date from AI response '{date_text}': {e}")
            return None

class EnterpriseFileDiscovery:
    """Enhanced file discovery with long path support."""
    def __init__(self, root_path: str, config: ClassificationConfig):
        self.root_path = Path(root_path)
        self.config = config

    def discover_all_documents(self) -> List[DocumentMetadata]:
        """Document discovery with long path fix."""
        logger.info(f"Starting document discovery from: {self.root_path}")
        agreements_path = self.root_path / self.config.agreements_folder
        
        if not agreements_path.exists():
            logger.error(f"'{self.config.agreements_folder}' folder not found at: {agreements_path}")
            return []

        all_documents = []
        state_folders = [f for f in agreements_path.iterdir() if f.is_dir() and self._is_state_folder(f)]
        logger.info(f"Found {len(state_folders)} state folders to process.")

        for state_folder in state_folders:
            all_documents.extend(self._process_single_state(state_folder))

        logger.info(f"Discovery complete. Total documents found: {len(all_documents)}")
        return all_documents

    def _process_single_state(self, state_folder: Path) -> List[DocumentMetadata]:
        """Process state folder with long path support."""
        logger.info(f"Processing State: {state_folder.name}")
        hospital_base_path = state_folder / self.config.hospital_subfolder
        
        if not hospital_base_path.exists():
            logger.warning(f"No '{self.config.hospital_subfolder}' folder found in {state_folder.name}.")
            return []

        state_documents = []
        hospital_subfolders = [d for d in hospital_base_path.iterdir() if d.is_dir()]
        logger.info(f"Found {len(hospital_subfolders)} hospital subfolders in {state_folder.name}.")

        # Apply long-path fix to hospital folder paths
        for hospital_folder_original in hospital_subfolders:
            try:
                # Convert the path to a long-path-aware string
                hospital_path_str = str(hospital_folder_original)
                if hospital_path_str.startswith('\\\\') and not hospital_path_str.startswith('\\\\?\\'):
                    hospital_path_str = '\\\\?\\UNC\\' + hospital_path_str[2:]
                
                # Create a new, corrected Path object to use for processing
                hospital_folder = Path(hospital_path_str)
                
                logger.info(f"  Scanning hospital: {hospital_folder_original.name}")
                pdf_paths = hospital_folder.rglob("*.pdf")
                
                for pdf_path in pdf_paths:
                    try:
                        path_str = str(pdf_path)
                        # The long-path prefix is already part of pdf_path, but we ensure it for safety
                        if path_str.startswith('\\\\') and not path_str.startswith('\\\\?\\'):
                             path_str = '\\\\?\\UNC\\' + path_str[2:]
                        
                        if os.path.exists(path_str):
                            stats = os.stat(path_str)
                            doc = DocumentMetadata(
                                source_path=path_str,
                                pdf_name=pdf_path.name,
                                state=state_folder.name,
                                file_size=stats.st_size,
                                last_modified=datetime.fromtimestamp(stats.st_mtime)
                            )
                            state_documents.append(doc)
                    except Exception as e:
                        logger.warning(f"    Could not stat file {pdf_path.name}: {e}")
                        continue
                        
            except Exception as e:
                # Use the original folder name in the error for easier identification
                logger.error(f"  Could not process hospital folder {hospital_folder_original.name}: {e}")
        
        logger.info(f"  Found {len(state_documents)} documents in {state_folder.name}.")
        return state_documents

    def _is_state_folder(self, folder_path: Path) -> bool:
        """Enhanced state folder identification."""
        return any(state in folder_path.name.upper() for state in self.config.us_states)

class HealthcareClassificationSystem:
    """Main orchestration class."""
    
    def __init__(self, client, output_dir: str = "healthcare_classification_results"):
        self.client = client
        self.config = ClassificationConfig()
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def run_classification(self, root_folder_path: str) -> Dict[str, Any]:
        """Execute the complete classification process."""
        start_time = time.time()
        
        logger.info("=" * 80)
        logger.info("STARTING HEALTHCARE CLASSIFICATION")
        logger.info("=" * 80)
        logger.info(f"Configuration:")
        logger.info(f"  Threads: {self.config.max_threads}")
        logger.info(f"  File date filter: {self.config.min_file_modification_date}")
        logger.info(f"  Incremental processing: {self.config.enable_incremental_processing}")
        logger.info("=" * 80)

        # Step 1: Document discovery
        discovery = EnterpriseFileDiscovery(root_folder_path, self.config)
        all_documents = discovery.discover_all_documents()
        
        if not all_documents:
            logger.error("No documents were found. Exiting.")
            return {
                'success': False,
                'error': 'No documents found',
                'output_directory': str(self.output_dir)
            }

        # Step 2: Classification with exact working logic
        classifier = HealthcareDocumentClassifier(self.client, self.config)
        results = classifier.classify_all_documents(all_documents)

        # Step 3: Save comprehensive results
        output_files = self._save_reports(all_documents, results)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        logger.info("=" * 80)
        logger.info("CLASSIFICATION COMPLETED")
        logger.info(f"Total processing time: {total_time/3600:.2f} hours")
        logger.info("=" * 80)
        
        self._print_summary(results, total_time)
        
        # Return comprehensive results
        return {
            'success': True,
            'output_directory': str(self.output_dir),
            'total_documents_discovered': len(all_documents),
            'relevant_documents_found': len(results['relevant']),
            'processing_time_hours': total_time / 3600,
            'processing_time_seconds': total_time,
            'output_files': output_files,
            'classification_breakdown': {
                'relevant': len(results['relevant']),
                'not_healthcare': len(results['not_healthcare']),
                'old_date': len(results['old_date']),
                'no_date': len(results['no_date']),
                'form_field_issues': len(results['form_field_issues']),
                'pdf_corrupted': len(results['pdf_corrupted']),
                'failed': len(results['failed']),
                'old_file_date': len(results['old_file_date']),
                'skipped_unchanged': len(results['skipped_unchanged'])
            },
            'complexity_analysis': results.get('complexity_analysis', {})
        }

    def _save_reports(self, all_docs: List[DocumentMetadata], results: Dict[str, List[DocumentMetadata]]) -> Dict[str, str]:
        """Save comprehensive reports."""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_files = {}
        
        # Comprehensive report
        report_path = self.output_dir / f"classification_report_{timestamp}.csv"
        with open(report_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                'source_path', 'pdf_name', 'state', 'classification_result', 
                'effective_date', 'detected_on_page', 'stage_failed', 'error_message', 
                'complexity_level', 'pdf_issues', 'processing_time', 'confidence_score',
                'file_modification_date', 'file_date_meets_criteria', 'is_newly_modified',
                'thread_id', 'retry_count'
            ])
            for doc in all_docs:
                writer.writerow([
                    doc.source_path, doc.pdf_name, doc.state, doc.classification_result, 
                    doc.effective_date.strftime('%Y-%m-%d') if doc.effective_date else '', 
                    doc.detected_on_page or '', doc.stage_failed, doc.error_message,
                    doc.pdf_complexity_level, '; '.join(doc.pdf_issues) if doc.pdf_issues else '',
                    f"{doc.processing_time:.2f}s", f"{doc.confidence_score:.2f}",
                    doc.file_modification_date.strftime('%Y-%m-%d') if doc.file_modification_date else '',
                    doc.file_date_meets_criteria, doc.is_newly_modified,
                    doc.thread_id, doc.retry_count
                ])
        output_files['detailed_report'] = str(report_path)
        
        # Relevant documents for extraction pipeline
        relevant_paths = self.output_dir / f"relevant_healthcare_documents_{timestamp}.txt"
        with open(relevant_paths, 'w', encoding='utf-8') as f:
            for doc in results['relevant']:
                f.write(f"{doc.source_path}\n")
        output_files['relevant_documents'] = str(relevant_paths)
        
        # Summary statistics
        summary_path = self.output_dir / f"processing_summary_{timestamp}.json"
        summary_data = {
            'total_documents': len(all_docs),
            'processing_results': {category: len(docs) for category, docs in results.items() 
                                 if category != 'complexity_analysis'},
            'complexity_analysis': results.get('complexity_analysis', {}),
            'average_processing_time': sum(doc.processing_time for doc in all_docs) / len(all_docs) if all_docs else 0,
            'success_rate': (len(results['relevant']) / len(all_docs)) * 100 if all_docs else 0
        }
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, indent=2, default=str)
        output_files['processing_summary'] = str(summary_path)
        
        logger.info(f"Reports saved to: {self.output_dir}")
        return output_files

    def _print_summary(self, results: Dict[str, List[DocumentMetadata]], total_time: float):
        """Print comprehensive summary."""
        print(f"\n{'='*80}")
        print("CLASSIFICATION SUMMARY")
        print(f"{'='*80}")
        print(f"Processing Time: {total_time/3600:.2f} hours ({total_time/60:.1f} minutes)")
        print(f"{'='*80}")
        
        # Results breakdown
        print(f"  Relevant Documents (2020+):     {len(results['relevant'])}")
        print(f"  Not Healthcare Contracts:       {len(results['not_healthcare'])}")
        print(f"  Old Effective Dates (< 2020):   {len(results['old_date'])}")
        print(f"  No Date Found:                  {len(results['no_date'])}")
        print(f"  Form Field Issues:              {len(results['form_field_issues'])}")
        print(f"  PDF Corrupted/Unreadable:       {len(results['pdf_corrupted'])}")
        print(f"  Failed Processing:              {len(results['failed'])}")
        print(f"-" * 80)
        print("EFFICIENCY GAINS:")
        print(f"  Old File Dates (< 2019):        {len(results['old_file_date'])}")
        print(f"  Skipped (Unchanged):            {len(results['skipped_unchanged'])}")
        
        print(f"{'='*80}")

# ==========================================
# CLIENT SETUP & MAIN EXECUTION
# ==========================================

def create_client():
    """Create Google AI client."""
    return genai.Client(vertexai=True, project="anbc-hcb-dev", location="us-central1")

def run_healthcare_classification(
    root_folder_path: str,
    output_dir: str = "healthcare_classification_results",
    threads: int = 5,
    min_file_date: str = "2019-01-01",
    enable_incremental: bool = True
) -> Dict[str, Any]:
    """
    Run healthcare classification with exact working logic and parallel processing.
    
    Args:
        root_folder_path: Path to root folder containing AGREEMENTS-NON STANDARD
        output_dir: Directory to save classification results
        threads: Number of parallel processing threads (1-10)
        min_file_date: Minimum file modification date (YYYY-MM-DD format)
        enable_incremental: Enable incremental processing to skip unchanged files
        
    Returns:
        Dictionary with comprehensive classification results
    """
    client = create_client()
    classification_system = HealthcareClassificationSystem(
        client=client, 
        output_dir=output_dir
    )
    
    # Configure the system based on parameters
    classification_system.config.max_threads = max(1, min(threads, 10))  # Safety bounds
    classification_system.config.enable_incremental_processing = enable_incremental
    
    if min_file_date:
        try:
            classification_system.config.min_file_modification_date = parser.parse(min_file_date).date()
        except Exception as e:
            logger.warning(f"Invalid date format '{min_file_date}', using default 2019-01-01: {e}")
            classification_system.config.min_file_modification_date = date(2019, 1, 1)
    
    # Execute the classification
    return classification_system.run_classification(root_folder_path)

# ==========================================
# MAIN EXECUTION
# ==========================================

if __name__ == "__main__":
    # Production configuration for enterprise environment
    ROOT_FOLDER_PATH = r"\\Network\midp-sfs-007\midamreprc"  # Your network path
    OUTPUT_DIR = "healthcare_classification_results"
    
    # Configuration settings
    THREADS = 5                # 5 parallel workers
    MIN_FILE_DATE = "2019-01-01"  # Only process files modified from this date
    ENABLE_INCREMENTAL = True     # Enable incremental processing for future runs
    
    try:
        print("="*80)
        print("STARTING HEALTHCARE CLASSIFICATION")
        print("="*80)
        print(f"EXACT WORKING LOGIC PRESERVED:")
        print(f"  ✓ Stage 1: Healthcare contract detection (exact prompts)")
        print(f"  ✓ Stage 2: Effective date extraction (exact prompts)")
        print(f"  ✓ Page selection logic (first 10, last 10)")
        print(f"  ✓ PDF handling strategies (exact repair methods)")
        print(f"  ✓ Long path support for folder names")
        print("="*80)
        print(f"Configuration:")
        print(f"  Source Path: {ROOT_FOLDER_PATH}")
        print(f"  Output Directory: {OUTPUT_DIR}")
        print(f"  Processing Threads: {THREADS}")
        print(f"  Minimum File Date: {MIN_FILE_DATE}")
        print(f"  Incremental Processing: {'ENABLED' if ENABLE_INCREMENTAL else 'DISABLED'}")
        print("="*80)
        
        # Run the healthcare document classification
        results = run_healthcare_classification(
            root_folder_path=ROOT_FOLDER_PATH,
            output_dir=OUTPUT_DIR,
            threads=THREADS,
            min_file_date=MIN_FILE_DATE,
            enable_incremental=ENABLE_INCREMENTAL
        )
        
        print("\n" + "="*80)
        print("CLASSIFICATION COMPLETED SUCCESSFULLY!")
        print("="*80)
        print(f"Output Directory: {results.get('output_directory')}")
        print(f"Processing Time: {results.get('processing_time_hours', 0):.2f} hours")
        print(f"Documents Discovered: {results.get('total_documents_discovered', 0):,}")
        print(f"Relevant Documents Found: {results.get('relevant_documents_found', 0):,}")
        
        breakdown = results.get('classification_breakdown', {})
        if breakdown:
            print(f"\nDETAILED BREAKDOWN:")
            print(f"  Relevant (2020+):               {breakdown.get('relevant', 0):,}")
            print(f"  Not Healthcare Contracts:       {breakdown.get('not_healthcare', 0):,}")
            print(f"  Old Effective Dates (< 2020):   {breakdown.get('old_date', 0):,}")
            print(f"  No Date Found:                  {breakdown.get('no_date', 0):,}")
            print(f"  Form Field Issues:              {breakdown.get('form_field_issues', 0):,}")
            print(f"  PDF Corrupted:                  {breakdown.get('pdf_corrupted', 0):,}")
            print(f"  Failed:                         {breakdown.get('failed', 0):,}")
            print(f"  Old File Dates:                 {breakdown.get('old_file_date', 0):,}")
            print(f"  Skipped (Unchanged):            {breakdown.get('skipped_unchanged', 0):,}")
        
        print(f"\nKEY PRESERVATION:")
        print(f"  ✓ Exact Stage 1 & Stage 2 logic from working code")
        print(f"  ✓ Same prompts, same DPI, same page selection")
        print(f"  ✓ All PDF repair strategies preserved")
        print(f"  ✓ Parallel processing added without changing core logic")
        
        print(f"\nNEXT STEPS:")
        print(f"  1. Use 'relevant_healthcare_documents_*.txt' with your extraction pipeline")
        print(f"  2. Review 'classification_report_*.csv' for detailed results")
        print(f"  3. Check 'processing_summary_*.json' for statistics")
        print(f"  4. Future runs will be faster due to incremental processing")
        
        print("="*80)
        
    except KeyboardInterrupt:
        print("\n" + "="*80)
        print("GRACEFUL SHUTDOWN INITIATED")
        print("="*80)
        print("Processing has been safely stopped.")
        print("Progress has been saved and can be resumed by running the script again.")
        print("="*80)
        
    except Exception as e:
        logger.error(f"Critical error in execution: {e}")
        print(f"\nERROR: {e}")
        print("\nTROUBLESHOoting:")
        print("1. Check network connectivity to the source path")
        print("2. Verify Google AI API credentials are configured")
        print("3. Ensure sufficient disk space for output files")
        print("4. Review the log file 'healthcare_classification.log' for details")
        print("5. Long path errors should now be resolved with the applied fixes")
        
    finally:
        print(f"\nLOG FILES:")
        print(f"  Main log: healthcare_classification.log")
        print(f"  State file: classification_state.json")
        print(f"  Resume capability: Enabled - rerun script to continue from last checkpoint")
