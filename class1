# Core libraries for file handling, AI interaction, and data processing.
import fitz
import csv
import json
import logging
import os
import time
from pathlib import Path
from typing import List, Optional, Dict, Any
from google.genai import types
import google.genai as genai
from datetime import datetime
from dataclasses import dataclass
from dateutil import parser

# Set up logging to track progress and errors, writing to both a file and the console.
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('healthcare_classification.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# These dataclasses act as blueprints for our data, ensuring consistency.
@dataclass
class DocumentMetadata:
    """Represents a single PDF, holding all its metadata and final classification details."""
    source_path: str
    pdf_name: str
    state: str
    file_size: int
    last_modified: datetime
    classification_status: str = "pending"
    classification_result: str = "unknown"
    detected_on_page: Optional[int] = None
    effective_date: Optional[datetime] = None
    stage_failed: str = ""
    confidence_score: float = 0.0
    processing_time: float = 0.0
    error_message: str = ""

@dataclass
class ClassificationResult:
    """A simple structure to pass results between classification methods."""
    is_relevant: bool
    detected_on_page: Optional[int] = None
    effective_date: Optional[datetime] = None
    stage_failed: str = ""
    confidence: float = 0.0
    processing_time: float = 0.0

class ClassificationConfig:
    """A central configuration class to make tweaking settings easy without digging through the code."""
    def __init__(self):
        # Folder names the script will look for.
        self.agreements_folder = "AGREEMENTS-NON STANDARD"
        self.hospital_subfolder = "HOSPITAL"
        
        # Key business logic setting: the minimum year for a document to be considered relevant.
        self.min_effective_year = 2020
        
        # Settings for AI processing.
        self.image_dpi = 150  # Higher DPI can improve OCR accuracy but increases processing time.
        self.first_pages_count = 10
        self.last_pages_count = 10
        
        # Prompts are carefully worded to guide the AI's response for each task.
        self.healthcare_contract_prompt = "Analyze the image. Is this a formal letter from Aetna about a 'charge master increase' where they state 'we have received' a notification? Answer only YES or NO."
        self.effective_date_prompt = 'This is an Aetna charge master increase letter. Find the EFFECTIVE DATE mentioned in the letter (phrases like "effective January 1, 2024" or "to become effective 1/1/2025"). Respond with just the date found, or "NOT_FOUND" if no effective date exists.'
        
        # A static set of US states used to identify state-level folders.
        self.us_states = {
            'ALABAMA', 'ALASKA', 'ARIZONA', 'ARKANSAS', 'CALIFORNIA', 'COLORADO', 
            'CONNECTICUT', 'DELAWARE', 'FLORIDA', 'GEORGIA', 'HAWAII', 'IDAHO', 
            'ILLINOIS', 'INDIANA', 'IOWA', 'KANSAS', 'KENTUCKY', 'LOUISIANA', 
            'MAINE', 'MARYLAND', 'MASSACHUSETTS', 'MICHIGAN', 'MINNESOTA', 
            'MISSISSIPPI', 'MISSOURI', 'MONTANA', 'NEBRASKA', 'NEVADA', 
            'NEW HAMPSHIRE', 'NEW JERSEY', 'NEW MEXICO', 'NEW YORK', 
            'NORTH CAROLINA', 'NORTH DAKOTA', 'OHIO', 'OKLAHOMA', 'OREGON', 
            'PENNSYLVANIA', 'RHODE ISLAND', 'SOUTH CAROLINA', 'SOUTH DAKOTA', 
            'TENNESSEE', 'TEXAS', 'UTAH', 'VERMONT', 'VIRGINIA', 'WASHINGTON', 
            'WEST VIRGINIA', 'WISCONSIN', 'WYOMING', 'PUERTO RICO', 'DISTRICT OF COLUMBIA'
        }

class EnterpriseFileDiscovery:
    """Handles the logic of finding all target PDF documents within the specified folder structure."""
    def __init__(self, root_path: str, config: ClassificationConfig):
        self.root_path = Path(root_path)
        self.config = config

    def discover_all_documents(self) -> List[DocumentMetadata]:
        """Runs the discovery process by scanning through each state folder."""
        logger.info(f"Starting document discovery from: {self.root_path}")
        agreements_path = self.root_path / self.config.agreements_folder
        if not agreements_path.exists():
            logger.error(f"'{self.config.agreements_folder}' folder not found at: {agreements_path}")
            return []

        all_documents = []
        state_folders = [f for f in agreements_path.iterdir() if f.is_dir() and self._is_state_folder(f)]
        logger.info(f"Found {len(state_folders)} state folders to process.")

        for state_folder in state_folders:
            all_documents.extend(self._process_single_state(state_folder))

        logger.info(f"Discovery complete. Total documents found: {len(all_documents)}")
        return all_documents

    def _process_single_state(self, state_folder: Path) -> List[DocumentMetadata]:
        """Processes a single state folder to find its hospital subfolders and their PDFs."""
        logger.info(f"Processing State: {state_folder.name}")
        hospital_base_path = state_folder / self.config.hospital_subfolder
        if not hospital_base_path.exists():
            logger.warning(f"No '{self.config.hospital_subfolder}' folder found in {state_folder.name}.")
            return []

        state_documents = []
        hospital_subfolders = [d for d in hospital_base_path.iterdir() if d.is_dir()]
        logger.info(f"Found {len(hospital_subfolders)} hospital subfolders in {state_folder.name}.")

        for hospital_folder in hospital_subfolders:
            try:
                logger.info(f"  Scanning hospital: {hospital_folder.name}")
                # We use rglob() to efficiently find all PDFs recursively.
                pdf_paths = hospital_folder.rglob("*.pdf")
                
                for pdf_path in pdf_paths:
                    # This is a critical fix for Windows environments. It prepends a special prefix
                    # to the path string, allowing Python to access files with paths longer than 
                    # the default 260 character limit.
                    path_str = str(pdf_path)
                    if path_str.startswith('\\\\') and not path_str.startswith('\\\\?\\'):
                        path_str = '\\\\?\\UNC\\' + path_str[2:]
                    
                    if os.path.exists(path_str):
                        stats = os.stat(path_str)
                        doc = DocumentMetadata(
                            source_path=path_str,
                            pdf_name=pdf_path.name,
                            state=state_folder.name,
                            file_size=stats.st_size,
                            last_modified=datetime.fromtimestamp(stats.st_mtime)
                        )
                        state_documents.append(doc)
                    else:
                        logger.warning(f"    Path discovered but not found, likely a broken shortcut: {pdf_path.name}")

            except Exception as e:
                logger.error(f"  Could not process hospital folder {hospital_folder.name}: {e}")
        
        logger.info(f"  Found {len(state_documents)} documents in {state_folder.name}.")
        return state_documents

    def _is_state_folder(self, folder_path: Path) -> bool:
        """A helper to identify state folders based on the configured list of names."""
        return any(state in folder_path.name.upper() for state in self.config.us_states)

class HealthcareDocumentClassifier:
    """This class encapsulates the two-stage, AI-powered classification logic."""
    def __init__(self, client, config: ClassificationConfig):
        self.client = client
        self.config = config

    def classify_all_documents(self, documents: List[DocumentMetadata]) -> Dict[str, List[DocumentMetadata]]:
        """
        Classifies a list of documents sequentially.
        For each document, it determines relevance and updates its metadata object.
        """
        logger.info(f"Starting classification of {len(documents)} documents...")
        results = {'relevant': [], 'not_healthcare': [], 'old_date': [], 'no_date': [], 'failed': []}
        
        for i, doc in enumerate(documents, 1):
            logger.info(f"--> Processing {i}/{len(documents)}: {doc.pdf_name} (State: {doc.state})")
            try:
                classification_result = self._classify_single_document(doc)
                
                # Update the original document object with the classification outcome.
                doc.processing_time = classification_result.processing_time
                doc.confidence_score = classification_result.confidence
                doc.detected_on_page = classification_result.detected_on_page
                doc.effective_date = classification_result.effective_date
                doc.stage_failed = classification_result.stage_failed
                doc.classification_status = "completed"

                if classification_result.is_relevant:
                    doc.classification_result = "relevant"
                    results['relevant'].append(doc)
                    logger.info(f"    └── SUCCESS: Document is relevant. Effective Date: {doc.effective_date.strftime('%Y-%m-%d')}")
                else:
                    # Categorize the rejection for better reporting.
                    reason_key = 'not_healthcare' # Default reason
                    if "Stage 2: No effective date" in doc.stage_failed: reason_key = 'no_date'
                    elif "Stage 2: Effective date before" in doc.stage_failed: reason_key = 'old_date'
                    doc.classification_result = reason_key
                    results[reason_key].append(doc)
                    logger.info(f"    └── REJECTED: {doc.stage_failed}")

            except Exception as e:
                logger.error(f"    └── CLASSIFICATION FAILED for {doc.pdf_name}: {e}")
                doc.classification_result = "failed"
                doc.error_message = str(e)
                results['failed'].append(doc)
        return results

    def _classify_single_document(self, doc: DocumentMetadata) -> ClassificationResult:
        """
        Runs the core two-stage AI classification on a single PDF.
        It's designed to fail fast: if a page isn't a contract, it moves on quickly.
        """
        start_time = time.time()
        try:
            with fitz.open(doc.source_path) as pdf_doc:
                pages_to_check = self._get_pages_to_analyze(len(pdf_doc))

                for page_idx in pages_to_check:
                    page = pdf_doc.load_page(page_idx)
                    
                    # Stage 1: Check if the page contains a relevant healthcare contract.
                    if not self._is_healthcare_contract(page):
                        continue # If not, this page is irrelevant, so we check the next one.

                    # Stage 2: If it is a contract, check for a valid effective date.
                    date_result = self._get_effective_date(page)
                    processing_time = time.time() - start_time
                    
                    if date_result and date_result.year >= self.config.min_effective_year:
                        # Success! The document is relevant and its date is current.
                        return ClassificationResult(True, page_idx + 1, date_result, confidence=0.9, processing_time=processing_time)
                    elif date_result:
                        # Fail: A date was found, but it's older than our minimum year.
                        return ClassificationResult(False, page_idx + 1, date_result, stage_failed=f"Stage 2: Effective date before {self.config.min_effective_year} ({date_result.year})", confidence=0.9, processing_time=processing_time)
                    else: 
                        # Fail: The page looked like a contract, but we couldn't find a date on it.
                         return ClassificationResult(False, page_idx + 1, None, stage_failed="Stage 2: No effective date found on relevant page", confidence=0.8, processing_time=processing_time)

            # If we've checked all relevant pages and found no contract, the document is rejected.
            return ClassificationResult(False, stage_failed="Stage 1: Not a healthcare contract", confidence=0.9, processing_time=time.time() - start_time)
        except Exception as e:
            # This will be caught by the calling function to mark the document as 'failed'.
            raise RuntimeError(f"Failed to process PDF {doc.pdf_name}") from e

    def _get_pages_to_analyze(self, total_pages: int) -> List[int]:
        """Optimizes processing by selecting only a subset of pages from large documents."""
        # For large docs, we only check the beginning and end, where key info usually is.
        if total_pages <= (self.config.first_pages_count + self.config.last_pages_count):
            return list(range(total_pages))
        else:
            first = list(range(self.config.first_pages_count))
            last = list(range(total_pages - self.config.last_pages_count, total_pages))
            # Using a set handles any overlap for documents that aren't extremely large.
            return sorted(list(set(first + last)))

    def _call_gemini(self, page: fitz.Page, prompt: str) -> str:
        """A helper that bundles the logic of converting a page to an image and calling the Gemini API."""
        mat = fitz.Matrix(self.config.image_dpi / 72, self.config.image_dpi / 72)
        pix = page.get_pixmap(matrix=mat)
        image_bytes = pix.tobytes("png")
        
        response = self.client.models.generate_content(
            model='gemini-1.5-flash',
            contents=[types.Part.from_bytes(data=image_bytes, mime_type='image/png'), prompt]
        )
        return response.text.strip()

    def _is_healthcare_contract(self, page: fitz.Page) -> bool:
        """Stage 1: Call the AI to determine if the page content is a relevant contract."""
        try:
            answer = self._call_gemini(page, self.config.healthcare_contract_prompt)
            return 'YES' in answer.upper()
        except Exception as e:
            logger.warning(f"AI call failed for contract check: {e}")
            return False

    def _get_effective_date(self, page: fitz.Page) -> Optional[datetime]:
        """Stage 2: Call the AI to extract an effective date and parse it into a datetime object."""
        try:
            date_text = self._call_gemini(page, self.config.effective_date_prompt)
            if "NOT_FOUND" in date_text:
                return None
            # We use dateutil.parser for its ability to handle various date formats.
            return parser.parse(date_text)
        except (parser.ParserError, TypeError, Exception) as e:
            logger.warning(f"Could not parse date from AI response '{date_text}': {e}")
            return None

class EnterpriseHealthcareClassificationSystem:
    """This main class orchestrates the entire workflow: discovery, classification, and reporting."""
    def __init__(self, client, output_dir: str = "classification_results"):
        self.client = client
        self.config = ClassificationConfig()
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def run(self, root_folder_path: str):
        """Executes the end-to-end process."""
        start_time = time.time()
        logger.info("--- STARTING ENTERPRISE CLASSIFICATION ---")

        # Step 1: Discover all potential documents.
        discovery = EnterpriseFileDiscovery(root_folder_path, self.config)
        all_documents = discovery.discover_all_documents()
        if not all_documents:
            logger.error("No documents were found. Exiting.")
            return

        # Step 2: Classify all discovered documents.
        classifier = HealthcareDocumentClassifier(self.client, self.config)
        results = classifier.classify_all_documents(all_documents)

        # Step 3: Save the results to output files.
        self._save_reports(all_documents, results)
        
        end_time = time.time()
        logger.info("--- CLASSIFICATION COMPLETE ---")
        logger.info(f"Total time: {(end_time - start_time):.2f} seconds")
        self._print_summary(results)

    def _save_reports(self, all_docs: List[DocumentMetadata], results: Dict[str, List[DocumentMetadata]]):
        """Saves a detailed CSV for auditing and a simple TXT file for downstream use."""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # This detailed report is useful for auditing and reviewing every decision the script made.
        report_path = self.output_dir / f"full_classification_report_{timestamp}.csv"
        with open(report_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['source_path', 'pdf_name', 'state', 'classification_result', 'effective_date', 'detected_on_page', 'stage_failed', 'error_message'])
            for doc in all_docs:
                writer.writerow([doc.source_path, doc.pdf_name, doc.state, doc.classification_result, doc.effective_date.strftime('%Y-%m-%d') if doc.effective_date else '', doc.detected_on_page, doc.stage_failed, doc.error_message])
        logger.info(f"Full CSV report saved to: {report_path}")

        # This simple text file is designed to be easily fed into the next script in a pipeline.
        relevant_paths = self.output_dir / f"relevant_document_paths_{timestamp}.txt"
        with open(relevant_paths, 'w', encoding='utf-8') as f:
            for doc in results['relevant']:
                f.write(f"{doc.source_path}\n")
        logger.info(f"Relevant paths list for extraction saved to: {relevant_paths}")

    def _print_summary(self, results: Dict[str, List[DocumentMetadata]]):
        """Prints a final, clean summary of the results to the console."""
        print("\n--- FINAL CLASSIFICATION SUMMARY ---")
        print(f"  Relevant Documents (2020+): {len(results['relevant'])}")
        print(f"  Not Healthcare Contracts:   {len(results['not_healthcare'])}")
        print(f"  Old Dates (< 2020):         {len(results['old_date'])}")
        print(f"  No Date Found:              {len(results['no_date'])}")
        print(f"  Failed Processing:          {len(results['failed'])}")
        print("-" * 34)

if __name__ == "__main__":
    # --- Script Configuration ---
    GCP_PROJECT_ID = "anbc-hcb-dev"
    GCP_LOCATION = "us-central1"
    
    # Use a raw string (r"...") for Windows network paths to avoid issues with backslashes.
    ROOT_FOLDER_PATH = r"\\Network\midp-sfs-007\midamreprc"
    OUTPUT_DIR = "healthcare_classification_results"

    try:
        # Before running, ensure your local environment is authenticated with Google Cloud.
        # Typically, this is done by running `gcloud auth application-default login` in your terminal.
        client = genai.Client(project=GCP_PROJECT_ID, location=GCP_LOCATION)

        # Initialize and run the main classification system.
        system = EnterpriseHealthcareClassificationSystem(client=client, output_dir=OUTPUT_DIR)
        system.run(root_folder_path=ROOT_FOLDER_PATH)

    except Exception as e:
        logger.error(f"A critical error occurred in the main execution block: {e}", exc_info=True)
        print(f"CRITICAL ERROR: {e}. Check healthcare_classification.log for details.")
